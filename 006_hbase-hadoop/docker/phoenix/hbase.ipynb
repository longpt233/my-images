{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b29876-205a-497e-a1b2-87ac1b13b8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/04 16:53:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with HBase configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WriteToMySQL222\") \\\n",
    "    .config(\"spark.jars\", \"/tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,/tmp/mysql-connector-java-8.0.29.jar,/tmp/hbase-spark-1.1.0-SNAPSHOT.jar\") \\\n",
    "    .config(\"spark.extraListeners\", \"datahub.spark.DatahubSparkListener\")\\\n",
    "    .config(\"spark.datahub.rest.server\", \"http://10.208.164.167:8080\")\\\n",
    "    .config(\"spark.datahub.metadata.dataset.materialize\", \"true\")\\\n",
    "    .config(\"spark.datahub.metadata.dataset.experimental_include_schema_metadata\", \"true\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "# cau hinh chuan hbase tren mang\n",
    "    # .config(\"spark.jars\", \n",
    "#\"/tmp/hbase-spark-1.0.1-SNAPSHOT_spark331_hbase2415.jar,\\\n",
    "# /tmp/hbase-spark-protocol-shaded-1.0.1-SNAPSHOT_spark331_hbase2415.jar,\n",
    "# /tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,\\\n",
    "# /tmp/hbase-shaded-mapreduce-2.4.15.jar,\n",
    "# /hbase-site.xml.jar,\n",
    "# /tmp/scala-parser-combinators_2.12-2.1.1.jar,\n",
    "# /tmp/htrace-core4-4.2.0-incubating.jar\") \\\n",
    "\n",
    "\n",
    "# backup\n",
    "    # .config(\"spark.hadoop.hbase.zookeeper.quorum\", \"localhost\") \\\n",
    "    # .config(\"spark.hadoop.hbase.zookeeper.property.clientPort\", \"2181\") \\\n",
    "    # .config(\"spark.hadoop.hbase.master\", \"localhost:16000\") \\\n",
    "    # .config(\"spark.executor.extraClassPath\", \"/usr/local/hbase/lib/\")\\\n",
    "    # .config(\"spark.driver.extraClassPath\", \"/usr/local/hbase/lib/\")\\\n",
    "# /hbase-site.xml.jar, ?????\n",
    "#     /tmp/hbase-client-2.4.14.jar,/tmp/hbase-server-2.4.14.jar,/tmp/hbase-mapreduce-2.4.15.jar\n",
    "#     /tmp/hbase-spark3-protocol-shaded-1.0.0.7.2.17.0-334.jar\n",
    "#    /tmp/phoenix5-spark3-6.0.0.7.2.17.0-334.jar,/tmp/phoenix-hbase-compat-2.4.1-5.1.3.jar,/tmp/scala-parser-combinators_2.12-2.1.1.jar,\\\n",
    "#    /usr/local/phoenix/phoenix-client-hbase-2.4-5.1.3.jar,/tmp/phoenix-core-5.1.3.jar,\\\n",
    "    # /tmp/hbase-client-2.4.14.jar,/tmp/hbase-server-2.4.14.jar,/tmp/hbase-mapreduce-2.4.15.jar, \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55b6b3e-6716-42ee-9e4b-8a08e05b0cf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/04 16:53:29 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@427db629, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@427db629, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@427db629, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@427db629, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "         +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$pythonToJava$1\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$pythonToJava$1) is now cleaned +++\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$toJavaArray$1\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$toJavaArray$1) is now cleaned +++\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$applySchemaToPythonRDD$1\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$applySchemaToPythonRDD$1) is now cleaned +++\n",
      "24/12/04 16:53:29 DEBUG KafkaRelationVisitor: Checking if provider is KafkaSourceProvider\n",
      "24/12/04 16:53:29 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:29 DEBUG KafkaRelationVisitor: Kafka classes are not available to check whether provider is KafkaSourceProvider\n",
      "24/12/04 16:53:29 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:29 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:29 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:29 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:29 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:29 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:29 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:29 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:29 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:mysql://10.208.164.167:3306/my_database and properties: {user=my_user, password=my_password}\n",
      "24/12/04 16:53:29 DEBUG PlanUtils: Visitor null visited org.apache.spark.sql.execution.datasources.LogicalRelation, returned [io.openlineage.client.OpenLineage$OutputDataset@27cd1bea]\n",
      "24/12/04 16:53:29 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator rdd_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     rdd_input_0 = inputs[0];\n",
      "/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( rdd_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();\n",
      "/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 028 */       boolean rdd_isNull_0 = rdd_row_0.isNullAt(0);\n",
      "/* 029 */       UTF8String rdd_value_0 = rdd_isNull_0 ?\n",
      "/* 030 */       null : (rdd_row_0.getUTF8String(0));\n",
      "/* 031 */       boolean rdd_isNull_1 = rdd_row_0.isNullAt(1);\n",
      "/* 032 */       UTF8String rdd_value_1 = rdd_isNull_1 ?\n",
      "/* 033 */       null : (rdd_row_0.getUTF8String(1));\n",
      "/* 034 */       boolean rdd_isNull_2 = rdd_row_0.isNullAt(2);\n",
      "/* 035 */       UTF8String rdd_value_2 = rdd_isNull_2 ?\n",
      "/* 036 */       null : (rdd_row_0.getUTF8String(2));\n",
      "/* 037 */       rdd_mutableStateArray_0[0].reset();\n",
      "/* 038 */\n",
      "/* 039 */       rdd_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 040 */\n",
      "/* 041 */       if (rdd_isNull_0) {\n",
      "/* 042 */         rdd_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 043 */       } else {\n",
      "/* 044 */         rdd_mutableStateArray_0[0].write(0, rdd_value_0);\n",
      "/* 045 */       }\n",
      "/* 046 */\n",
      "/* 047 */       if (rdd_isNull_1) {\n",
      "/* 048 */         rdd_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */       } else {\n",
      "/* 050 */         rdd_mutableStateArray_0[0].write(1, rdd_value_1);\n",
      "/* 051 */       }\n",
      "/* 052 */\n",
      "/* 053 */       if (rdd_isNull_2) {\n",
      "/* 054 */         rdd_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 055 */       } else {\n",
      "/* 056 */         rdd_mutableStateArray_0[0].write(2, rdd_value_2);\n",
      "/* 057 */       }\n",
      "/* 058 */       append((rdd_mutableStateArray_0[0].getRow()));\n",
      "/* 059 */       if (shouldStop()) return;\n",
      "/* 060 */     }\n",
      "/* 061 */   }\n",
      "/* 062 */\n",
      "/* 063 */ }\n",
      "\n",
      "24/12/04 16:53:29 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator rdd_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     rdd_input_0 = inputs[0];\n",
      "/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( rdd_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();\n",
      "/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 028 */       boolean rdd_isNull_0 = rdd_row_0.isNullAt(0);\n",
      "/* 029 */       UTF8String rdd_value_0 = rdd_isNull_0 ?\n",
      "/* 030 */       null : (rdd_row_0.getUTF8String(0));\n",
      "/* 031 */       boolean rdd_isNull_1 = rdd_row_0.isNullAt(1);\n",
      "/* 032 */       UTF8String rdd_value_1 = rdd_isNull_1 ?\n",
      "/* 033 */       null : (rdd_row_0.getUTF8String(1));\n",
      "/* 034 */       boolean rdd_isNull_2 = rdd_row_0.isNullAt(2);\n",
      "/* 035 */       UTF8String rdd_value_2 = rdd_isNull_2 ?\n",
      "/* 036 */       null : (rdd_row_0.getUTF8String(2));\n",
      "/* 037 */       rdd_mutableStateArray_0[0].reset();\n",
      "/* 038 */\n",
      "/* 039 */       rdd_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 040 */\n",
      "/* 041 */       if (rdd_isNull_0) {\n",
      "/* 042 */         rdd_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 043 */       } else {\n",
      "/* 044 */         rdd_mutableStateArray_0[0].write(0, rdd_value_0);\n",
      "/* 045 */       }\n",
      "/* 046 */\n",
      "/* 047 */       if (rdd_isNull_1) {\n",
      "/* 048 */         rdd_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */       } else {\n",
      "/* 050 */         rdd_mutableStateArray_0[0].write(1, rdd_value_1);\n",
      "/* 051 */       }\n",
      "/* 052 */\n",
      "/* 053 */       if (rdd_isNull_2) {\n",
      "/* 054 */         rdd_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 055 */       } else {\n",
      "/* 056 */         rdd_mutableStateArray_0[0].write(2, rdd_value_2);\n",
      "/* 057 */       }\n",
      "/* 058 */       append((rdd_mutableStateArray_0[0].getRow()));\n",
      "/* 059 */       if (shouldStop()) return;\n",
      "/* 060 */     }\n",
      "/* 061 */   }\n",
      "/* 062 */\n",
      "/* 063 */ }\n",
      "\n",
      "24/12/04 16:53:29 INFO CodeGenerator: Code generated in 7.922863 ms\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "24/12/04 16:53:29 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 15 took 0.000105 seconds\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: submitStage(ResultStage 1 (name=showString at NativeMethodAccessorImpl.java:0;jobs=1))\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: missing: List()\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: submitMissingTasks(ResultStage 1)\n",
      "24/12/04 16:53:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.2 MiB)\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Put block broadcast_1 locally took 2 ms\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Putting block broadcast_1 without replication took 2 ms\n",
      "24/12/04 16:53:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 366.2 MiB)\n",
      "24/12/04 16:53:29 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop113:38317 (size: 6.4 KiB, free: 366.3 MiB)\n",
      "24/12/04 16:53:29 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Told master about block broadcast_1_piece0\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Put block broadcast_1_piece0 locally took 1 ms\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Putting block broadcast_1_piece0 without replication took 1 ms\n",
      "24/12/04 16:53:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/12/04 16:53:29 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/12/04 16:53:29 DEBUG TaskSetManager: Epoch for TaskSet 1.0: 0\n",
      "24/12/04 16:53:29 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "24/12/04 16:53:29 DEBUG TaskSetManager: Valid locality levels for TaskSet 1.0: NO_PREF, ANY\n",
      "24/12/04 16:53:29 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 0\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 20) (hadoop113, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "24/12/04 16:53:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 20)\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 1\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Getting local block broadcast_1\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "24/12/04 16:53:29 INFO RemovePathPatternUtils: Removing path pattern from dataset name person\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 42, boot = -244, init = 285, finish = 1\n",
      "24/12/04 16:53:29 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(0)\n",
      "24/12/04 16:53:29 DEBUG ContextCleaner: Cleaning broadcast 0\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 20). 1832 bytes result sent to driver\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 0\n",
      "24/12/04 16:53:29 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 0\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 20) in 141 ms on hadoop113 (executor driver) (1/1)\n",
      "24/12/04 16:53:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/12/04 16:53:29 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.154 s\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/04 16:53:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.158481 s\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "24/12/04 16:53:29 DEBUG BlockManagerStorageEndpoint: removing broadcast 0\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Removing broadcast 0\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "24/12/04 16:53:29 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 15 took 0.000065 seconds\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: submitStage(ResultStage 2 (name=showString at NativeMethodAccessorImpl.java:0;jobs=2))\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: missing: List()\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: submitMissingTasks(ResultStage 2)\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Removing block broadcast_0_piece0\n",
      "24/12/04 16:53:29 DEBUG MemoryStore: Block broadcast_0_piece0 of size 13437 dropped from memory (free 384046837)\n",
      "24/12/04 16:53:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.1 KiB, free 366.2 MiB)\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Put block broadcast_2 locally took 1 ms\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Putting block broadcast_2 without replication took 1 ms\n",
      "24/12/04 16:53:29 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:29 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hadoop113:38317 in memory (size: 13.1 KiB, free: 366.3 MiB)\n",
      "24/12/04 16:53:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 366.2 MiB)\n",
      "24/12/04 16:53:29 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Told master about block broadcast_0_piece0\n",
      "24/12/04 16:53:29 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Removing block broadcast_0\n",
      "24/12/04 16:53:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop113:38317 (size: 6.4 KiB, free: 366.3 MiB)\n",
      "24/12/04 16:53:29 DEBUG BlockManagerMaster: Updated info of block broadcast_2_piece0\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Told master about block broadcast_2_piece0\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Put block broadcast_2_piece0 locally took 0 ms\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Putting block broadcast_2_piece0 without replication took 0 ms\n",
      "24/12/04 16:53:29 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/04 16:53:29 DEBUG MemoryStore: Block broadcast_0 of size 27560 dropped from memory (free 384055400)\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 2 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/12/04 16:53:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 4 tasks resource profile 0\n",
      "24/12/04 16:53:29 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 0, response is 0\n",
      "24/12/04 16:53:29 DEBUG TaskSetManager: Epoch for TaskSet 2.0: 0\n",
      "24/12/04 16:53:29 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "24/12/04 16:53:29 DEBUG TaskSetManager: Valid locality levels for TaskSet 2.0: NO_PREF, ANY\n",
      "24/12/04 16:53:29 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to hadoop113:36571\n",
      "24/12/04 16:53:29 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2.0, runningTasks: 0\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 21) (hadoop113, executor driver, partition 1, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 22) (hadoop113, executor driver, partition 2, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 DEBUG ContextCleaner: Cleaned broadcast 0\n",
      "24/12/04 16:53:29 DEBUG ContextCleaner: Got cleaning task CleanAccum(2)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 23) (hadoop113, executor driver, partition 3, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 24) (hadoop113, executor driver, partition 4, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 DEBUG ContextCleaner: Cleaning accumulator 2\n",
      "24/12/04 16:53:29 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "24/12/04 16:53:29 INFO Executor: Running task 1.0 in stage 2.0 (TID 22)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 0.0 in stage 2.0 (TID 21)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 2.0 in stage 2.0 (TID 23)\n",
      "24/12/04 16:53:29 DEBUG ContextCleaner: Cleaned accumulator 2\n",
      "24/12/04 16:53:29 INFO Executor: Running task 3.0 in stage 2.0 (TID 24)\n",
      "24/12/04 16:53:29 DEBUG ContextCleaner: Got cleaning task CleanAccum(1)\n",
      "24/12/04 16:53:29 DEBUG ContextCleaner: Cleaning accumulator 1\n",
      "24/12/04 16:53:29 DEBUG ContextCleaner: Cleaned accumulator 1\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 1\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 2\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 3\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 4\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Getting local block broadcast_2\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 42, boot = -395, init = 436, finish = 1\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 42, boot = -404, init = 445, finish = 1\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 43, boot = -410, init = 453, finish = 0\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 43, boot = -416, init = 459, finish = 0\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 1.0 in stage 2.0 (TID 22). 1789 bytes result sent to driver\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 2.0 in stage 2.0 (TID 23). 1850 bytes result sent to driver\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 3\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 2\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 22) in 47 ms on hadoop113 (executor driver) (1/4)\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 0.0 in stage 2.0 (TID 21). 1789 bytes result sent to driver\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 3.0 in stage 2.0 (TID 24). 1789 bytes result sent to driver\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 1\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 0\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 23) in 47 ms on hadoop113 (executor driver) (2/4)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 21) in 47 ms on hadoop113 (executor driver) (3/4)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 24) in 46 ms on hadoop113 (executor driver) (4/4)\n",
      "24/12/04 16:53:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/12/04 16:53:29 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.056 s\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: After removal of stage 2, remaining stages = 0\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/04 16:53:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.059016 s\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "24/12/04 16:53:29 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "24/12/04 16:53:29 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 15 took 0.000135 seconds\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 15 output partitions\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: submitStage(ResultStage 3 (name=showString at NativeMethodAccessorImpl.java:0;jobs=3))\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: missing: List()\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: submitMissingTasks(ResultStage 3)\n",
      "24/12/04 16:53:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.1 KiB, free 366.3 MiB)\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Put block broadcast_3 locally took 0 ms\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Putting block broadcast_3 without replication took 1 ms\n",
      "24/12/04 16:53:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 366.2 MiB)\n",
      "24/12/04 16:53:29 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop113:38317 (size: 6.4 KiB, free: 366.3 MiB)\n",
      "24/12/04 16:53:29 DEBUG BlockManagerMaster: Updated info of block broadcast_3_piece0\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Told master about block broadcast_3_piece0\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Put block broadcast_3_piece0 locally took 1 ms\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Putting block broadcast_3_piece0 without replication took 1 ms\n",
      "24/12/04 16:53:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Submitting 15 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19))\n",
      "24/12/04 16:53:29 INFO TaskSchedulerImpl: Adding task set 3.0 with 15 tasks resource profile 0\n",
      "24/12/04 16:53:29 DEBUG TaskSetManager: Epoch for TaskSet 3.0: 0\n",
      "24/12/04 16:53:29 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "24/12/04 16:53:29 DEBUG TaskSetManager: Valid locality levels for TaskSet 3.0: NO_PREF, ANY\n",
      "24/12/04 16:53:29 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_3.0, runningTasks: 0\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 25) (hadoop113, executor driver, partition 5, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 26) (hadoop113, executor driver, partition 6, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 27) (hadoop113, executor driver, partition 7, PROCESS_LOCAL, 4482 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 28) (hadoop113, executor driver, partition 8, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 29) (hadoop113, executor driver, partition 9, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 30) (hadoop113, executor driver, partition 10, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 31) (hadoop113, executor driver, partition 11, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 32) (hadoop113, executor driver, partition 12, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 33) (hadoop113, executor driver, partition 13, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 34) (hadoop113, executor driver, partition 14, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 35) (hadoop113, executor driver, partition 15, PROCESS_LOCAL, 4483 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 36) (hadoop113, executor driver, partition 16, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 37) (hadoop113, executor driver, partition 17, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 38) (hadoop113, executor driver, partition 18, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 39) (hadoop113, executor driver, partition 19, PROCESS_LOCAL, 4487 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:29 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "24/12/04 16:53:29 INFO Executor: Running task 0.0 in stage 3.0 (TID 25)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 3.0 in stage 3.0 (TID 28)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 2.0 in stage 3.0 (TID 27)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 1.0 in stage 3.0 (TID 26)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 5.0 in stage 3.0 (TID 30)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 4.0 in stage 3.0 (TID 29)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 6.0 in stage 3.0 (TID 31)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 7.0 in stage 3.0 (TID 32)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 10.0 in stage 3.0 (TID 35)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 9.0 in stage 3.0 (TID 34)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 11.0 in stage 3.0 (TID 36)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 12.0 in stage 3.0 (TID 37)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 13.0 in stage 3.0 (TID 38)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 14.0 in stage 3.0 (TID 39)\n",
      "24/12/04 16:53:29 INFO Executor: Running task 8.0 in stage 3.0 (TID 33)\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 1\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 2\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 3\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 4\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 5\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Getting local block broadcast_3\n",
      "24/12/04 16:53:29 DEBUG BlockManager: Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 6\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 7\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 8\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 9\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 10\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 11\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 12\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 13\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 15\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 14\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 42, boot = -471, init = 512, finish = 1\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 43, boot = -463, init = 506, finish = 0\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 43, boot = -474, init = 516, finish = 1\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 43, boot = -467, init = 510, finish = 0\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 43, boot = -487, init = 530, finish = 0\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 5.0 in stage 3.0 (TID 30). 1789 bytes result sent to driver\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 14\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 44, boot = -488, init = 532, finish = 0\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 44, boot = -467, init = 511, finish = 0\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 44, boot = -464, init = 508, finish = 0\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 30) in 51 ms on hadoop113 (executor driver) (1/15)\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 44, boot = -475, init = 519, finish = 0\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 10.0 in stage 3.0 (TID 35). 1847 bytes result sent to driver\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 13\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 45, boot = -454, init = 499, finish = 0\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 45, boot = -452, init = 497, finish = 0\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 35) in 52 ms on hadoop113 (executor driver) (2/15)\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 44, boot = -437, init = 481, finish = 0\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 44, boot = -455, init = 499, finish = 0\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 48, boot = -485, init = 533, finish = 0\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 8.0 in stage 3.0 (TID 33). 1789 bytes result sent to driver\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 12\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 0.0 in stage 3.0 (TID 25). 1789 bytes result sent to driver\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 14.0 in stage 3.0 (TID 39). 1855 bytes result sent to driver\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 3.0 in stage 3.0 (TID 28). 1789 bytes result sent to driver\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 2.0 in stage 3.0 (TID 27). 1842 bytes result sent to driver\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 11\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 10\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 9\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 8\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 9.0 in stage 3.0 (TID 34). 1789 bytes result sent to driver\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 6.0 in stage 3.0 (TID 31). 1850 bytes result sent to driver\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 7\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 6\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 33) in 60 ms on hadoop113 (executor driver) (3/15)\n",
      "24/12/04 16:53:29 INFO PythonRunner: Times: total = 46, boot = -483, init = 529, finish = 0\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 1.0 in stage 3.0 (TID 26). 1832 bytes result sent to driver\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 7.0 in stage 3.0 (TID 32). 1789 bytes result sent to driver\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 5\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 4\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 11.0 in stage 3.0 (TID 36). 1789 bytes result sent to driver\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 4.0 in stage 3.0 (TID 29). 1789 bytes result sent to driver\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 3\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 2\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 12.0 in stage 3.0 (TID 37). 1789 bytes result sent to driver\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 1\n",
      "24/12/04 16:53:29 INFO Executor: Finished task 13.0 in stage 3.0 (TID 38). 1789 bytes result sent to driver\n",
      "24/12/04 16:53:29 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 0\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 28) in 71 ms on hadoop113 (executor driver) (4/15)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 27) in 71 ms on hadoop113 (executor driver) (5/15)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 25) in 72 ms on hadoop113 (executor driver) (6/15)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 39) in 69 ms on hadoop113 (executor driver) (7/15)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 26) in 72 ms on hadoop113 (executor driver) (8/15)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 34) in 71 ms on hadoop113 (executor driver) (9/15)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 31) in 72 ms on hadoop113 (executor driver) (10/15)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 32) in 72 ms on hadoop113 (executor driver) (11/15)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 36) in 71 ms on hadoop113 (executor driver) (12/15)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 37) in 71 ms on hadoop113 (executor driver) (13/15)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 29) in 73 ms on hadoop113 (executor driver) (14/15)\n",
      "24/12/04 16:53:29 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 38) in 71 ms on hadoop113 (executor driver) (15/15)\n",
      "24/12/04 16:53:29 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/12/04 16:53:29 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.084 s\n",
      "24/12/04 16:53:29 DEBUG DAGScheduler: After removal of stage 3, remaining stages = 0\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/04 16:53:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/12/04 16:53:29 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.088268 s\n",
      "24/12/04 16:53:29 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:29 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:29 INFO CodeGenerator: Code generated in 9.468648 ms\n",
      "+---+---------+----------+\n",
      "| id|     city|      name|\n",
      "+---+---------+----------+\n",
      "|  2|Telangana|   Chhetri|\n",
      "|  3|   Sikkim|    Bhutia|\n",
      "|  4|Hyderabad|   Shabbir|\n",
      "|  5|   Kerala|   Vijayan|\n",
      "|  6|  Mizoram|Lalpekhlua|\n",
      "+---+---------+----------+\n",
      "\n",
      "24/12/04 16:53:30 DEBUG FileSystem: Starting: Acquiring creator semaphore for hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:30 DEBUG FileSystem: Acquiring creator semaphore for hdfs://hadoop:9000/tmp/test: duration 0:00.001s\n",
      "24/12/04 16:53:30 DEBUG FileSystem: Starting: Creating FS hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:30 DEBUG FileSystem: Looking for FS supporting hdfs\n",
      "24/12/04 16:53:30 DEBUG FileSystem: looking for configuration option fs.hdfs.impl\n",
      "24/12/04 16:53:30 DEBUG FileSystem: Looking in service filesystems for implementation class\n",
      "24/12/04 16:53:30 DEBUG FileSystem: FS for hdfs is class org.apache.hadoop.hdfs.DistributedFileSystem\n",
      "24/12/04 16:53:30 DEBUG DfsClientConf: dfs.client.use.legacy.blockreader.local = false\n",
      "24/12/04 16:53:30 DEBUG DfsClientConf: dfs.client.read.shortcircuit = false\n",
      "24/12/04 16:53:30 DEBUG DfsClientConf: dfs.client.domain.socket.data.traffic = false\n",
      "24/12/04 16:53:30 DEBUG DfsClientConf: dfs.domain.socket.path = \n",
      "24/12/04 16:53:30 DEBUG DFSClient: Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0\n",
      "24/12/04 16:53:30 DEBUG RetryUtils: multipleLinearRandomRetry = null\n",
      "24/12/04 16:53:30 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker@1f6ebb6f\n",
      "24/12/04 16:53:30 DEBUG Client: getting client out of cache: Client-1bc7be3a6e4a482f82dcee722b40672c\n",
      "24/12/04 16:53:30 DEBUG HdfsPathDataset: path: person\n",
      "24/12/04 16:53:30 DEBUG RemovePathPatternUtils: Transformed path is person\n",
      "24/12/04 16:53:30 DEBUG SparkSQLExecutionContext: Posting event for start 0: io.openlineage.client.OpenLineage$RunEvent@34c5a938\n",
      "24/12/04 16:53:30 DEBUG OpenLineageClient: OpenLineageClient will emit lineage event: {\"eventTime\":\"2024-12-04T09:53:27.93Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-c181-790c-916b-a4cf11febaed\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-bb7d-780d-8509-030ede4fabd2\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:30 INFO ConsoleTransport: {\"eventTime\":\"2024-12-04T09:53:27.93Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-c181-790c-916b-a4cf11febaed\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-bb7d-780d-8509-030ede4fabd2\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:30 DEBUG EventEmitter: Emitting lineage completed successfully: {\"eventTime\":\"2024-12-04T09:53:27.93Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-c181-790c-916b-a4cf11febaed\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-bb7d-780d-8509-030ede4fabd2\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:30 INFO AsyncEventQueue: Process of event SparkListenerSQLExecutionStart(0,save at NativeMethodAccessorImpl.java:0,org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "java.lang.reflect.Method.invoke(Method.java:498)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:282)\n",
      "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750),== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand (1)\n",
      "   +- SaveIntoDataSourceCommand (2)\n",
      "         +- LogicalRDD (3)\n",
      "\n",
      "\n",
      "(1) Execute SaveIntoDataSourceCommand\n",
      "Output: []\n",
      "\n",
      "(2) SaveIntoDataSourceCommand\n",
      "Arguments: org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@427db629, [url=*********(redacted), driver=com.mysql.cj.jdbc.Driver, dbtable=person, user=my_user, password=*********(redacted)], Overwrite\n",
      "\n",
      "(3) LogicalRDD\n",
      "Arguments: [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      ",org.apache.spark.sql.execution.SparkPlanInfo@e60025fe,1733306007930,Map()) by listener DatahubSparkListener took 2.143519511s.\n",
      "24/12/04 16:53:30 INFO DatahubSparkListener: sparkEnv: spark.app.id=local-1733306005596\n",
      "spark.app.initial.jar.urls=spark://hadoop113:36571/jars/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,spark://hadoop113:36571/jars/hbase-spark-1.1.0-SNAPSHOT.jar,spark://hadoop113:36571/jars/mysql-connector-java-8.0.29.jar\n",
      "spark.app.name=WriteToMySQL222\n",
      "spark.app.startTime=1733306005134\n",
      "spark.app.submitTime=1733306005071\n",
      "spark.datahub.metadata.dataset.experimental_include_schema_metadata=true\n",
      "spark.datahub.metadata.dataset.materialize=true\n",
      "spark.datahub.rest.server=http://10.208.164.167:8080\n",
      "spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.driver.host=hadoop113\n",
      "spark.driver.port=36571\n",
      "spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.executor.id=driver\n",
      "spark.extraListeners=datahub.spark.DatahubSparkListener\n",
      "spark.jars=/tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,/tmp/mysql-connector-java-8.0.29.jar,/tmp/hbase-spark-1.1.0-SNAPSHOT.jar\n",
      "spark.master=local[*]\n",
      "spark.rdd.compress=True\n",
      "spark.repl.local.jars=file:///tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,file:///tmp/mysql-connector-java-8.0.29.jar,file:///tmp/hbase-spark-1.1.0-SNAPSHOT.jar\n",
      "spark.serializer.objectStreamReset=100\n",
      "spark.sql.warehouse.dir=file:/spark-warehouse\n",
      "spark.submit.deployMode=client\n",
      "spark.submit.pyFiles=\n",
      "spark.ui.showConsoleProgress=true\n",
      "24/12/04 16:53:30 INFO DatahubSparkListener: Datahub configuration: {\n",
      "    # String: 1-3\n",
      "    \"metadata\" : {\n",
      "        # String: 1-3\n",
      "        \"dataset\" : {\n",
      "            # String: 1\n",
      "            \"experimental_include_schema_metadata\" : \"true\",\n",
      "            # String: 3\n",
      "            \"materialize\" : \"true\"\n",
      "        }\n",
      "    },\n",
      "    # String: 2\n",
      "    \"rest\" : {\n",
      "        # String: 2\n",
      "        \"server\" : \"http://10.208.164.167:8080\"\n",
      "    }\n",
      "}\n",
      "\n",
      "24/12/04 16:53:30 INFO DatahubSparkListener: REST Emitter Configuration: GMS url http://10.208.164.167:8080\n",
      "24/12/04 16:53:30 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(2)\n",
      "24/12/04 16:53:30 DEBUG ContextCleaner: Cleaning broadcast 2\n",
      "24/12/04 16:53:30 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 2\n",
      "24/12/04 16:53:30 DEBUG BlockManagerStorageEndpoint: removing broadcast 2\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Removing broadcast 2\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Removing block broadcast_2_piece0\n",
      "24/12/04 16:53:30 DEBUG MemoryStore: Block broadcast_2_piece0 of size 6557 dropped from memory (free 384042957)\n",
      "24/12/04 16:53:30 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:30 INFO BlockManagerInfo: Removed broadcast_2_piece0 on hadoop113:38317 in memory (size: 6.4 KiB, free: 366.3 MiB)\n",
      "24/12/04 16:53:30 DEBUG BlockManagerMaster: Updated info of block broadcast_2_piece0\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Told master about block broadcast_2_piece0\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Removing block broadcast_2\n",
      "24/12/04 16:53:30 DEBUG MemoryStore: Block broadcast_2 of size 12440 dropped from memory (free 384055397)\n",
      "24/12/04 16:53:30 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 2, response is 0\n",
      "24/12/04 16:53:30 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to hadoop113:36571\n",
      "24/12/04 16:53:30 DEBUG ContextCleaner: Cleaned broadcast 2\n",
      "24/12/04 16:53:30 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(3)\n",
      "24/12/04 16:53:30 DEBUG ContextCleaner: Cleaning broadcast 3\n",
      "24/12/04 16:53:30 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 3\n",
      "24/12/04 16:53:30 DEBUG BlockManagerStorageEndpoint: removing broadcast 3\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Removing broadcast 3\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Removing block broadcast_3\n",
      "24/12/04 16:53:30 DEBUG MemoryStore: Block broadcast_3 of size 12440 dropped from memory (free 384067837)\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Removing block broadcast_3_piece0\n",
      "24/12/04 16:53:30 DEBUG MemoryStore: Block broadcast_3_piece0 of size 6560 dropped from memory (free 384074397)\n",
      "24/12/04 16:53:30 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:30 INFO BlockManagerInfo: Removed broadcast_3_piece0 on hadoop113:38317 in memory (size: 6.4 KiB, free: 366.3 MiB)\n",
      "24/12/04 16:53:30 DEBUG BlockManagerMaster: Updated info of block broadcast_3_piece0\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Told master about block broadcast_3_piece0\n",
      "24/12/04 16:53:30 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 3, response is 0\n",
      "24/12/04 16:53:30 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to hadoop113:36571\n",
      "24/12/04 16:53:30 DEBUG ContextCleaner: Cleaned broadcast 3\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: loadDatahubConfig completed successfully in 116 ms\n",
      "24/12/04 16:53:30 INFO ArgumentParser: Couldn't log config from file, will read it from SparkConf\n",
      "24/12/04 16:53:30 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework\n",
      "24/12/04 16:53:30 DEBUG CompositeMeterRegistry: A MeterFilter is being configured after a Meter has been registered to this registry. All MeterFilters should be configured before any Meters are registered. If that is not possible or you have a use case where it should be allowed, let the Micrometer maintainers know at https://github.com/micrometer-metrics/micrometer/issues/4920.\n",
      "java.lang.Thread.getStackTrace(Thread.java:1564)\n",
      "\tat io.micrometer.core.instrument.MeterRegistry$Config.logWarningAboutLateFilter(MeterRegistry.java:844)\n",
      "\tat io.micrometer.core.instrument.MeterRegistry$Config.meterFilter(MeterRegistry.java:830)\n",
      "\tat io.micrometer.core.instrument.MeterRegistry$Config.commonTags(MeterRegistry.java:807)\n",
      "\tat datahub.spark.DatahubSparkListener.initializeMetrics(DatahubSparkListener.java:292)\n",
      "\tat datahub.spark.DatahubSparkListener.initializeContextFactoryIfNotInitialized(DatahubSparkListener.java:339)\n",
      "\tat datahub.spark.DatahubSparkListener.initializeContextFactoryIfNotInitialized(DatahubSparkListener.java:328)\n",
      "\tat datahub.spark.DatahubSparkListener.lambda$initializeContextFactoryIfNotInitialized$1(DatahubSparkListener.java:314)\n",
      "\tat java.util.Optional.ifPresent(Optional.java:159)\n",
      "\tat datahub.spark.DatahubSparkListener.initializeContextFactoryIfNotInitialized(DatahubSparkListener.java:314)\n",
      "\tat datahub.spark.DatahubSparkListener.onJobStart(DatahubSparkListener.java:221)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "24/12/04 16:53:30 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/04 16:53:30 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/04 16:53:30 ERROR ContextFactory: Query execution is null: can't emit event for executionId 0\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onJobStart completed successfully in 128 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onJobEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd\n",
      "24/12/04 16:53:30 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/04 16:53:30 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:30 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:30 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionEnd - executionId: 0\n",
      "24/12/04 16:53:30 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"ID\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"Name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"Age\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]\n",
      "24/12/04 16:53:30 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.ExecutedCommandExec\",\"num-children\":0,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"ID\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"Name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"Age\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]}]\n",
      "24/12/04 16:53:30 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@427db629, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@427db629, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@427db629, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@427db629, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "         +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:30 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@427db629, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@427db629, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@427db629, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@427db629, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "         +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:30 DEBUG KafkaRelationVisitor: Checking if provider is KafkaSourceProvider\n",
      "24/12/04 16:53:30 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:30 DEBUG KafkaRelationVisitor: Kafka classes are not available to check whether provider is KafkaSourceProvider\n",
      "24/12/04 16:53:30 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:30 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:30 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:30 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:30 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:30 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:30 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:30 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/04 16:53:30 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:mysql://10.208.164.167:3306/my_database and properties: {user=my_user, password=my_password}\n",
      "24/12/04 16:53:30 DEBUG PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.\n",
      "24/12/04 16:53:30 DEBUG DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection\n",
      "24/12/04 16:53:30 DEBUG FileSystem: Creating FS hdfs://hadoop:9000/tmp/test: duration 0:00.212s\n",
      "24/12/04 16:53:30 DEBUG PlanUtils: Visitor null visited org.apache.spark.sql.execution.datasources.LogicalRelation, returned [io.openlineage.client.OpenLineage$OutputDataset@9353acc]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/04 16:53:30 INFO RemovePathPatternUtils: Removing path pattern from dataset name person\n",
      "24/12/04 16:53:30 DEBUG HdfsPathDataset: path: person\n",
      "24/12/04 16:53:30 DEBUG RemovePathPatternUtils: Transformed path is person\n",
      "24/12/04 16:53:30 DEBUG SparkSQLExecutionContext: Posting event for end 0: {\"eventTime\":\"2024-12-04T09:53:29.552Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-ca5d-7b24-b571-a9915a2ac31a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:30 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:29.552Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-ca5d-7b24-b571-a9915a2ac31a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:30 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:29.552Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-ca5d-7b24-b571-a9915a2ac31a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:30 DEBUG FileCommitProtocol: Creating committer org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol; job 5187aa4d-e649-457b-8943-b641f61c8259; output=hdfs://hadoop:9000/tmp/test; dynamic=false\n",
      "24/12/04 16:53:30 DEBUG FileCommitProtocol: Using (String, String, Boolean) constructor\n",
      "24/12/04 16:53:30 DEBUG Client: The ping interval is 60000 ms.\n",
      "24/12/04 16:53:30 DEBUG Client: Connecting to hadoop/172.25.1.12:9000\n",
      "24/12/04 16:53:30 DEBUG Client: Setup connection to hadoop/172.25.1.12:9000\n",
      "24/12/04 16:53:30 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@11e3274f, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root: starting, having connections 1\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #0\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 37ms\n",
      "24/12/04 16:53:30 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:30 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@11e3274f, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #1 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 DEBUG OpenLineageToDataHub: Dataset URN: Optional[urn:li:dataset:(urn:li:dataPlatform:mysql,person,PROD)], alias_list: {}\n",
      "24/12/04 16:53:30 DEBUG OpenLineageToDataHub: Building output with person\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #1\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 2ms\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #2 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #2\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: delete took 4ms\n",
      "24/12/04 16:53:30 INFO ParquetFileFormat: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_save_into_data_source_command.person), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306009552}, name=execute_save_into_data_source_command.person, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306010303}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1733306009552, status=COMPLETE}, dataProcessInstanceProperties={name=01939116-ca5d-7b24-b571-a9915a2ac31a, created={actor=urn:li:corpuser:datahub, time=1733306009552}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_save_into_data_source_command.person), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-ca5d-7b24-b571-a9915a2ac31a, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:mysql,person,PROD), schemaMetadata={platformSchema={com.linkedin.schema.MySqlDDL={tableSchema=[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]}}, schemaName=, fields=[{fieldPath=ID, type={type={com.linkedin.schema.NumberType={}}}, nativeDataType=long}, {fieldPath=Name, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=Age, type={type={com.linkedin.schema.NumberType={}}}, nativeDataType=long}], version=1, hash=, platform=urn:li:dataPlatform:mysql}, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306009552, eventFormatter=datahub.event.EventFormatter@48ecddb9), from {\"eventTime\":\"2024-12-04T09:53:29.552Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-ca5d-7b24-b571-a9915a2ac31a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:30 INFO DatahubEventEmitter: Collecting lineage successfully in 123 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart\n",
      "24/12/04 16:53:30 ERROR ContextFactory: Query execution is null: can't emit event for executionId 1\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/04 16:53:30 ERROR ContextFactory: Query execution is null: can't emit event for executionId 1\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onJobStart completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onJobEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/04 16:53:30 ERROR ContextFactory: Query execution is null: can't emit event for executionId 1\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onJobStart completed successfully in 1 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onJobEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/04 16:53:30 ERROR ContextFactory: Query execution is null: can't emit event for executionId 1\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onJobStart completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: onJobEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd\n",
      "24/12/04 16:53:30 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/04 16:53:30 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:30 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:30 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionEnd - executionId: 1\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653304108806179280809368_0000}; taskId=attempt_202412041653304108806179280809368_0000_m_000000_0, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@1ff56d27}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653304108806179280809368_0000}; taskId=attempt_202412041653304108806179280809368_0000_m_000000_0, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653304108806179280809368_0000}; taskId=attempt_202412041653304108806179280809368_0000_m_000000_0, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@5dde8320}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653304108806179280809368_0000}; taskId=attempt_202412041653304108806179280809368_0000_m_000000_0, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG DFSClient: /tmp/test/_temporary/0: masked={ masked: rwxr-xr-x, unmasked: rwxrwxrwx }\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #3 org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs\n",
      "24/12/04 16:53:30 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.GlobalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.LocalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}]\n",
      "24/12/04 16:53:30 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.CollectLimitExec\",\"num-children\":1,\"limit\":21,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.RDDScanExec\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"name\":\"ExistingRDD\",\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[]}]\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #3\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: mkdirs took 5ms\n",
      "24/12/04 16:53:30 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#16 as string) AS id#25, cast(city#17 as string) AS city#26, cast(name#18 as string) AS name#27]\n",
      "      +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#16 as string) AS id#25, cast(city#17 as string) AS city#26, cast(name#18 as string) AS name#27]\n",
      "      +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#16,city#17,name#18]\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:30 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#16 as string) AS id#25, cast(city#17 as string) AS city#26, cast(name#18 as string) AS name#27]\n",
      "      +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#16 as string) AS id#25, cast(city#17 as string) AS city#26, cast(name#18 as string) AS name#27]\n",
      "      +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#16,city#17,name#18]\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:30 DEBUG SparkSQLExecutionContext: Posting event for end 1: {\"eventTime\":\"2024-12-04T09:53:29.998Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-caf1-729f-9d83-223cec4dce5b\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:30 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator rdd_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     rdd_input_0 = inputs[0];\n",
      "/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( rdd_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();\n",
      "/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 028 */       boolean rdd_isNull_0 = rdd_row_0.isNullAt(0);\n",
      "/* 029 */       UTF8String rdd_value_0 = rdd_isNull_0 ?\n",
      "/* 030 */       null : (rdd_row_0.getUTF8String(0));\n",
      "/* 031 */       boolean rdd_isNull_1 = rdd_row_0.isNullAt(1);\n",
      "/* 032 */       UTF8String rdd_value_1 = rdd_isNull_1 ?\n",
      "/* 033 */       null : (rdd_row_0.getUTF8String(1));\n",
      "/* 034 */       boolean rdd_isNull_2 = rdd_row_0.isNullAt(2);\n",
      "/* 035 */       UTF8String rdd_value_2 = rdd_isNull_2 ?\n",
      "/* 036 */       null : (rdd_row_0.getUTF8String(2));\n",
      "/* 037 */       rdd_mutableStateArray_0[0].reset();\n",
      "/* 038 */\n",
      "/* 039 */       rdd_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 040 */\n",
      "/* 041 */       if (rdd_isNull_0) {\n",
      "/* 042 */         rdd_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 043 */       } else {\n",
      "/* 044 */         rdd_mutableStateArray_0[0].write(0, rdd_value_0);\n",
      "/* 045 */       }\n",
      "/* 046 */\n",
      "/* 047 */       if (rdd_isNull_1) {\n",
      "/* 048 */         rdd_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */       } else {\n",
      "/* 050 */         rdd_mutableStateArray_0[0].write(1, rdd_value_1);\n",
      "/* 051 */       }\n",
      "/* 052 */\n",
      "/* 053 */       if (rdd_isNull_2) {\n",
      "/* 054 */         rdd_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 055 */       } else {\n",
      "/* 056 */         rdd_mutableStateArray_0[0].write(2, rdd_value_2);\n",
      "/* 057 */       }\n",
      "/* 058 */       append((rdd_mutableStateArray_0[0].getRow()));\n",
      "/* 059 */       if (shouldStop()) return;\n",
      "/* 060 */     }\n",
      "/* 061 */   }\n",
      "/* 062 */\n",
      "/* 063 */ }\n",
      "\n",
      "24/12/04 16:53:30 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "24/12/04 16:53:30 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:29.998Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-caf1-729f-9d83-223cec4dce5b\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:30 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:29.998Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-caf1-729f-9d83-223cec4dce5b\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:30 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@36d81b0, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:30 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:30 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@36d81b0, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:30 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "24/12/04 16:53:30 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306009998}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306010370}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1733306009998, status=COMPLETE}, dataProcessInstanceProperties={name=01939116-caf1-729f-9d83-223cec4dce5b, created={actor=urn:li:corpuser:datahub, time=1733306009998}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-caf1-729f-9d83-223cec4dce5b, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306009998, eventFormatter=datahub.event.EventFormatter@391e5d8b), from {\"eventTime\":\"2024-12-04T09:53:29.998Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-caf1-729f-9d83-223cec4dce5b\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:30 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/04 16:53:30 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart\n",
      "24/12/04 16:53:30 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/04 16:53:30 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:30 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:30 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$write$21\n",
      "24/12/04 16:53:30 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionStart - executionId: 2\n",
      "24/12/04 16:53:30 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$write$21) is now cleaned +++\n",
      "24/12/04 16:53:30 DEBUG InsertIntoHadoopFsRelationVisitor: Matched io.openlineage.spark.agent.lifecycle.plan.InsertIntoHadoopFsRelationVisitor<org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand,io.openlineage.client.OpenLineage$OutputDataset> to logical plan InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "24/12/04 16:53:30 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand\",\"num-children\":1,\"outputPath\":null,\"staticPartitions\":null,\"ifPartitionNotExists\":false,\"partitionColumns\":[],\"fileFormat\":null,\"options\":null,\"query\":0,\"mode\":null,\"outputColumnNames\":\"[id, city, name]\"},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}]\n",
      "24/12/04 16:53:30 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "24/12/04 16:53:30 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 16 took 0.000173 seconds\n",
      "24/12/04 16:53:30 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/04 16:53:30 INFO DAGScheduler: Got job 4 (parquet at NativeMethodAccessorImpl.java:0) with 20 output partitions\n",
      "24/12/04 16:53:30 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "24/12/04 16:53:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/04 16:53:30 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/04 16:53:30 DEBUG DAGScheduler: submitStage(ResultStage 4 (name=parquet at NativeMethodAccessorImpl.java:0;jobs=4))\n",
      "24/12/04 16:53:30 DEBUG DAGScheduler: missing: List()\n",
      "24/12/04 16:53:30 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/04 16:53:30 DEBUG DAGScheduler: submitMissingTasks(ResultStage 4)\n",
      "24/12/04 16:53:30 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 214.5 KiB, free 366.1 MiB)\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Put block broadcast_4 locally took 1 ms\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Putting block broadcast_4 without replication took 1 ms\n",
      "24/12/04 16:53:30 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 77.2 KiB, free 366.0 MiB)\n",
      "24/12/04 16:53:30 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:30 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop113:38317 (size: 77.2 KiB, free: 366.2 MiB)\n",
      "24/12/04 16:53:30 DEBUG BlockManagerMaster: Updated info of block broadcast_4_piece0\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Told master about block broadcast_4_piece0\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Put block broadcast_4_piece0 locally took 1 ms\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Putting block broadcast_4_piece0 without replication took 1 ms\n",
      "24/12/04 16:53:30 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/04 16:53:30 INFO DAGScheduler: Submitting 20 missing tasks from ResultStage 4 (MapPartitionsRDD[16] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "24/12/04 16:53:30 INFO TaskSchedulerImpl: Adding task set 4.0 with 20 tasks resource profile 0\n",
      "24/12/04 16:53:30 DEBUG TaskSetManager: Epoch for TaskSet 4.0: 0\n",
      "24/12/04 16:53:30 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "24/12/04 16:53:30 DEBUG TaskSetManager: Valid locality levels for TaskSet 4.0: NO_PREF, ANY\n",
      "24/12/04 16:53:30 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_4.0, runningTasks: 0\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 40) (hadoop113, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 41) (hadoop113, executor driver, partition 1, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 42) (hadoop113, executor driver, partition 2, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 43) (hadoop113, executor driver, partition 3, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 4.0 in stage 4.0 (TID 44) (hadoop113, executor driver, partition 4, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 5.0 in stage 4.0 (TID 45) (hadoop113, executor driver, partition 5, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 6.0 in stage 4.0 (TID 46) (hadoop113, executor driver, partition 6, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 7.0 in stage 4.0 (TID 47) (hadoop113, executor driver, partition 7, PROCESS_LOCAL, 4482 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 8.0 in stage 4.0 (TID 48) (hadoop113, executor driver, partition 8, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 9.0 in stage 4.0 (TID 49) (hadoop113, executor driver, partition 9, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 10.0 in stage 4.0 (TID 50) (hadoop113, executor driver, partition 10, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 11.0 in stage 4.0 (TID 51) (hadoop113, executor driver, partition 11, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 12.0 in stage 4.0 (TID 52) (hadoop113, executor driver, partition 12, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 13.0 in stage 4.0 (TID 53) (hadoop113, executor driver, partition 13, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 14.0 in stage 4.0 (TID 54) (hadoop113, executor driver, partition 14, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 15.0 in stage 4.0 (TID 55) (hadoop113, executor driver, partition 15, PROCESS_LOCAL, 4483 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 16.0 in stage 4.0 (TID 56) (hadoop113, executor driver, partition 16, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 17.0 in stage 4.0 (TID 57) (hadoop113, executor driver, partition 17, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 18.0 in stage 4.0 (TID 58) (hadoop113, executor driver, partition 18, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Starting task 19.0 in stage 4.0 (TID 59) (hadoop113, executor driver, partition 19, PROCESS_LOCAL, 4487 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:30 INFO Executor: Running task 0.0 in stage 4.0 (TID 40)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 1.0 in stage 4.0 (TID 41)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 3.0 in stage 4.0 (TID 43)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 2.0 in stage 4.0 (TID 42)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 4.0 in stage 4.0 (TID 44)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 5.0 in stage 4.0 (TID 45)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 6.0 in stage 4.0 (TID 46)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 10.0 in stage 4.0 (TID 50)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 9.0 in stage 4.0 (TID 49)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 13.0 in stage 4.0 (TID 53)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 8.0 in stage 4.0 (TID 48)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 11.0 in stage 4.0 (TID 51)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 15.0 in stage 4.0 (TID 55)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 18.0 in stage 4.0 (TID 58)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 19.0 in stage 4.0 (TID 59)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 16.0 in stage 4.0 (TID 56)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 7.0 in stage 4.0 (TID 47)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 17.0 in stage 4.0 (TID 57)\n",
      "24/12/04 16:53:30 INFO Executor: Running task 12.0 in stage 4.0 (TID 52)\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 1\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 2\n",
      "24/12/04 16:53:30 INFO Executor: Running task 14.0 in stage 4.0 (TID 54)\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 3\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 4\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 5\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 7\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 8\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 9\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 10\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 12\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 6\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 11\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Getting local block broadcast_4\n",
      "24/12/04 16:53:30 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 14\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 13\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 15\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 16\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 17\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 19\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 18\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 20\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306914331393683665557_0004}; taskId=attempt_202412041653306914331393683665557_0004_m_000001_41, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@6cb7a0a1}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306914331393683665557_0004}; taskId=attempt_202412041653306914331393683665557_0004_m_000001_41, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653305481321543453047061_0004}; taskId=attempt_202412041653305481321543453047061_0004_m_000005_45, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@2905610b}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653305481321543453047061_0004}; taskId=attempt_202412041653305481321543453047061_0004_m_000005_45, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653301503196464597393543_0004}; taskId=attempt_202412041653301503196464597393543_0004_m_000007_47, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@1ff82304}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653301503196464597393543_0004}; taskId=attempt_202412041653301503196464597393543_0004_m_000007_47, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653308993897262543194432_0004}; taskId=attempt_202412041653308993897262543194432_0004_m_000012_52, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@3e85fd03}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653308993897262543194432_0004}; taskId=attempt_202412041653308993897262543194432_0004_m_000012_52, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653301503196464597393543_0004}; taskId=attempt_202412041653301503196464597393543_0004_m_000007_47, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@6235d27e}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653301503196464597393543_0004}; taskId=attempt_202412041653301503196464597393543_0004_m_000007_47, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653304722447511113489167_0004}; taskId=attempt_202412041653304722447511113489167_0004_m_000010_50, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@1de7b2bf}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653304722447511113489167_0004}; taskId=attempt_202412041653304722447511113489167_0004_m_000010_50, status=''}\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653302310833311133822454_0004}; taskId=attempt_202412041653302310833311133822454_0004_m_000016_56, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@5231da27}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653302310833311133822454_0004}; taskId=attempt_202412041653302310833311133822454_0004_m_000016_56, status=''}\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653305481321543453047061_0004}; taskId=attempt_202412041653305481321543453047061_0004_m_000005_45, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@281b55e7}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653305481321543453047061_0004}; taskId=attempt_202412041653305481321543453047061_0004_m_000005_45, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20241204165330160662144119117500_0004}; taskId=attempt_20241204165330160662144119117500_0004_m_000018_58, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@7e337fb6}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_20241204165330160662144119117500_0004}; taskId=attempt_20241204165330160662144119117500_0004_m_000018_58, status=''}\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653308993897262543194432_0004}; taskId=attempt_202412041653308993897262543194432_0004_m_000012_52, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@45793604}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653308993897262543194432_0004}; taskId=attempt_202412041653308993897262543194432_0004_m_000012_52, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653302569995319439503301_0004}; taskId=attempt_202412041653302569995319439503301_0004_m_000019_59, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@3112aaa3}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653302569995319439503301_0004}; taskId=attempt_202412041653302569995319439503301_0004_m_000019_59, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653301280816625551335318_0004}; taskId=attempt_202412041653301280816625551335318_0004_m_000004_44, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@66d4380e}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653301280816625551335318_0004}; taskId=attempt_202412041653301280816625551335318_0004_m_000004_44, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20241204165330160662144119117500_0004}; taskId=attempt_20241204165330160662144119117500_0004_m_000018_58, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@13d27d0}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_20241204165330160662144119117500_0004}; taskId=attempt_20241204165330160662144119117500_0004_m_000018_58, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653304722447511113489167_0004}; taskId=attempt_202412041653304722447511113489167_0004_m_000010_50, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@66035440}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653304722447511113489167_0004}; taskId=attempt_202412041653304722447511113489167_0004_m_000010_50, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653302569995319439503301_0004}; taskId=attempt_202412041653302569995319439503301_0004_m_000019_59, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@5c8bc9b0}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653302569995319439503301_0004}; taskId=attempt_202412041653302569995319439503301_0004_m_000019_59, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653308635078048294802408_0004}; taskId=attempt_202412041653308635078048294802408_0004_m_000013_53, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@1517d44f}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653308635078048294802408_0004}; taskId=attempt_202412041653308635078048294802408_0004_m_000013_53, status=''}\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653308635078048294802408_0004}; taskId=attempt_202412041653308635078048294802408_0004_m_000013_53, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@796769ad}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653308635078048294802408_0004}; taskId=attempt_202412041653308635078048294802408_0004_m_000013_53, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653302310833311133822454_0004}; taskId=attempt_202412041653302310833311133822454_0004_m_000016_56, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@6cc5ebc7}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653302310833311133822454_0004}; taskId=attempt_202412041653302310833311133822454_0004_m_000016_56, status=''}\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653301280816625551335318_0004}; taskId=attempt_202412041653301280816625551335318_0004_m_000004_44, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@75a2bfde}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653301280816625551335318_0004}; taskId=attempt_202412041653301280816625551335318_0004_m_000004_44, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306300299701037560990_0004}; taskId=attempt_202412041653306300299701037560990_0004_m_000011_51, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@3e5f1bde}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306300299701037560990_0004}; taskId=attempt_202412041653306300299701037560990_0004_m_000011_51, status=''}\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306300299701037560990_0004}; taskId=attempt_202412041653306300299701037560990_0004_m_000011_51, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@58c75f60}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306300299701037560990_0004}; taskId=attempt_202412041653306300299701037560990_0004_m_000011_51, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306914331393683665557_0004}; taskId=attempt_202412041653306914331393683665557_0004_m_000001_41, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@6c8f253c}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306914331393683665557_0004}; taskId=attempt_202412041653306914331393683665557_0004_m_000001_41, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653305721823965509162494_0004}; taskId=attempt_202412041653305721823965509162494_0004_m_000017_57, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@5868a463}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653305721823965509162494_0004}; taskId=attempt_202412041653305721823965509162494_0004_m_000017_57, status=''}\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306985848896609675936_0004}; taskId=attempt_202412041653306985848896609675936_0004_m_000000_40, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@404e6554}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306985848896609675936_0004}; taskId=attempt_202412041653306985848896609675936_0004_m_000000_40, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653305388066297851318446_0004}; taskId=attempt_202412041653305388066297851318446_0004_m_000015_55, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@78f9285e}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653305388066297851318446_0004}; taskId=attempt_202412041653305388066297851318446_0004_m_000015_55, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653305721823965509162494_0004}; taskId=attempt_202412041653305721823965509162494_0004_m_000017_57, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@1b89accc}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653305721823965509162494_0004}; taskId=attempt_202412041653305721823965509162494_0004_m_000017_57, status=''}\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306985848896609675936_0004}; taskId=attempt_202412041653306985848896609675936_0004_m_000000_40, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@7ab80c8a}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306985848896609675936_0004}; taskId=attempt_202412041653306985848896609675936_0004_m_000000_40, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653305388066297851318446_0004}; taskId=attempt_202412041653305388066297851318446_0004_m_000015_55, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@2e96545}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653305388066297851318446_0004}; taskId=attempt_202412041653305388066297851318446_0004_m_000015_55, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653308724400462210760176_0004}; taskId=attempt_202412041653308724400462210760176_0004_m_000006_46, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@5b6d87fa}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653308724400462210760176_0004}; taskId=attempt_202412041653308724400462210760176_0004_m_000006_46, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306970023585126218705_0004}; taskId=attempt_202412041653306970023585126218705_0004_m_000003_43, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@2a2d9979}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306970023585126218705_0004}; taskId=attempt_202412041653306970023585126218705_0004_m_000003_43, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653301935513722741494394_0004}; taskId=attempt_202412041653301935513722741494394_0004_m_000008_48, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@7e3ba265}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653301935513722741494394_0004}; taskId=attempt_202412041653301935513722741494394_0004_m_000008_48, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653308724400462210760176_0004}; taskId=attempt_202412041653308724400462210760176_0004_m_000006_46, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@3910fff8}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653308724400462210760176_0004}; taskId=attempt_202412041653308724400462210760176_0004_m_000006_46, status=''}\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306970023585126218705_0004}; taskId=attempt_202412041653306970023585126218705_0004_m_000003_43, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@6cafb0a5}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653306970023585126218705_0004}; taskId=attempt_202412041653306970023585126218705_0004_m_000003_43, status=''}\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653301935513722741494394_0004}; taskId=attempt_202412041653301935513722741494394_0004_m_000008_48, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@53293405}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653301935513722741494394_0004}; taskId=attempt_202412041653301935513722741494394_0004_m_000008_48, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20241204165330338142509441298402_0004}; taskId=attempt_20241204165330338142509441298402_0004_m_000014_54, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@17ea549}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_20241204165330338142509441298402_0004}; taskId=attempt_20241204165330338142509441298402_0004_m_000014_54, status=''}\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2024120416533041848328459808970_0004}; taskId=attempt_2024120416533041848328459808970_0004_m_000009_49, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@415a8902}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_2024120416533041848328459808970_0004}; taskId=attempt_2024120416533041848328459808970_0004_m_000009_49, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20241204165330338142509441298402_0004}; taskId=attempt_20241204165330338142509441298402_0004_m_000014_54, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@7a059541}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_20241204165330338142509441298402_0004}; taskId=attempt_20241204165330338142509441298402_0004_m_000014_54, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653303839374627966127542_0004}; taskId=attempt_202412041653303839374627966127542_0004_m_000002_42, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@793de630}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653303839374627966127542_0004}; taskId=attempt_202412041653303839374627966127542_0004_m_000002_42, status=''}\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_2024120416533041848328459808970_0004}; taskId=attempt_2024120416533041848328459808970_0004_m_000009_49, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@70c84c3f}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_2024120416533041848328459808970_0004}; taskId=attempt_2024120416533041848328459808970_0004_m_000009_49, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653303839374627966127542_0004}; taskId=attempt_202412041653303839374627966127542_0004_m_000002_42, status=''}; org.apache.parquet.hadoop.ParquetOutputCommitter@53521085}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path hdfs://hadoop:9000/tmp/test and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653303839374627966127542_0004}; taskId=attempt_202412041653303839374627966127542_0004_m_000002_42, status=''}\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n",
      "24/12/04 16:53:30 INFO CodecConfig: Compression: SNAPPY\n",
      "24/12/04 16:53:30 INFO CodecConfig: Compression: SNAPPY\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 42, boot = -604, init = 646, finish = 0\n",
      "24/12/04 16:53:30 INFO CodecConfig: Compression: SNAPPY\n",
      "24/12/04 16:53:30 INFO CodecConfig: Compression: SNAPPY\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 45, boot = -605, init = 650, finish = 0\n",
      "24/12/04 16:53:30 INFO CodecConfig: Compression: SNAPPY\n",
      "24/12/04 16:53:30 INFO CodecConfig: Compression: SNAPPY\n",
      "24/12/04 16:53:30 INFO CodecConfig: Compression: SNAPPY\n",
      "24/12/04 16:53:30 INFO CodecConfig: Compression: SNAPPY\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #4 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #5 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #4\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #5\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 2ms\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 2ms\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653306914331393683665557_0004_m_000001_41\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653304722447511113489167_0004_m_000010_50\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 44, boot = -534, init = 578, finish = 0\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #6 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 42, boot = -536, init = 578, finish = 0\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 55, boot = -526, init = 581, finish = 0\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #6\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 2ms\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653308993897262543194432_0004_m_000012_52\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #7 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 46, boot = -541, init = 587, finish = 0\n",
      "24/12/04 16:53:30 INFO CodecConfig: Compression: SNAPPY\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 47, boot = -538, init = 585, finish = 0\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 45, boot = -542, init = 587, finish = 0\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 55, boot = -529, init = 584, finish = 0\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #9 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #8 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 INFO CodecConfig: Compression: SNAPPY\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #7\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #9\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #8\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 8ms\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653301280816625551335318_0004_m_000004_44\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #10 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 18ms\n",
      "24/12/04 16:53:30 INFO CodecConfig: Compression: SNAPPY\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653308724400462210760176_0004_m_000006_46\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 60, boot = -534, init = 594, finish = 0\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 43, boot = -546, init = 589, finish = 0\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 47, boot = -543, init = 590, finish = 0\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 11ms\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 43, boot = -547, init = 590, finish = 0\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 45, boot = -547, init = 592, finish = 0\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20241204165330160662144119117500_0004_m_000018_58\n",
      "24/12/04 16:53:30 INFO CodecConfig: Compression: SNAPPY\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #14 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #10\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #16 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #11 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 23ms\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #14\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 3ms\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #15 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653308635078048294802408_0004_m_000013_53\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_2024120416533041848328459808970_0004_m_000009_49\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #13 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #16\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 3ms\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #11\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 23ms\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653305721823965509162494_0004_m_000017_57\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653302310833311133822454_0004_m_000016_56\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #15\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 5ms\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #12 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653303839374627966127542_0004_m_000002_42\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #13\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #17 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 7ms\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #12\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 8ms\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653305481321543453047061_0004_m_000005_45\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653301935513722741494394_0004_m_000008_48\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #17\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 2ms\n",
      "24/12/04 16:53:30 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_20241204165330338142509441298402_0004_m_000014_54\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 6.0 in stage 4.0 (TID 46). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 18.0 in stage 4.0 (TID 58). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 9.0 in stage 4.0 (TID 49). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 13.0 in stage 4.0 (TID 53). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 2.0 in stage 4.0 (TID 42). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 19\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 18\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 1.0 in stage 4.0 (TID 41). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 17\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 4.0 in stage 4.0 (TID 44). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 16\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 15\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 12.0 in stage 4.0 (TID 52). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 14\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 13\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 12\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 10.0 in stage 4.0 (TID 50). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 11\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 14.0 in stage 4.0 (TID 54). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 10\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 17.0 in stage 4.0 (TID 57). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 8.0 in stage 4.0 (TID 48). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 9\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 8\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 5.0 in stage 4.0 (TID 45). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 7\n",
      "24/12/04 16:53:30 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_4.0, runningTasks: 19\n",
      "24/12/04 16:53:30 INFO Executor: Finished task 16.0 in stage 4.0 (TID 56). 2828 bytes result sent to driver\n",
      "24/12/04 16:53:30 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 6\n",
      "24/12/04 16:53:30 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 18.0 in stage 4.0 (TID 58) in 152 ms on hadoop113 (executor driver) (1/20)\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 6.0 in stage 4.0 (TID 46) in 155 ms on hadoop113 (executor driver) (2/20)\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 42) in 155 ms on hadoop113 (executor driver) (3/20)\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 9.0 in stage 4.0 (TID 49) in 155 ms on hadoop113 (executor driver) (4/20)\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 41) in 157 ms on hadoop113 (executor driver) (5/20)\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 4.0 in stage 4.0 (TID 44) in 156 ms on hadoop113 (executor driver) (6/20)\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 13.0 in stage 4.0 (TID 53) in 154 ms on hadoop113 (executor driver) (7/20)\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 12.0 in stage 4.0 (TID 52) in 155 ms on hadoop113 (executor driver) (8/20)\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 14.0 in stage 4.0 (TID 54) in 155 ms on hadoop113 (executor driver) (9/20)\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 10.0 in stage 4.0 (TID 50) in 156 ms on hadoop113 (executor driver) (10/20)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@35c086f0)\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 17.0 in stage 4.0 (TID 57) in 155 ms on hadoop113 (executor driver) (11/20)\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 16.0 in stage 4.0 (TID 56) in 156 ms on hadoop113 (executor driver) (12/20)\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 8.0 in stage 4.0 (TID 48) in 157 ms on hadoop113 (executor driver) (13/20)\n",
      "24/12/04 16:53:30 INFO TaskSetManager: Finished task 5.0 in stage 4.0 (TID 45) in 158 ms on hadoop113 (executor driver) (14/20)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@7f2b08a2)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@12c6dc3b)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@69704abb)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@58b6b663)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@7f7fcbcf)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@5b7cf71f)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@30b25b59)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@5d5af682)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@5c7a7f55)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@6082afca)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@42484810)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@48ac483d)\n",
      "24/12/04 16:53:30 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@472316aa)\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Validation is off\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Validation is off\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Parquet block size to 134217728\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Validation is off\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Validation is off\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Validation is off\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Validation is off\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Maximum row group padding size is 8388608 bytes\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "24/12/04 16:53:30 INFO ParquetOutputFormat: Parquet properties are:\n",
      "Parquet page size to 1048576\n",
      "Parquet dictionary page size to 1048576\n",
      "Dictionary is true\n",
      "Writer version is: PARQUET_1_0\n",
      "Page size checking is: estimated\n",
      "Min row count for page size check is: 100\n",
      "Max row count for page size check is: 10000\n",
      "Truncate length for column indexes is: 64\n",
      "Truncate length for statistics min/max  is: 2147483647\n",
      "Bloom filter enabled: false\n",
      "Max Bloom filter size for a column is 1048576\n",
      "Bloom filter expected number of distinct values are: null\n",
      "Page row count limit to 20000\n",
      "Writing page checksums is: on\n",
      "24/12/04 16:53:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/12/04 16:53:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/12/04 16:53:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/12/04 16:53:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/12/04 16:53:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/12/04 16:53:30 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n",
      "{\n",
      "  \"type\" : \"struct\",\n",
      "  \"fields\" : [ {\n",
      "    \"name\" : \"id\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"city\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  }, {\n",
      "    \"name\" : \"name\",\n",
      "    \"type\" : \"string\",\n",
      "    \"nullable\" : true,\n",
      "    \"metadata\" : { }\n",
      "  } ]\n",
      "}\n",
      "and corresponding Parquet message type:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "       \n",
      "24/12/04 16:53:30 DEBUG EncryptionPropertiesFactory: EncryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:30 DEBUG EncryptionPropertiesFactory: EncryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:30 DEBUG EncryptionPropertiesFactory: EncryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:30 DEBUG EncryptionPropertiesFactory: EncryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:30 DEBUG EncryptionPropertiesFactory: EncryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:30 DEBUG EncryptionPropertiesFactory: EncryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:30 DEBUG DFSClient: /tmp/test/_temporary/0/_temporary/attempt_202412041653306300299701037560990_0004_m_000011_51/part-00011-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet: masked={ masked: rw-r--r--, unmasked: rw-rw-rw- }\n",
      "24/12/04 16:53:30 DEBUG DFSClient: /tmp/test/_temporary/0/_temporary/attempt_202412041653306970023585126218705_0004_m_000003_43/part-00003-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet: masked={ masked: rw-r--r--, unmasked: rw-rw-rw- }\n",
      "24/12/04 16:53:30 DEBUG DFSClient: /tmp/test/_temporary/0/_temporary/attempt_202412041653305388066297851318446_0004_m_000015_55/part-00015-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet: masked={ masked: rw-r--r--, unmasked: rw-rw-rw- }\n",
      "24/12/04 16:53:30 DEBUG DFSClient: /tmp/test/_temporary/0/_temporary/attempt_202412041653306985848896609675936_0004_m_000000_40/part-00000-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet: masked={ masked: rw-r--r--, unmasked: rw-rw-rw- }\n",
      "24/12/04 16:53:30 DEBUG DFSClient: /tmp/test/_temporary/0/_temporary/attempt_202412041653301503196464597393543_0004_m_000007_47/part-00007-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet: masked={ masked: rw-r--r--, unmasked: rw-rw-rw- }\n",
      "24/12/04 16:53:30 DEBUG DFSClient: /tmp/test/_temporary/0/_temporary/attempt_202412041653302569995319439503301_0004_m_000019_59/part-00019-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet: masked={ masked: rw-r--r--, unmasked: rw-rw-rw- }\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #18 org.apache.hadoop.hdfs.protocol.ClientProtocol.create\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #20 org.apache.hadoop.hdfs.protocol.ClientProtocol.create\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #22 org.apache.hadoop.hdfs.protocol.ClientProtocol.create\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #23 org.apache.hadoop.hdfs.protocol.ClientProtocol.create\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #19 org.apache.hadoop.hdfs.protocol.ClientProtocol.create\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #21 org.apache.hadoop.hdfs.protocol.ClientProtocol.create\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #18\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: create took 4ms\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #20\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #22\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: create took 7ms\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #23\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: create took 7ms\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #19\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: create took 7ms\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: create took 7ms\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #21\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: create took 7ms\n",
      "24/12/04 16:53:30 DEBUG DFSClient: computePacketChunkSize: src=/tmp/test/_temporary/0/_temporary/attempt_202412041653306300299701037560990_0004_m_000011_51/part-00011-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, chunkSize=516, chunksPerPacket=126, packetSize=65016\n",
      "24/12/04 16:53:30 DEBUG DFSClient: computePacketChunkSize: src=/tmp/test/_temporary/0/_temporary/attempt_202412041653306985848896609675936_0004_m_000000_40/part-00000-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, chunkSize=516, chunksPerPacket=126, packetSize=65016\n",
      "24/12/04 16:53:30 DEBUG DFSClient: computePacketChunkSize: src=/tmp/test/_temporary/0/_temporary/attempt_202412041653301503196464597393543_0004_m_000007_47/part-00007-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, chunkSize=516, chunksPerPacket=126, packetSize=65016\n",
      "24/12/04 16:53:30 DEBUG DFSClient: computePacketChunkSize: src=/tmp/test/_temporary/0/_temporary/attempt_202412041653305388066297851318446_0004_m_000015_55/part-00015-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, chunkSize=516, chunksPerPacket=126, packetSize=65016\n",
      "24/12/04 16:53:30 DEBUG DFSClient: computePacketChunkSize: src=/tmp/test/_temporary/0/_temporary/attempt_202412041653302569995319439503301_0004_m_000019_59/part-00019-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, chunkSize=516, chunksPerPacket=126, packetSize=65016\n",
      "24/12/04 16:53:30 DEBUG DFSClient: computePacketChunkSize: src=/tmp/test/_temporary/0/_temporary/attempt_202412041653306970023585126218705_0004_m_000003_43/part-00003-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, chunkSize=516, chunksPerPacket=126, packetSize=65016\n",
      "24/12/04 16:53:30 DEBUG LeaseRenewer: Lease renewer daemon for [DFSClient_NONMAPREDUCE_1786049293_24] with renew id 1 started\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 0: start\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 0: start\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 0: start\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 0: start\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 0: start\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 0: start\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Allocated total memory pool is: 906992014\n",
      "24/12/04 16:53:30 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/12/04 16:53:30 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/12/04 16:53:30 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/12/04 16:53:30 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/12/04 16:53:30 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/12/04 16:53:30 INFO CodecPool: Got brand-new compressor [.snappy]\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 1 initialCapacity 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@53915e1c\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@662c3ca4\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@53915e1c\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@7bd4d38\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@662c3ca4\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@53915e1c\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@7bd4d38\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@662c3ca4\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@7a5af103\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@53915e1c\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- start message -->\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: < MESSAGE START >\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@64d073b4\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- start message -->\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- start message -->\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@7bd4d38\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: < MESSAGE START >\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: < MESSAGE START >\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@662c3ca4\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@7a5af103\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- start message -->\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@53915e1c\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: < MESSAGE START >\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- start message -->\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: < MESSAGE START >\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@64d073b4\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <id>\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@7bd4d38\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <id>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(id, 0)\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@662c3ca4\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@7a5af103\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(id, 0)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [id] r:0\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@15d06af9\n",
      "24/12/04 16:53:30 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@53915e1c\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [id] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <id>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(id, 0)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <id>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [id] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(id, 0)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <id>\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 42, boot = -538, init = 580, finish = 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [id] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(id, 0)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [id] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- flush -->\n",
      "24/12/04 16:53:30 DEBUG InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 0\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: column indexes\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: offset indexes\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: bloom filters\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: end\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [50]\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [54]\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [51]\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(1 bytes)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [52]\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(1 bytes)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [53]\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(1 bytes)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(1 bytes)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(1 bytes)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [id] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [id] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [id] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </id>\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </id>\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </id>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [id] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(id, 0)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={}}: [id] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(id, 0)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(id, 0)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </id>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <city>\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </id>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(city, 1)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <city>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(id, 0)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(city, 1)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(id, 0)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [city] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [city] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [84, 101, 108, 97, 110, 103, 97, 110, 97]\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [72, 121, 100, 101, 114, 97, 98, 97, 100]\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(9 bytes)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <city>\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <city>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(9 bytes)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <city>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [city] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(city, 1)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(city, 1)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </city>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [city] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(city, 1)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(city, 1)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </city>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [city] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [city] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(city, 1)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [city] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <name>\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [75, 101, 114, 97, 108, 97]\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(name, 2)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [83, 105, 107, 107, 105, 109]\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <name>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(6 bytes)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [name] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [77, 105, 122, 111, 114, 97, 109]\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(name, 2)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(6 bytes)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [67, 104, 104, 101, 116, 114, 105]\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [name] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(7 bytes)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(7 bytes)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [83, 104, 97, 98, 98, 105, 114]\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(7 bytes)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [city] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [name] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </city>\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </name>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [city] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0}}: [city] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [name] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(name, 2)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(city, 1)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </name>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1, 2}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </city>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(name, 2)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </city>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1, 2}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(city, 1)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: < MESSAGE END >\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: < MESSAGE END >\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(city, 1)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1, 2}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1, 2}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- end message -->\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- end message -->\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <name>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <name>\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <name>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(name, 2)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(name, 2)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: startField(name, 2)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [name] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [name] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [66, 104, 117, 116, 105, 97]\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [name] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(6 bytes)\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [76, 97, 108, 112, 101, 107, 104, 108, 117, 97]\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: [86, 105, 106, 97, 121, 97, 110]\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(10 bytes)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: addBinary(7 bytes)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: r: 0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [name] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [name] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1}}: [name] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </name>\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </name>\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: </name>\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(name, 2)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(name, 2)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: endField(name, 2)\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1, 2}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1, 2}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1, 2}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: < MESSAGE END >\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: < MESSAGE END >\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: < MESSAGE END >\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1, 2}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1, 2}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG MessageColumnIO: 0, VistedIndex{vistedIndexes={0, 1, 2}}: [] r:0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- end message -->\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- end message -->\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- end message -->\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 43, boot = -610, init = 653, finish = 0\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 58, boot = -608, init = 665, finish = 1\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 42, boot = -761, init = 803, finish = 0\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 42, boot = -531, init = 573, finish = 0\n",
      "24/12/04 16:53:30 INFO PythonRunner: Times: total = 53, boot = -534, init = 587, finish = 0\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- flush -->\n",
      "24/12/04 16:53:30 DEBUG InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 41\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: start block\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- flush -->\n",
      "24/12/04 16:53:30 DEBUG InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 41\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: start block\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- flush -->\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- flush -->\n",
      "24/12/04 16:53:30 DEBUG RecordConsumerLoggingWrapper: <!-- flush -->\n",
      "24/12/04 16:53:30 DEBUG InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 38\n",
      "24/12/04 16:53:30 DEBUG InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 37\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: start block\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: start block\n",
      "24/12/04 16:53:30 DEBUG InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 42\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: start block\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 13\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 11\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 10\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 13\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 10\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 13 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 13 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 10 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 10 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 11 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 18 to byteArray of 18 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 19 to byteArray of 19 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 21 to byteArray of 21 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 18 to byteArray of 18 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 21 to byteArray of 21 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 21 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 44 to byteArray of 44 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 18 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 18 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 41 to byteArray of 41 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 19 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 21 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 41 to byteArray of 41 bytes\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 42 to byteArray of 42 bytes\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 44 to byteArray of 44 bytes\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 5\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 5\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 5\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 5\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 5 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 13 to byteArray of 13 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 5 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 5\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 5 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 13 to byteArray of 13 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 13 to byteArray of 13 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 13 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 36 to byteArray of 36 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 13 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 13 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 36 to byteArray of 36 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 36 to byteArray of 36 bytes\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 5 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 13 to byteArray of 13 bytes\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 5 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 13 to byteArray of 13 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 13 bytes to out\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 11\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 36 to byteArray of 36 bytes\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 11\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 11\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 13 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 11 bytes to out\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 36 to byteArray of 36 bytes\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 11 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 19 to byteArray of 19 bytes\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 11 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 19 to byteArray of 19 bytes\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 19 to byteArray of 19 bytes\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 64\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 19 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 19 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 42 to byteArray of 42 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 42 to byteArray of 42 bytes\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: max dic id 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 22 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 19 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 41 to byteArray of 41 bytes\n",
      "24/12/04 16:53:30 DEBUG RunLengthBitPackingHybridEncoder: Encoding: RunLengthBitPackingHybridEncoder with bithWidth: 0 initialCapacity 1024\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 10\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG DictionaryValuesWriter: rle encoded bytes 1\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput: BytesInput from array of 1 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: write data pages content\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: used 0 slabs, adding new slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG PlainValuesWriter: writing a buffer of size 14\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 10 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 18 to byteArray of 18 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 0 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 6 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: {\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 4 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 2 => 2 0 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 2 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 18 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: }\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 41 to byteArray of 41 bytes\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 14 bytes to out\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 22 to byteArray of 22 bytes\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: end column\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: end column\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: end column\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: end column\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 23 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput$SequenceBytesIn: write 22 bytes to out\n",
      "24/12/04 16:53:30 DEBUG BytesInput: converted 45 to byteArray of 45 bytes\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 64\n",
      "24/12/04 16:53:30 DEBUG CapacityByteArrayOutputStream: initial slab of size 1024\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 4: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: end column\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 36B for [id] optional binary id (STRING): 1 values, 11B raw, 13B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 36B for [id] optional binary id (STRING): 1 values, 11B raw, 13B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 36B for [id] optional binary id (STRING): 1 values, 11B raw, 13B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 36B for [id] optional binary id (STRING): 1 values, 11B raw, 13B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 36B for [id] optional binary id (STRING): 1 values, 11B raw, 13B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 82: end column\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 81: end column\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 40: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 84: end column\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 84: end column\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 81: end column\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 42B for [city] optional binary city (STRING): 1 values, 17B raw, 19B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 41B for [city] optional binary city (STRING): 1 values, 16B raw, 18B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 44B for [city] optional binary city (STRING): 1 values, 19B raw, 21B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 82: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 81: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 82: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 84: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 81: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 84: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 127: end column\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 123: end column\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 126: end column\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 45B for [name] optional binary name (STRING): 1 values, 20B raw, 22B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 44B for [city] optional binary city (STRING): 1 values, 19B raw, 21B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 42B for [name] optional binary name (STRING): 1 values, 17B raw, 19B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 42B for [name] optional binary name (STRING): 1 values, 17B raw, 19B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 41B for [city] optional binary city (STRING): 1 values, 16B raw, 18B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 127: end block\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 126: end block\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 123: end block\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 84: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 81: write data pages\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 84: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 81: write data pages content\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 125: end column\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 122: end column\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 123: column indexes\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 127: column indexes\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 126: column indexes\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 41B for [name] optional binary name (STRING): 1 values, 17B raw, 19B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ColumnChunkPageWriteStore: written 41B for [name] optional binary name (STRING): 1 values, 16B raw, 18B comp, 1 pages, encodings: [PLAIN]\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 125: end block\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 122: end block\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 125: column indexes\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 122: column indexes\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 205: offset indexes\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 208: offset indexes\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 193: offset indexes\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 196: offset indexes\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 204: offset indexes\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 236: bloom filters\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 235: bloom filters\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 235: end\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 236: end\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 227: bloom filters\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 239: bloom filters\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 227: end\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 239: end\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 224: bloom filters\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 224: end\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 461: footer length = 457\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 457 => 201 1 0 0\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 963: footer length = 724\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 956: footer length = 720\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 724 => 212 2 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 720 => 208 2 0 0\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 955: footer length = 720\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 928: footer length = 704\n",
      "24/12/04 16:53:30 DEBUG ParquetFileWriter: 935: footer length = 708\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 720 => 208 2 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 708 => 196 2 0 0\n",
      "24/12/04 16:53:30 DEBUG BytesUtils: write le int: 704 => 192 2 0 0\n",
      "24/12/04 16:53:30 DEBUG DFSClient: WriteChunk allocating new packet seqno=0, src=/tmp/test/_temporary/0/_temporary/attempt_202412041653306970023585126218705_0004_m_000003_43/part-00003-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0, output stream=DFSOutputStream:block==null\n",
      "24/12/04 16:53:30 DEBUG DFSClient: WriteChunk allocating new packet seqno=0, src=/tmp/test/_temporary/0/_temporary/attempt_202412041653302569995319439503301_0004_m_000019_59/part-00019-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0, output stream=DFSOutputStream:block==null\n",
      "24/12/04 16:53:30 DEBUG DFSClient: WriteChunk allocating new packet seqno=0, src=/tmp/test/_temporary/0/_temporary/attempt_202412041653306985848896609675936_0004_m_000000_40/part-00000-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0, output stream=DFSOutputStream:block==null\n",
      "24/12/04 16:53:30 DEBUG DFSClient: WriteChunk allocating new packet seqno=0, src=/tmp/test/_temporary/0/_temporary/attempt_202412041653306300299701037560990_0004_m_000011_51/part-00011-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0, output stream=DFSOutputStream:block==null\n",
      "24/12/04 16:53:30 DEBUG DFSClient: WriteChunk allocating new packet seqno=0, src=/tmp/test/_temporary/0/_temporary/attempt_202412041653301503196464597393543_0004_m_000007_47/part-00007-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0, output stream=DFSOutputStream:block==null\n",
      "24/12/04 16:53:30 DEBUG DFSClient: WriteChunk allocating new packet seqno=0, src=/tmp/test/_temporary/0/_temporary/attempt_202412041653305388066297851318446_0004_m_000015_55/part-00015-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, packetSize=65016, chunksPerPacket=126, bytesCurBlock=0, output stream=DFSOutputStream:block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Queued packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 964, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Queued packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 971, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Queued packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 469, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Queued packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 936, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Queued packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 943, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Queued packet seqno: 1 offsetInBlock: 964 lastPacketInBlock: true lastByteOffsetInBlock: 964, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Queued packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 963, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Queued packet seqno: 1 offsetInBlock: 469 lastPacketInBlock: true lastByteOffsetInBlock: 469, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Queued packet seqno: 1 offsetInBlock: 943 lastPacketInBlock: true lastByteOffsetInBlock: 943, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: block==null waiting for ack for: 1\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Queued packet seqno: 1 offsetInBlock: 971 lastPacketInBlock: true lastByteOffsetInBlock: 971, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: block==null waiting for ack for: 1\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: block==null waiting for ack for: 1\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: block==null waiting for ack for: 1\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Queued packet seqno: 1 offsetInBlock: 963 lastPacketInBlock: true lastByteOffsetInBlock: 963, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: block==null waiting for ack for: 1\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Queued packet seqno: 1 offsetInBlock: 936 lastPacketInBlock: true lastByteOffsetInBlock: 936, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: block==null waiting for ack for: 1\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: stage=PIPELINE_SETUP_CREATE, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: stage=PIPELINE_SETUP_CREATE, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Allocating new block: block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: stage=PIPELINE_SETUP_CREATE, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: stage=PIPELINE_SETUP_CREATE, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: stage=PIPELINE_SETUP_CREATE, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Allocating new block: block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Allocating new block: block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: stage=PIPELINE_SETUP_CREATE, block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Allocating new block: block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Allocating new block: block==null\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Allocating new block: block==null\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #24 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #29 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #26 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #28 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #27 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #25 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #24\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #29\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: addBlock took 6ms\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #26\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: addBlock took 6ms\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #28\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: addBlock took 6ms\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #27\n",
      "24/12/04 16:53:30 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #25\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: addBlock took 6ms\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: addBlock took 6ms\n",
      "24/12/04 16:53:30 DEBUG ProtobufRpcEngine2: Call: addBlock took 6ms\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: pipeline = [DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]], blk_1073741992_1168\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: pipeline = [DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]], blk_1073741994_1170\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: pipeline = [DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]], blk_1073741991_1167\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: pipeline = [DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]], blk_1073741995_1171\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: pipeline = [DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]], blk_1073741996_1172\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: pipeline = [DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]], blk_1073741993_1169\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Send buf size 43520\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Send buf size 43520\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Send buf size 43520\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Send buf size 43520\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Send buf size 43520\n",
      "24/12/04 16:53:30 DEBUG DataStreamer: Send buf size 43520\n",
      "24/12/04 16:53:30 DEBUG SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "24/12/04 16:53:30 DEBUG SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "24/12/04 16:53:30 DEBUG SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "24/12/04 16:53:30 DEBUG SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "24/12/04 16:53:30 DEBUG SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "24/12/04 16:53:30 DEBUG SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #30 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #34 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #32 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #30\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getServerDefaults took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #34\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #35 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getServerDefaults took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #32\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #33 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getServerDefaults took 2ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #35\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getServerDefaults took 2ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #31 org.apache.hadoop.hdfs.protocol.ClientProtocol.getServerDefaults\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #33\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getServerDefaults took 2ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #31\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getServerDefaults took 2ms\n",
      "24/12/04 16:53:31 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.25.1.12, datanodeId = DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]\n",
      "24/12/04 16:53:31 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.25.1.12, datanodeId = DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]\n",
      "24/12/04 16:53:31 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.25.1.12, datanodeId = DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]\n",
      "24/12/04 16:53:31 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.25.1.12, datanodeId = DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]\n",
      "24/12/04 16:53:31 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.25.1.12, datanodeId = DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]\n",
      "24/12/04 16:53:31 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.25.1.12, datanodeId = DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: nodes [DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]] storageTypes [DISK] storageIDs [DS-0b947a4d-f204-46b4-8552-ce209f80a8f9]\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: nodes [DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]] storageTypes [DISK] storageIDs [DS-0b947a4d-f204-46b4-8552-ce209f80a8f9]\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: nodes [DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]] storageTypes [DISK] storageIDs [DS-0b947a4d-f204-46b4-8552-ce209f80a8f9]\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: nodes [DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]] storageTypes [DISK] storageIDs [DS-0b947a4d-f204-46b4-8552-ce209f80a8f9]\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: nodes [DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]] storageTypes [DISK] storageIDs [DS-0b947a4d-f204-46b4-8552-ce209f80a8f9]\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: nodes [DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]] storageTypes [DISK] storageIDs [DS-0b947a4d-f204-46b4-8552-ce209f80a8f9]\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: blk_1073741994_1170 sending packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 469\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: blk_1073741995_1171 sending packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 971\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: blk_1073741993_1169 sending packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 963\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: blk_1073741991_1167 sending packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 936\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: blk_1073741996_1172 sending packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 943\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: blk_1073741992_1168 sending packet seqno: 0 offsetInBlock: 0 lastPacketInBlock: false lastByteOffsetInBlock: 964\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073741994_1170\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073741993_1169\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073741996_1172\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073741992_1168\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073741991_1167\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: stage=DATA_STREAMING, blk_1073741995_1171\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: DFSClient seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: DFSClient seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: DFSClient seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: DFSClient seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: DFSClient seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: DFSClient seqno: 0 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: blk_1073741996_1172 sending packet seqno: 1 offsetInBlock: 943 lastPacketInBlock: true lastByteOffsetInBlock: 943\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: blk_1073741993_1169 sending packet seqno: 1 offsetInBlock: 963 lastPacketInBlock: true lastByteOffsetInBlock: 963\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: blk_1073741992_1168 sending packet seqno: 1 offsetInBlock: 964 lastPacketInBlock: true lastByteOffsetInBlock: 964\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: blk_1073741995_1171 sending packet seqno: 1 offsetInBlock: 971 lastPacketInBlock: true lastByteOffsetInBlock: 971\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: blk_1073741994_1170 sending packet seqno: 1 offsetInBlock: 469 lastPacketInBlock: true lastByteOffsetInBlock: 469\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: blk_1073741991_1167 sending packet seqno: 1 offsetInBlock: 936 lastPacketInBlock: true lastByteOffsetInBlock: 936\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: DFSClient seqno: 1 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: Closing old block BP-71074493-127.0.0.1-1733217510273:blk_1073741994_1170\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: DFSClient seqno: 1 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: Closing old block BP-71074493-127.0.0.1-1733217510273:blk_1073741993_1169\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: DFSClient seqno: 1 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: Closing old block BP-71074493-127.0.0.1-1733217510273:blk_1073741992_1168\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: DFSClient seqno: 1 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: Closing old block BP-71074493-127.0.0.1-1733217510273:blk_1073741995_1171\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #37 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #39 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #36 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: DFSClient seqno: 1 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #38 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: DFSClient seqno: 1 reply: SUCCESS downstreamAckTimeNanos: 0 flag: 0\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: Closing old block BP-71074493-127.0.0.1-1733217510273:blk_1073741991_1167\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: Closing old block BP-71074493-127.0.0.1-1733217510273:blk_1073741996_1172\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #40 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #41 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #37\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: complete took 3ms\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@64d073b4\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@7bd4d38\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@662c3ca4\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@15d06af9\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@53915e1c\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #42 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #39\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #36\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: complete took 4ms\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: complete took 5ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #38\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #40\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: complete took 5ms\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: complete took 3ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #41\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: complete took 3ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #42\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@64d073b4\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@7bd4d38\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@662c3ca4\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@53915e1c\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@64d073b4\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@662c3ca4\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@53915e1c\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@64d073b4\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@662c3ca4\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #43 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG MemoryManager: Adjust block size from 134,217,728 to 134,217,728 for writer: org.apache.parquet.hadoop.InternalParquetRecordWriter@64d073b4\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #44 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #45 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #43\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #46 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #44\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #47 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #45\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #46\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #48 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #47\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #49 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #50 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #48\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #49\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #51 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #53 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #50\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #51\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #52 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #53\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #52\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG OutputCommitCoordinator: Commit allowed for stage=4.0, partition=19, task attempt 0\n",
      "24/12/04 16:53:31 DEBUG OutputCommitCoordinator: Commit allowed for stage=4.0, partition=15, task attempt 0\n",
      "24/12/04 16:53:31 DEBUG OutputCommitCoordinator: Commit allowed for stage=4.0, partition=3, task attempt 0\n",
      "24/12/04 16:53:31 DEBUG OutputCommitCoordinator: Commit allowed for stage=4.0, partition=0, task attempt 0\n",
      "24/12/04 16:53:31 DEBUG OutputCommitCoordinator: Commit allowed for stage=4.0, partition=7, task attempt 0\n",
      "24/12/04 16:53:31 DEBUG OutputCommitCoordinator: Commit allowed for stage=4.0, partition=11, task attempt 0\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #54 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #59 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #58 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #56 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #57 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #55 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #54\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #58\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #59\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #57\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #56\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #55\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #60 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #61 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #65 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #60\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #61\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #63 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #64 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #65\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #62 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #63\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #64\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #62\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #66 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #67 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #68 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #69 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #70 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #71 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #66\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: rename took 3ms\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: Saved output of task 'attempt_202412041653305388066297851318446_0004_m_000015_55' to hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653305388066297851318446_0004_m_000015\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: attempt_202412041653305388066297851318446_0004_m_000015_55: Committed. Elapsed time: 8 ms.\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #67\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #68\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #69\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: rename took 5ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #70\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 15.0 in stage 4.0 (TID 55). 2914 bytes result sent to driver\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #71\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: rename took 5ms\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: rename took 5ms\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: rename took 5ms\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: rename took 5ms\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: Saved output of task 'attempt_202412041653306300299701037560990_0004_m_000011_51' to hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306300299701037560990_0004_m_000011\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 5\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: Saved output of task 'attempt_202412041653306970023585126218705_0004_m_000003_43' to hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306970023585126218705_0004_m_000003\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: Saved output of task 'attempt_202412041653302569995319439503301_0004_m_000019_59' to hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653302569995319439503301_0004_m_000019\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: Saved output of task 'attempt_202412041653306985848896609675936_0004_m_000000_40' to hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306985848896609675936_0004_m_000000\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: Saved output of task 'attempt_202412041653301503196464597393543_0004_m_000007_47' to hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653301503196464597393543_0004_m_000007\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: attempt_202412041653306300299701037560990_0004_m_000011_51: Committed. Elapsed time: 9 ms.\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: attempt_202412041653301503196464597393543_0004_m_000007_47: Committed. Elapsed time: 9 ms.\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: attempt_202412041653306970023585126218705_0004_m_000003_43: Committed. Elapsed time: 9 ms.\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: attempt_202412041653302569995319439503301_0004_m_000019_59: Committed. Elapsed time: 9 ms.\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: attempt_202412041653306985848896609675936_0004_m_000000_40: Committed. Elapsed time: 9 ms.\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 15.0 in stage 4.0 (TID 55) in 653 ms on hadoop113 (executor driver) (15/20)\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 11.0 in stage 4.0 (TID 51). 2914 bytes result sent to driver\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 0.0 in stage 4.0 (TID 40). 2871 bytes result sent to driver\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 3.0 in stage 4.0 (TID 43). 2914 bytes result sent to driver\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 4\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 7.0 in stage 4.0 (TID 47). 2914 bytes result sent to driver\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 19.0 in stage 4.0 (TID 59). 2914 bytes result sent to driver\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 3\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 2\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 1\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (4, 0) -> 0\n",
      "24/12/04 16:53:31 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@322a1d7b)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 40) in 657 ms on hadoop113 (executor driver) (16/20)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 19.0 in stage 4.0 (TID 59) in 653 ms on hadoop113 (executor driver) (17/20)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 7.0 in stage 4.0 (TID 47) in 655 ms on hadoop113 (executor driver) (18/20)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 11.0 in stage 4.0 (TID 51) in 655 ms on hadoop113 (executor driver) (19/20)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 43) in 656 ms on hadoop113 (executor driver) (20/20)\n",
      "24/12/04 16:53:31 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@3222cbe6)\n",
      "24/12/04 16:53:31 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/12/04 16:53:31 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@18e5903f)\n",
      "24/12/04 16:53:31 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@472c53cf)\n",
      "24/12/04 16:53:31 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@250f4f94)\n",
      "24/12/04 16:53:31 INFO DAGScheduler: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.683 s\n",
      "24/12/04 16:53:31 DEBUG DAGScheduler: After removal of stage 4, remaining stages = 0\n",
      "24/12/04 16:53:31 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/04 16:53:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/12/04 16:53:31 DEBUG SQLHadoopMapReduceCommitProtocol: onTaskCommit(org.apache.spark.internal.io.FileCommitProtocol$TaskCommitMessage@1d6cd28)\n",
      "24/12/04 16:53:31 INFO DAGScheduler: Job 4 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.689391 s\n",
      "24/12/04 16:53:31 INFO FileFormatWriter: Start to commit write Job f8415f77-39c3-45dc-b554-8d3b37cdce1a.\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #72 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #72\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getListing took 1ms\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Starting: Merging data from HdfsLocatedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653301503196464597393543_0004_m_000007; isDirectory=true; modification_time=1733306010673; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #73 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #73\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #74 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #74\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getListing took 1ms\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Starting: Merging data from HdfsNamedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653301503196464597393543_0004_m_000007/part-00007-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet; isDirectory=false; length=936; replication=3; blocksize=134217728; modification_time=1733306011075; access_time=1733306010673; owner=root; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test/part-00007-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #75 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #75\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #76 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #76\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: rename took 3ms\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Merging data from HdfsNamedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653301503196464597393543_0004_m_000007/part-00007-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet; isDirectory=false; length=936; replication=3; blocksize=134217728; modification_time=1733306011075; access_time=1733306010673; owner=root; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test/part-00007-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet: duration 0:00.004s\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Merging data from HdfsLocatedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653301503196464597393543_0004_m_000007; isDirectory=true; modification_time=1733306010673; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test: duration 0:00.008s\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Starting: Merging data from HdfsLocatedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653302569995319439503301_0004_m_000019; isDirectory=true; modification_time=1733306010676; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #77 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #77\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #78 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #78\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getListing took 0ms\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Starting: Merging data from HdfsNamedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653302569995319439503301_0004_m_000019/part-00019-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet; isDirectory=false; length=971; replication=3; blocksize=134217728; modification_time=1733306011073; access_time=1733306010676; owner=root; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test/part-00019-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #79 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #79\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #80 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #80\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: rename took 2ms\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Merging data from HdfsNamedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653302569995319439503301_0004_m_000019/part-00019-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet; isDirectory=false; length=971; replication=3; blocksize=134217728; modification_time=1733306011073; access_time=1733306010676; owner=root; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test/part-00019-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet: duration 0:00.003s\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Merging data from HdfsLocatedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653302569995319439503301_0004_m_000019; isDirectory=true; modification_time=1733306010676; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test: duration 0:00.004s\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Starting: Merging data from HdfsLocatedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653305388066297851318446_0004_m_000015; isDirectory=true; modification_time=1733306010675; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #81 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #81\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #82 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #82\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getListing took 0ms\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Starting: Merging data from HdfsNamedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653305388066297851318446_0004_m_000015/part-00015-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet; isDirectory=false; length=943; replication=3; blocksize=134217728; modification_time=1733306011075; access_time=1733306010675; owner=root; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test/part-00015-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #83 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #83\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #84 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #84\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: rename took 2ms\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Merging data from HdfsNamedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653305388066297851318446_0004_m_000015/part-00015-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet; isDirectory=false; length=943; replication=3; blocksize=134217728; modification_time=1733306011075; access_time=1733306010675; owner=root; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test/part-00015-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet: duration 0:00.003s\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Merging data from HdfsLocatedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653305388066297851318446_0004_m_000015; isDirectory=true; modification_time=1733306010675; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test: duration 0:00.004s\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Starting: Merging data from HdfsLocatedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306300299701037560990_0004_m_000011; isDirectory=true; modification_time=1733306010675; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #85 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #85\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #86 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #86\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getListing took 0ms\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Starting: Merging data from HdfsNamedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306300299701037560990_0004_m_000011/part-00011-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet; isDirectory=false; length=963; replication=3; blocksize=134217728; modification_time=1733306011073; access_time=1733306010675; owner=root; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test/part-00011-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #87 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #87\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #88 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #88\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: rename took 2ms\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Merging data from HdfsNamedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306300299701037560990_0004_m_000011/part-00011-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet; isDirectory=false; length=963; replication=3; blocksize=134217728; modification_time=1733306011073; access_time=1733306010675; owner=root; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test/part-00011-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet: duration 0:00.003s\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Merging data from HdfsLocatedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306300299701037560990_0004_m_000011; isDirectory=true; modification_time=1733306010675; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test: duration 0:00.004s\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Starting: Merging data from HdfsLocatedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306970023585126218705_0004_m_000003; isDirectory=true; modification_time=1733306010675; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #89 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #89\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #90 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #90\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getListing took 1ms\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Starting: Merging data from HdfsNamedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306970023585126218705_0004_m_000003/part-00003-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet; isDirectory=false; length=964; replication=3; blocksize=134217728; modification_time=1733306011074; access_time=1733306010675; owner=root; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test/part-00003-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #91 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #91\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #92 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #92\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: rename took 3ms\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Merging data from HdfsNamedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306970023585126218705_0004_m_000003/part-00003-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet; isDirectory=false; length=964; replication=3; blocksize=134217728; modification_time=1733306011074; access_time=1733306010675; owner=root; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test/part-00003-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet: duration 0:00.003s\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Merging data from HdfsLocatedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306970023585126218705_0004_m_000003; isDirectory=true; modification_time=1733306010675; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test: duration 0:00.005s\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Starting: Merging data from HdfsLocatedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306985848896609675936_0004_m_000000; isDirectory=true; modification_time=1733306010675; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #93 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #93\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #94 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #94\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getListing took 0ms\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Starting: Merging data from HdfsNamedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306985848896609675936_0004_m_000000/part-00000-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet; isDirectory=false; length=469; replication=3; blocksize=134217728; modification_time=1733306011073; access_time=1733306010675; owner=root; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test/part-00000-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #95 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #95\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #96 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #96\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: rename took 2ms\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Merging data from HdfsNamedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306985848896609675936_0004_m_000000/part-00000-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet; isDirectory=false; length=469; replication=3; blocksize=134217728; modification_time=1733306011073; access_time=1733306010675; owner=root; group=supergroup; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test/part-00000-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet: duration 0:00.003s\n",
      "24/12/04 16:53:31 DEBUG FileOutputCommitter: Merging data from HdfsLocatedFileStatus{path=hdfs://hadoop:9000/tmp/test/_temporary/0/task_202412041653306985848896609675936_0004_m_000000; isDirectory=true; modification_time=1733306010675; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to hdfs://hadoop:9000/tmp/test: duration 0:00.004s\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #97 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #97\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: delete took 3ms\n",
      "24/12/04 16:53:31 DEBUG DFSClient: /tmp/test/_SUCCESS: masked={ masked: rw-r--r--, unmasked: rw-rw-rw- }\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #98 org.apache.hadoop.hdfs.protocol.ClientProtocol.create\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #98\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: create took 3ms\n",
      "24/12/04 16:53:31 DEBUG DFSClient: computePacketChunkSize: src=/tmp/test/_SUCCESS, chunkSize=516, chunksPerPacket=126, packetSize=65016\n",
      "24/12/04 16:53:31 DEBUG DataStreamer: block==null waiting for ack for: -1\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #99 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #99\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: complete took 3ms\n",
      "24/12/04 16:53:31 DEBUG SQLHadoopMapReduceCommitProtocol: Committing files staged for absolute locations Map()\n",
      "24/12/04 16:53:31 DEBUG SQLHadoopMapReduceCommitProtocol: Create absolute parent directories: Set()\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #100 org.apache.hadoop.hdfs.protocol.ClientProtocol.delete\n",
      "24/12/04 16:53:31 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #100\n",
      "24/12/04 16:53:31 DEBUG ProtobufRpcEngine2: Call: delete took 0ms\n",
      "24/12/04 16:53:31 INFO FileFormatWriter: Write Job f8415f77-39c3-45dc-b554-8d3b37cdce1a committed. Elapsed time: 47 ms.\n",
      "24/12/04 16:53:31 INFO FileFormatWriter: Finished processing stats for write job f8415f77-39c3-45dc-b554-8d3b37cdce1a.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data = [('2','Telangana','Chhetri'),\n",
    "  ('3','Sikkim','Bhutia'),\n",
    "  ('4','Hyderabad','Shabbir'),\n",
    "  ('5','Kerala','Vijayan'),\n",
    "  ('6','Mizoram','Lalpekhlua')\n",
    "  ]\n",
    "columns = [\"id\",\"city\",\"name\"]\n",
    "goalsDF = spark.createDataFrame(data=data, schema = columns)\n",
    "goalsDF.show()\n",
    "\n",
    "goalsDF.write.mode(\"overwrite\").parquet(\"hdfs://hadoop:9000/tmp/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a54d69-0bdd-4cd1-ae31-23bbea063efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.DataWritingCommandExec\",\"num-children\":1,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand\",\"num-children\":1,\"outputPath\":null,\"staticPartitions\":null,\"ifPartitionNotExists\":false,\"partitionColumns\":[],\"fileFormat\":null,\"options\":null,\"query\":0,\"mode\":null,\"outputColumnNames\":\"[id, city, name]\"},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.RDDScanExec\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"name\":\"ExistingRDD\",\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[]}]\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- *(1) Scan ExistingRDD[id#16,city#17,name#18]\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- *(1) Scan ExistingRDD[id#16,city#17,name#18]\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:31 DEBUG InsertIntoHadoopFsRelationVisitor: Matched io.openlineage.spark.agent.lifecycle.plan.InsertIntoHadoopFsRelationVisitor<org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand,io.openlineage.client.OpenLineage$OutputDataset> to logical plan InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "24/12/04 16:53:31 DEBUG InsertIntoHadoopFsRelationVisitor: Matched io.openlineage.spark.agent.lifecycle.plan.InsertIntoHadoopFsRelationVisitor<org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand,io.openlineage.client.OpenLineage$OutputDataset> to logical plan InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "24/12/04 16:53:31 DEBUG PlanUtils: Visitor io.openlineage.spark.agent.lifecycle.plan.InsertIntoHadoopFsRelationVisitor visited org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand, returned [io.openlineage.client.OpenLineage$OutputDataset@37b7b102]\n",
      "24/12/04 16:53:31 INFO RemovePathPatternUtils: Removing path pattern from dataset name /tmp/test\n",
      "24/12/04 16:53:31 DEBUG HdfsPathDataset: path: /tmp/test\n",
      "24/12/04 16:53:31 DEBUG RemovePathPatternUtils: Transformed path is /tmp/test\n",
      "24/12/04 16:53:31 DEBUG SparkSQLExecutionContext: Posting event for start 2: io.openlineage.client.OpenLineage$RunEvent@402990ec\n",
      "24/12/04 16:53:31 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:30.247Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-cb05-72df-b47a-1b530980c8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:31 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:30.247Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-cb05-72df-b47a-1b530980c8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:31 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@75a964fd, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:31 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:31 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@75a964fd, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:31 DEBUG OpenLineageToDataHub: Dataset URN: Optional[urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD)], alias_list: {}\n",
      "24/12/04 16:53:31 DEBUG OpenLineageToDataHub: Building output with /tmp/test\n",
      "24/12/04 16:53:31 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306010247}, name=execute_insert_into_hadoop_fs_relation_command.tmp_test, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306011159}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1733306010247, status=STARTED}, dataProcessInstanceProperties={name=01939116-cb05-72df-b47a-1b530980c8d9, created={actor=urn:li:corpuser:datahub, time=1733306010247}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-cb05-72df-b47a-1b530980c8d9, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD), schemaMetadata={platformSchema={com.linkedin.schema.MySqlDDL={tableSchema=[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}}, schemaName=, fields=[{fieldPath=id, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=city, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=name, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}], version=1, hash=, platform=urn:li:dataPlatform:hdfs}, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306010247, eventFormatter=datahub.event.EventFormatter@e2813cf), from {\"eventTime\":\"2024-12-04T09:53:30.247Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-cb05-72df-b47a-1b530980c8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:31 INFO DatahubEventEmitter: Collecting lineage successfully in 3 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/04 16:53:31 DEBUG SparkSQLExecutionContext: SparkListenerJobStart - executionId: 2\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand\",\"num-children\":1,\"outputPath\":null,\"staticPartitions\":null,\"ifPartitionNotExists\":false,\"partitionColumns\":[],\"fileFormat\":null,\"options\":null,\"query\":0,\"mode\":null,\"outputColumnNames\":\"[id, city, name]\"},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}]\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.DataWritingCommandExec\",\"num-children\":1,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand\",\"num-children\":1,\"outputPath\":null,\"staticPartitions\":null,\"ifPartitionNotExists\":false,\"partitionColumns\":[],\"fileFormat\":null,\"options\":null,\"query\":0,\"mode\":null,\"outputColumnNames\":\"[id, city, name]\"},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.RDDScanExec\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"name\":\"ExistingRDD\",\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[]}]\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- *(1) Scan ExistingRDD[id#16,city#17,name#18]\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- *(1) Scan ExistingRDD[id#16,city#17,name#18]\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:31 DEBUG InsertIntoHadoopFsRelationVisitor: Matched io.openlineage.spark.agent.lifecycle.plan.InsertIntoHadoopFsRelationVisitor<org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand,io.openlineage.client.OpenLineage$OutputDataset> to logical plan InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "24/12/04 16:53:31 DEBUG InsertIntoHadoopFsRelationVisitor: Matched io.openlineage.spark.agent.lifecycle.plan.InsertIntoHadoopFsRelationVisitor<org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand,io.openlineage.client.OpenLineage$OutputDataset> to logical plan InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "24/12/04 16:53:31 DEBUG PlanUtils: Visitor io.openlineage.spark.agent.lifecycle.plan.InsertIntoHadoopFsRelationVisitor visited org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand, returned [io.openlineage.client.OpenLineage$OutputDataset@61f7b0c3]\n",
      "24/12/04 16:53:31 INFO RemovePathPatternUtils: Removing path pattern from dataset name /tmp/test\n",
      "24/12/04 16:53:31 DEBUG HdfsPathDataset: path: /tmp/test\n",
      "24/12/04 16:53:31 DEBUG RemovePathPatternUtils: Transformed path is /tmp/test\n",
      "24/12/04 16:53:31 DEBUG SparkSQLExecutionContext: Posting event for start 2: io.openlineage.client.OpenLineage$RunEvent@9e870d5\n",
      "24/12/04 16:53:31 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:30.408Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-cb05-72df-b47a-1b530980c8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":4},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:31 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:30.408Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-cb05-72df-b47a-1b530980c8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":4},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:31 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@45a1269d, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:31 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:31 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@45a1269d, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:31 DEBUG OpenLineageToDataHub: Dataset URN: Optional[urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD)], alias_list: {}\n",
      "24/12/04 16:53:31 DEBUG OpenLineageToDataHub: Building output with /tmp/test\n",
      "24/12/04 16:53:31 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={jobId=4, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test), jobInfo={customProperties={jobId=4, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306010408}, name=execute_insert_into_hadoop_fs_relation_command.tmp_test, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306011172}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1733306010408, status=STARTED}, dataProcessInstanceProperties={name=01939116-cb05-72df-b47a-1b530980c8d9, created={actor=urn:li:corpuser:datahub, time=1733306010408}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-cb05-72df-b47a-1b530980c8d9, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD), schemaMetadata={platformSchema={com.linkedin.schema.MySqlDDL={tableSchema=[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}}, schemaName=, fields=[{fieldPath=id, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=city, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=name, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}], version=1, hash=, platform=urn:li:dataPlatform:hdfs}, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306010408, eventFormatter=datahub.event.EventFormatter@4fa14656), from {\"eventTime\":\"2024-12-04T09:53:30.408Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-cb05-72df-b47a-1b530980c8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":4},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:31 INFO DatahubEventEmitter: Collecting lineage successfully in 4 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onJobStart completed successfully in 13 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/04 16:53:31 DEBUG SparkSQLExecutionContext: SparkListenerJobEnd - executionId: 2\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand\",\"num-children\":1,\"outputPath\":null,\"staticPartitions\":null,\"ifPartitionNotExists\":false,\"partitionColumns\":[],\"fileFormat\":null,\"options\":null,\"query\":0,\"mode\":null,\"outputColumnNames\":\"[id, city, name]\"},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}]\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.DataWritingCommandExec\",\"num-children\":1,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand\",\"num-children\":1,\"outputPath\":null,\"staticPartitions\":null,\"ifPartitionNotExists\":false,\"partitionColumns\":[],\"fileFormat\":null,\"options\":null,\"query\":0,\"mode\":null,\"outputColumnNames\":\"[id, city, name]\"},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.RDDScanExec\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"name\":\"ExistingRDD\",\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[]}]\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- *(1) Scan ExistingRDD[id#16,city#17,name#18]\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- *(1) Scan ExistingRDD[id#16,city#17,name#18]\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:31 DEBUG InsertIntoHadoopFsRelationVisitor: Matched io.openlineage.spark.agent.lifecycle.plan.InsertIntoHadoopFsRelationVisitor<org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand,io.openlineage.client.OpenLineage$OutputDataset> to logical plan InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "24/12/04 16:53:31 DEBUG InsertIntoHadoopFsRelationVisitor: Matched io.openlineage.spark.agent.lifecycle.plan.InsertIntoHadoopFsRelationVisitor<org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand,io.openlineage.client.OpenLineage$OutputDataset> to logical plan InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "24/12/04 16:53:31 DEBUG PlanUtils: Visitor io.openlineage.spark.agent.lifecycle.plan.InsertIntoHadoopFsRelationVisitor visited org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand, returned [io.openlineage.client.OpenLineage$OutputDataset@2273ccf8]\n",
      "24/12/04 16:53:31 INFO RemovePathPatternUtils: Removing path pattern from dataset name /tmp/test\n",
      "24/12/04 16:53:31 DEBUG HdfsPathDataset: path: /tmp/test\n",
      "24/12/04 16:53:31 DEBUG RemovePathPatternUtils: Transformed path is /tmp/test\n",
      "24/12/04 16:53:31 DEBUG SparkSQLExecutionContext: Posting event for end 2: io.openlineage.client.OpenLineage$RunEvent@4ab1a838\n",
      "24/12/04 16:53:31 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:31.095Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-cb05-72df-b47a-1b530980c8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{\"outputStatistics\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-2/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet\",\"rowCount\":1,\"size\":964}}}]}\n",
      "24/12/04 16:53:31 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:31.095Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-cb05-72df-b47a-1b530980c8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{\"outputStatistics\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-2/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet\",\"rowCount\":1,\"size\":964}}}]}\n",
      "24/12/04 16:53:31 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@7923efed, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:31 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:31 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@7923efed, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:31 DEBUG OpenLineageToDataHub: Dataset URN: Optional[urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD)], alias_list: {}\n",
      "24/12/04 16:53:31 DEBUG OpenLineageToDataHub: Building output with /tmp/test\n",
      "24/12/04 16:53:31 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306011095}, name=execute_insert_into_hadoop_fs_relation_command.tmp_test, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306011183}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1733306011095, status=STARTED}, dataProcessInstanceProperties={name=01939116-cb05-72df-b47a-1b530980c8d9, created={actor=urn:li:corpuser:datahub, time=1733306011095}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-cb05-72df-b47a-1b530980c8d9, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD), schemaMetadata={platformSchema={com.linkedin.schema.MySqlDDL={tableSchema=[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}}, schemaName=, fields=[{fieldPath=id, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=city, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=name, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}], version=1, hash=, platform=urn:li:dataPlatform:hdfs}, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306011095, eventFormatter=datahub.event.EventFormatter@676d2534), from {\"eventTime\":\"2024-12-04T09:53:31.095Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-cb05-72df-b47a-1b530980c8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{\"outputStatistics\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-2/OutputStatisticsOutputDatasetFacet.json#/$defs/OutputStatisticsOutputDatasetFacet\",\"rowCount\":1,\"size\":964}}}]}\n",
      "24/12/04 16:53:31 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: onJobEnd completed successfully in 10 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerDriverAccumUpdates\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd\n",
      "24/12/04 16:53:31 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionEnd - executionId: 2\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand\",\"num-children\":1,\"outputPath\":null,\"staticPartitions\":null,\"ifPartitionNotExists\":false,\"partitionColumns\":[],\"fileFormat\":null,\"options\":null,\"query\":0,\"mode\":null,\"outputColumnNames\":\"[id, city, name]\"},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}]\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.DataWritingCommandExec\",\"num-children\":1,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand\",\"num-children\":1,\"outputPath\":null,\"staticPartitions\":null,\"ifPartitionNotExists\":false,\"partitionColumns\":[],\"fileFormat\":null,\"options\":null,\"query\":0,\"mode\":null,\"outputColumnNames\":\"[id, city, name]\"},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.RDDScanExec\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"name\":\"ExistingRDD\",\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[]}]\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- *(1) Scan ExistingRDD[id#16,city#17,name#18]\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- *(1) Scan ExistingRDD[id#16,city#17,name#18]\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:31 DEBUG InsertIntoHadoopFsRelationVisitor: Matched io.openlineage.spark.agent.lifecycle.plan.InsertIntoHadoopFsRelationVisitor<org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand,io.openlineage.client.OpenLineage$OutputDataset> to logical plan InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "24/12/04 16:53:31 DEBUG InsertIntoHadoopFsRelationVisitor: Matched io.openlineage.spark.agent.lifecycle.plan.InsertIntoHadoopFsRelationVisitor<org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand,io.openlineage.client.OpenLineage$OutputDataset> to logical plan InsertIntoHadoopFsRelationCommand hdfs://hadoop:9000/tmp/test, false, Parquet, [path=hdfs://hadoop:9000/tmp/test], Overwrite, [id, city, name]\n",
      "+- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "24/12/04 16:53:31 DEBUG PlanUtils: Visitor io.openlineage.spark.agent.lifecycle.plan.InsertIntoHadoopFsRelationVisitor visited org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand, returned [io.openlineage.client.OpenLineage$OutputDataset@46607ee8]\n",
      "24/12/04 16:53:31 INFO RemovePathPatternUtils: Removing path pattern from dataset name /tmp/test\n",
      "24/12/04 16:53:31 DEBUG HdfsPathDataset: path: /tmp/test\n",
      "24/12/04 16:53:31 DEBUG RemovePathPatternUtils: Transformed path is /tmp/test\n",
      "24/12/04 16:53:31 DEBUG SparkSQLExecutionContext: Posting event for end 2: {\"eventTime\":\"2024-12-04T09:53:31.147Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-cb05-72df-b47a-1b530980c8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:31 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:31.147Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-cb05-72df-b47a-1b530980c8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:31 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:31.147Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-cb05-72df-b47a-1b530980c8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:31 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@20762e58, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:31 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:31 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@20762e58, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:31 DEBUG OpenLineageToDataHub: Dataset URN: Optional[urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD)], alias_list: {}\n",
      "24/12/04 16:53:31 DEBUG OpenLineageToDataHub: Building output with /tmp/test\n",
      "24/12/04 16:53:31 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306011147}, name=execute_insert_into_hadoop_fs_relation_command.tmp_test, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306011194}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1733306011147, status=COMPLETE}, dataProcessInstanceProperties={name=01939116-cb05-72df-b47a-1b530980c8d9, created={actor=urn:li:corpuser:datahub, time=1733306011147}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-cb05-72df-b47a-1b530980c8d9, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD), schemaMetadata={platformSchema={com.linkedin.schema.MySqlDDL={tableSchema=[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}}, schemaName=, fields=[{fieldPath=id, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=city, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=name, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}], version=1, hash=, platform=urn:li:dataPlatform:hdfs}, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306011147, eventFormatter=datahub.event.EventFormatter@78777b81), from {\"eventTime\":\"2024-12-04T09:53:31.147Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-cb05-72df-b47a-1b530980c8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_insert_into_hadoop_fs_relation_command.tmp_test\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/04 16:53:31 INFO DatahubEventEmitter: Collecting lineage successfully in 1 ms\n",
      "24/12/04 16:53:31 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart\n",
      "24/12/04 16:53:31 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/04 16:53:31 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:31 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:31 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionStart - executionId: 3\n",
      "24/12/04 16:53:31 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]\n",
      "24/12/04 16:53:31 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:31 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:31 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 566.1 KiB, free 365.4 MiB)\n",
      "24/12/04 16:53:31 DEBUG BlockManager: Put block broadcast_5 locally took 6 ms\n",
      "24/12/04 16:53:31 DEBUG BlockManager: Putting block broadcast_5 without replication took 7 ms\n",
      "24/12/04 16:53:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 365.4 MiB)\n",
      "24/12/04 16:53:31 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_5_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:31 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop113:38317 (size: 38.4 KiB, free: 366.2 MiB)\n",
      "24/12/04 16:53:31 DEBUG BlockManagerMaster: Updated info of block broadcast_5_piece0\n",
      "24/12/04 16:53:31 DEBUG BlockManager: Told master about block broadcast_5_piece0\n",
      "24/12/04 16:53:31 DEBUG BlockManager: Put block broadcast_5_piece0 locally took 0 ms\n",
      "24/12/04 16:53:31 DEBUG BlockManager: Putting block broadcast_5_piece0 without replication took 0 ms\n",
      "24/12/04 16:53:31 INFO SparkContext: Created broadcast 5 from broadcast at HBaseContext.scala:71\n",
      "24/12/04 16:53:31 INFO HBaseRelation: newtable\n",
      "is not defined or no larger than 3, skip the create table\n",
      "24/12/04 16:53:31 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator rdd_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     rdd_input_0 = inputs[0];\n",
      "/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( rdd_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();\n",
      "/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 028 */       boolean rdd_isNull_0 = rdd_row_0.isNullAt(0);\n",
      "/* 029 */       UTF8String rdd_value_0 = rdd_isNull_0 ?\n",
      "/* 030 */       null : (rdd_row_0.getUTF8String(0));\n",
      "/* 031 */       boolean rdd_isNull_1 = rdd_row_0.isNullAt(1);\n",
      "/* 032 */       UTF8String rdd_value_1 = rdd_isNull_1 ?\n",
      "/* 033 */       null : (rdd_row_0.getUTF8String(1));\n",
      "/* 034 */       boolean rdd_isNull_2 = rdd_row_0.isNullAt(2);\n",
      "/* 035 */       UTF8String rdd_value_2 = rdd_isNull_2 ?\n",
      "/* 036 */       null : (rdd_row_0.getUTF8String(2));\n",
      "/* 037 */       rdd_mutableStateArray_0[0].reset();\n",
      "/* 038 */\n",
      "/* 039 */       rdd_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 040 */\n",
      "/* 041 */       if (rdd_isNull_0) {\n",
      "/* 042 */         rdd_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 043 */       } else {\n",
      "/* 044 */         rdd_mutableStateArray_0[0].write(0, rdd_value_0);\n",
      "/* 045 */       }\n",
      "/* 046 */\n",
      "/* 047 */       if (rdd_isNull_1) {\n",
      "/* 048 */         rdd_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */       } else {\n",
      "/* 050 */         rdd_mutableStateArray_0[0].write(1, rdd_value_1);\n",
      "/* 051 */       }\n",
      "/* 052 */\n",
      "/* 053 */       if (rdd_isNull_2) {\n",
      "/* 054 */         rdd_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 055 */       } else {\n",
      "/* 056 */         rdd_mutableStateArray_0[0].write(2, rdd_value_2);\n",
      "/* 057 */       }\n",
      "/* 058 */       append((rdd_mutableStateArray_0[0].getRow()));\n",
      "/* 059 */       if (shouldStop()) return;\n",
      "/* 060 */     }\n",
      "/* 061 */   }\n",
      "/* 062 */\n",
      "/* 063 */ }\n",
      "\n",
      "24/12/04 16:53:31 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "24/12/04 16:53:31 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "24/12/04 16:53:31 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$rdd$1\n",
      "24/12/04 16:53:31 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++\n",
      "24/12/04 16:53:31 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$insert$11\n",
      "24/12/04 16:53:31 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$insert$11) is now cleaned +++\n",
      "24/12/04 16:53:31 DEBUG HadoopMapRedWriteConfigUtil: Saving as hadoop file of type (LongWritable, Text)\n",
      "24/12/04 16:53:31 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "24/12/04 16:53:31 DEBUG FileCommitProtocol: Creating committer org.apache.spark.internal.io.HadoopMapRedCommitProtocol; job 21; output=null; dynamic=false\n",
      "24/12/04 16:53:31 DEBUG FileCommitProtocol: Falling back to (String, String) constructor\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43c8e4e0}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context JobContextImpl{jobId=job_202412041653319151691516548704819_0021}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 WARN FileOutputCommitter: Output Path is null in setupJob()\n",
      "24/12/04 16:53:31 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$write$1\n",
      "24/12/04 16:53:31 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$write$1) is now cleaned +++\n",
      "24/12/04 16:53:31 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
      "24/12/04 16:53:31 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 21 took 0.000075 seconds\n",
      "24/12/04 16:53:31 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/04 16:53:31 INFO DAGScheduler: Got job 5 (runJob at SparkHadoopWriter.scala:83) with 20 output partitions\n",
      "24/12/04 16:53:31 INFO DAGScheduler: Final stage: ResultStage 5 (runJob at SparkHadoopWriter.scala:83)\n",
      "24/12/04 16:53:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/04 16:53:31 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/04 16:53:31 DEBUG DAGScheduler: submitStage(ResultStage 5 (name=runJob at SparkHadoopWriter.scala:83;jobs=5))\n",
      "24/12/04 16:53:31 DEBUG DAGScheduler: missing: List()\n",
      "24/12/04 16:53:31 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[21] at map at DefaultSource.scala:277), which has no missing parents\n",
      "24/12/04 16:53:31 DEBUG DAGScheduler: submitMissingTasks(ResultStage 5)\n",
      "24/12/04 16:53:31 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 256.3 KiB, free 365.2 MiB)\n",
      "24/12/04 16:53:31 DEBUG BlockManager: Put block broadcast_6 locally took 0 ms\n",
      "24/12/04 16:53:31 DEBUG BlockManager: Putting block broadcast_6 without replication took 0 ms\n",
      "24/12/04 16:53:31 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 90.6 KiB, free 365.1 MiB)\n",
      "24/12/04 16:53:31 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_6_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:31 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop113:38317 (size: 90.6 KiB, free: 366.1 MiB)\n",
      "24/12/04 16:53:31 DEBUG BlockManagerMaster: Updated info of block broadcast_6_piece0\n",
      "24/12/04 16:53:31 DEBUG BlockManager: Told master about block broadcast_6_piece0\n",
      "24/12/04 16:53:31 DEBUG BlockManager: Put block broadcast_6_piece0 locally took 1 ms\n",
      "24/12/04 16:53:31 DEBUG BlockManager: Putting block broadcast_6_piece0 without replication took 1 ms\n",
      "24/12/04 16:53:31 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/04 16:53:31 INFO DAGScheduler: Submitting 20 missing tasks from ResultStage 5 (MapPartitionsRDD[21] at map at DefaultSource.scala:277) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "24/12/04 16:53:31 INFO TaskSchedulerImpl: Adding task set 5.0 with 20 tasks resource profile 0\n",
      "24/12/04 16:53:31 DEBUG TaskSetManager: Epoch for TaskSet 5.0: 0\n",
      "24/12/04 16:53:31 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "24/12/04 16:53:31 DEBUG TaskSetManager: Valid locality levels for TaskSet 5.0: NO_PREF, ANY\n",
      "24/12/04 16:53:31 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_5.0, runningTasks: 0\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 60) (hadoop113, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 61) (hadoop113, executor driver, partition 1, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 62) (hadoop113, executor driver, partition 2, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 63) (hadoop113, executor driver, partition 3, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 4.0 in stage 5.0 (TID 64) (hadoop113, executor driver, partition 4, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 5.0 in stage 5.0 (TID 65) (hadoop113, executor driver, partition 5, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 6.0 in stage 5.0 (TID 66) (hadoop113, executor driver, partition 6, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 7.0 in stage 5.0 (TID 67) (hadoop113, executor driver, partition 7, PROCESS_LOCAL, 4482 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 8.0 in stage 5.0 (TID 68) (hadoop113, executor driver, partition 8, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 9.0 in stage 5.0 (TID 69) (hadoop113, executor driver, partition 9, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 10.0 in stage 5.0 (TID 70) (hadoop113, executor driver, partition 10, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 11.0 in stage 5.0 (TID 71) (hadoop113, executor driver, partition 11, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 12.0 in stage 5.0 (TID 72) (hadoop113, executor driver, partition 12, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 13.0 in stage 5.0 (TID 73) (hadoop113, executor driver, partition 13, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 14.0 in stage 5.0 (TID 74) (hadoop113, executor driver, partition 14, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 15.0 in stage 5.0 (TID 75) (hadoop113, executor driver, partition 15, PROCESS_LOCAL, 4483 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 16.0 in stage 5.0 (TID 76) (hadoop113, executor driver, partition 16, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 17.0 in stage 5.0 (TID 77) (hadoop113, executor driver, partition 17, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 18.0 in stage 5.0 (TID 78) (hadoop113, executor driver, partition 18, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Starting task 19.0 in stage 5.0 (TID 79) (hadoop113, executor driver, partition 19, PROCESS_LOCAL, 4487 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:31 INFO Executor: Running task 1.0 in stage 5.0 (TID 61)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 3.0 in stage 5.0 (TID 63)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 2.0 in stage 5.0 (TID 62)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 0.0 in stage 5.0 (TID 60)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 5.0 in stage 5.0 (TID 65)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 4.0 in stage 5.0 (TID 64)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 6.0 in stage 5.0 (TID 66)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 7.0 in stage 5.0 (TID 67)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 9.0 in stage 5.0 (TID 69)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 8.0 in stage 5.0 (TID 68)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 10.0 in stage 5.0 (TID 70)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 12.0 in stage 5.0 (TID 72)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 17.0 in stage 5.0 (TID 77)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 18.0 in stage 5.0 (TID 78)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 11.0 in stage 5.0 (TID 71)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 14.0 in stage 5.0 (TID 74)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 13.0 in stage 5.0 (TID 73)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 19.0 in stage 5.0 (TID 79)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 15.0 in stage 5.0 (TID 75)\n",
      "24/12/04 16:53:31 INFO Executor: Running task 16.0 in stage 5.0 (TID 76)\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 1\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 3\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 2\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 4\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 5\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 6\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 7\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 8\n",
      "24/12/04 16:53:31 DEBUG BlockManager: Getting local block broadcast_6\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 9\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 10\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 11\n",
      "24/12/04 16:53:31 DEBUG BlockManager: Level for block broadcast_6 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 12\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 13\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 14\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 15\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 16\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 17\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 18\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 20\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 19\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000009_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@36385a28}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000009_0, status=''}\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000008_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@1b68308c}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000008_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@390ed34e}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000000_0, status=''}\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000015_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@42f47f02}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000015_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000002_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5974b10d}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000002_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000019_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3b574af5}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000019_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000013_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@61fd7e44}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000013_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000007_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2ba14324}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000007_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000017_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2d99e7a}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000017_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000012_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@52e980a1}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000012_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000003_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4112c202}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000003_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000001_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@219aef3a}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000001_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000016_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2f6bb1be}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000016_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000006_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@15f97e6}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000006_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000018_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@43a3ab2}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000018_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000011_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@31107930}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000011_0, status=''}\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000004_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@47f7c864}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000004_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000005_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@6034f26f}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000005_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000010_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@30fb8153}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000010_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:31 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/04 16:53:31 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000014_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@452e7df}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412041653319151691516548704819_0021}; taskId=attempt_202412041653319151691516548704819_0021_m_000014_0, status=''}\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/04 16:53:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@4741a0d9]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@57a9bf2a]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@6e1af71b]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@2f1c9]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@2ead6f10]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@6390512e]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@2a03e8de]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@6ed179f9]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@5d6acf83]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@1a5eaf92]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@5a0d7783]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@4076b9fc]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@4ee37594]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@852b9bd]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@59fe2ba6]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@dcc5c5d]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@5b728a8e]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@5f890f1b]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@3254163c]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$4192/118550860@382e0494]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x1429f5b8 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x03984772 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x55201f84 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x4d2583b0 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x25eb0d12 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x20dd85d7 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x1593bff5 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x78b23790 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x60f13965 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x0cf9318d to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x46813aff to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x36c00f67 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x2a6560e8 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x27d7be5e to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x30889693 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x5c368401 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x41fc1908 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x51bf98f8 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x5c1b9a9f to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Connect 0x748ca533 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:zookeeper.version=3.5.7-f0fdd52973d373ffd9c86b81d99842dc2c7f660e, built on 02/10/2020 11:30 GMT\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:host.name=hadoop113\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:java.version=1.8.0_432\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:java.vendor=Private Build\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:java.class.path=/usr/local/lib/python3.8/dist-packages/pyspark/conf:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-rbac-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-sql_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/zookeeper-jute-3.6.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-compress-1.21.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-xml_2.12-1.2.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-beeline-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-exec-2.3.9-core.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/joda-time-2.10.13.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-batch-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-dbcp-1.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-reflect-2.12.15.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-kubernetes_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/xbean-asm9-shaded-4.20.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arrow-memory-core-7.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hk2-utils-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/lz4-java-1.8.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/log4j-core-2.17.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/JTransforms-3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/curator-client-2.13.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-shaded-guava-1.1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/log4j-1.2-api-2.17.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-policy-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/flatbuffers-java-1.12.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-hive_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/HikariCP-2.5.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-jackson-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hk2-api-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/shims-0.9.25.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-extensions-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/antlr4-runtime-4.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/breeze-macros_2.12-1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-lang3-3.12.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/shapeless_2.12-2.3.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/datanucleus-api-jdo-4.2.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-launcher_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-text-1.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/generex-1.0.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-container-servlet-core-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-apiextensions-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-cli-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/javax.jdo-3.2.0-m3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jdo-api-3.0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jpam-1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/stax-api-1.0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-metrics-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/javolution-5.5.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/protobuf-java-2.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-autoscaling-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-databind-2.13.4.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arrow-memory-netty-7.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-networking-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-resolver-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-catalyst_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-compiler-2.12.15.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-serde-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/istack-commons-runtime-3.0.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/libfb303-0.9.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/avro-ipc-1.11.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-client-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-jmx-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-codec-1.15.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-core-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jline-2.14.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-collection-compat_2.12-2.1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.validation-api-2.0.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/core-1.1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/slf4j-api-1.7.32.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-logging-1.1.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/javassist-3.25.0-GA.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-events-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/zstd-jni-1.5.2-1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-hadoop-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/snakeyaml-1.31.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spire-macros_2.12-0.17.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-all-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-scheduling-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/zookeeper-3.6.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-cli-1.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/threeten-extra-1.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/snappy-java-1.1.8.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-tcnative-classes-2.0.48.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-network-common_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/curator-recipes-2.13.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-lang-2.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-parser-combinators_2.12-1.1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arpack_combined_all-0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/py4j-0.10.9.5.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-discovery-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jsr305-3.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-service-rpc-3.1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-datatype-jsr310-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-graphx_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spire_2.12-0.17.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/okhttp-3.12.12.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/bonecp-0.8.0.RELEASE.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jcl-over-slf4j-1.7.32.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/orc-shims-1.7.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/transaction-api-1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spire-platform_2.12-0.17.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/guava-14.0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/chill-java-0.10.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/minlog-1.3.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-buffer-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-pool-1.5.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/opencsv-2.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/orc-mapreduce-1.7.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/aopalliance-repackaged-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arrow-vector-7.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-common-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-library-2.12.15.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/univocity-parsers-2.9.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-unsafe_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-handler-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/chill_2.12-0.10.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-node-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-container-servlet-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-common-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json4s-jackson_2.12-3.7.0-M11.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arpack-2.2.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/JLargeArrays-1.5.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-math3-3.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-core-asl-1.9.13.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/datanucleus-core-4.1.17.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.inject-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/janino-3.0.16.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-api-3.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-server-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arrow-format-7.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-compiler-3.0.16.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-mllib-local_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/xz-1.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-dataformat-yaml-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.servlet-api-4.0.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json4s-ast_2.12-3.7.0-M11.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-collections-3.2.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-classes-kqueue-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/pickle-1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jul-to-slf4j-1.7.32.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/avro-1.11.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/RoaringBitmap-0.9.25.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jaxb-runtime-2.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/super-csv-2.2.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/httpcore-4.4.14.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-yarn_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-encoding-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json4s-core_2.12-3.7.0-M11.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/lapack-2.2.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.annotation-api-1.3.5.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-flowcontrol-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/orc-core-1.7.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-unix-common-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-module-scala_2.12-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/compress-lzf-1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/objenesis-3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-core-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/httpclient-4.5.13.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-yarn-server-web-proxy-3.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/log4j-api-2.17.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-shims-scheduler-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/automaton-1.11-8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/curator-framework-2.13.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ST4-4.0.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-kvstore_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-certificates-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-client-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/avro-mapred-1.11.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kryo-shaded-4.0.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-core-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/zjsonpatch-0.3.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jodd-core-3.5.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-crypto-1.1.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-sketch_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-shims-common-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/velocity-1.5.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-annotations-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-coordination-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-storage-api-2.7.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-format-structures-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-vector-code-gen-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/oro-2.0.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-metastore-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-common-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jta-1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-graphite-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-admissionregistration-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/osgi-resource-locator-1.0.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-collections4-4.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-io-2.11.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/derby-10.14.2.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spire-util_2.12-0.17.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/blas-2.2.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-shims-0.23-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-json-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/paranamer-2.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/aircompressor-0.21.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-mesos_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-column-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-codec-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/antlr-runtime-3.5.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-storageclass-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-jvm-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-network-shuffle_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/leveldbjni-all-1.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/stream-2.9.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/cats-kernel_2.12-2.1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/gson-2.2.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-repl_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/breeze_2.12-1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-shims-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/annotations-17.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/mesos-1.4.3-shaded-protobuf.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.ws.rs-api-2.1.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-mllib_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/okio-1.14.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/tink-1.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-classes-epoll-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-hive-thriftserver_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-apps-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-hk2-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-mapper-asl-1.9.13.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/algebra_2.12-2.0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/log4j-slf4j-impl-2.17.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-llap-common-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json-1.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/audience-annotations-0.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-jdbc-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.xml.bind-api-2.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/activation-1.1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-common-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/libthrift-0.12.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-core_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/datanucleus-rdbms-4.1.19.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/rocksdbjni-6.20.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/logging-interceptor-3.12.12.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json4s-scalap_2.12-3.7.0-M11.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-tags_2.12-3.3.1-tests.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-streaming_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hk2-locator-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-common-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-tags_2.12-3.3.1.jar\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:java.compiler=<NA>\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:os.name=Linux\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:os.arch=amd64\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:os.version=6.8.0-49-generic\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:user.name=root\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:user.home=/root\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:user.dir=/\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:os.memory.free=593MB\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:os.memory.max=910MB\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Client environment:os.memory.total=874MB\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$4279/1734274271@3b4853b\n",
      "24/12/04 16:53:31 INFO X509Util: Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53056, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53046, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53160, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53094, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53102, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53128, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53016, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53036, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53144, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53002, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53004, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53126, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53000, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53140, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53080, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53068, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53096, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:52982, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:53118, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:52992, server: localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb010c, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb0104, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb010b, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb0107, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb0106, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb0103, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb0105, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb0102, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb0101, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb00fc, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb00fb, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb0108, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb00fd, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb00fe, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb0100, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb0109, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb010a, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb010e, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb010d, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x100035c67eb00ff, negotiated timeout = 90000\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb00fb, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010d, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0100, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb00ff, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010a, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb00fd, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010b, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0108, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0109, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb00fc, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0105, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0107, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb00fe, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0104, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010c, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0106, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010e, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0102, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0103, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0101, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,599,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a3136303030fffffff8fffffff8ffffffbaffffff8fffffffa165ffffffa6ffffffdd50425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,13,1733275814277,1733275814277,0,0,0,0,67,0,13} \n",
      "24/12/04 16:53:31 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework\n",
      "24/12/04 16:53:31 DEBUG ResourceLeakDetector: -Dorg.apache.hbase.thirdparty.io.netty.leakDetection.level: simple\n",
      "24/12/04 16:53:31 DEBUG ResourceLeakDetector: -Dorg.apache.hbase.thirdparty.io.netty.leakDetection.targetRecords: 4\n",
      "24/12/04 16:53:31 DEBUG ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector@3270a70c\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent0: -Dio.netty.noUnsafe: false\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent0: Java version: 8\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent0: sun.misc.Unsafe.storeFence: available\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent0: java.nio.Buffer.address: available\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent0: direct buffer constructor: available\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent0: java.nio.Bits.unaligned: available, true\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent0: jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent: sun.misc.Unsafe: available\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent: -Dio.netty.maxDirectMemory: 954728448 bytes\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent: -Dio.netty.uninitializedArrayAllocationThreshold: -1\n",
      "24/12/04 16:53:31 DEBUG CleanerJava6: java.nio.ByteBuffer.cleaner(): available\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false\n",
      "24/12/04 16:53:31 DEBUG PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available\n",
      "24/12/04 16:53:31 DEBUG ClassSize: Using Unsafe to estimate memory layout\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@39e3f119, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@76de4cc9, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@22e5f542, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@514ed558, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@26b61dac, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@550e2513, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@1e5402b8, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@71d9ed8e, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@1e81c400, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@34de314c, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@7317dbf0, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@5850286b, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@6b8ee490, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@2fe783f, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@27f5d56c, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@16348921, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@2b0d6259, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@3d32715f, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@7492571c, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@335071f4, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/04 16:53:31 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 40\n",
      "24/12/04 16:53:31 DEBUG InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024\n",
      "24/12/04 16:53:31 DEBUG InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096\n",
      "24/12/04 16:53:31 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false\n",
      "24/12/04 16:53:31 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 48, boot = -831, init = 879, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 44, boot = -858, init = 902, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 45, boot = -832, init = 877, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 42, boot = -840, init = 882, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 46, boot = -830, init = 876, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 43, boot = -833, init = 876, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 45, boot = -837, init = 882, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 42, boot = -855, init = 897, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 41, boot = -863, init = 904, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 45, boot = -841, init = 886, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 47, boot = -834, init = 881, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 50, boot = -842, init = 892, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 45, boot = -858, init = 903, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 42, boot = -837, init = 879, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 41, boot = -840, init = 881, finish = 0\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x55201f84 to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x0cf9318d to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x30889693 to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x2a6560e8 to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x748ca533 to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x51bf98f8 to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x1429f5b8 to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x20dd85d7 to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x46813aff to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x5c1b9a9f to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb00fd\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb0106\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x36c00f67 to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb00fc\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x27d7be5e to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb0100\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb00fc\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb0106\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb010c\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb0108\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb010c\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb00fd\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb0108\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb010e\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000004_0\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000010_0\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb010e\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000005_0\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb00ff\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000001_0\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb0109\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb0109\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000008_0\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000013_0\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000017_0\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x41fc1908 to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x5c368401 to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x4d2583b0 to 127.0.0.1:2181\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb0102\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000014_0\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb0102\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb0103\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb0103\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb010a\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb010a\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb00ff\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb00fb\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb00fb\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 13.0 in stage 5.0 (TID 73). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 10.0 in stage 5.0 (TID 70). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 17.0 in stage 5.0 (TID 77). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 4.0 in stage 5.0 (TID 64). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 8.0 in stage 5.0 (TID 68). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000006_0\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 19\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb0107\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000012_0\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 18\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb0107\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 17\n",
      "24/12/04 16:53:31 DEBUG ZooKeeper: Closing session: 0x100035c67eb00fe\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 16\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb00fe\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 1.0 in stage 5.0 (TID 61). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 5.0 in stage 5.0 (TID 65). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 15\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 14.0 in stage 5.0 (TID 74). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 14\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 13\n",
      "24/12/04 16:53:31 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_5.0, runningTasks: 19\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb0100\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000016_0\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 12\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 12.0 in stage 5.0 (TID 72). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 11\n",
      "24/12/04 16:53:31 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000018_0\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000002_0\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000009_0\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 6.0 in stage 5.0 (TID 66). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 16.0 in stage 5.0 (TID 76). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 10\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 9\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 13.0 in stage 5.0 (TID 73) in 386 ms on hadoop113 (executor driver) (1/20)\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 18.0 in stage 5.0 (TID 78). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 8\n",
      "24/12/04 16:53:31 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000000_0\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 2.0 in stage 5.0 (TID 62). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 7\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 4.0 in stage 5.0 (TID 64) in 387 ms on hadoop113 (executor driver) (2/20)\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 9.0 in stage 5.0 (TID 69). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 6\n",
      "24/12/04 16:53:31 INFO Executor: Finished task 0.0 in stage 5.0 (TID 60). 1895 bytes result sent to driver\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 5.0 in stage 5.0 (TID 65) in 387 ms on hadoop113 (executor driver) (3/20)\n",
      "24/12/04 16:53:31 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 5\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 8.0 in stage 5.0 (TID 68) in 387 ms on hadoop113 (executor driver) (4/20)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 10.0 in stage 5.0 (TID 70) in 387 ms on hadoop113 (executor driver) (5/20)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 61) in 389 ms on hadoop113 (executor driver) (6/20)\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0106, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,601,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0109, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,600,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb0109\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb0106\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb0109 : Unable to read additional data from server sessionid 0x100035c67eb0109, likely server has closed socket\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 14.0 in stage 5.0 (TID 74) in 387 ms on hadoop113 (executor driver) (7/20)\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb0106 : Unable to read additional data from server sessionid 0x100035c67eb0106, likely server has closed socket\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 17.0 in stage 5.0 (TID 77) in 387 ms on hadoop113 (executor driver) (8/20)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 12.0 in stage 5.0 (TID 72) in 388 ms on hadoop113 (executor driver) (9/20)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 6.0 in stage 5.0 (TID 66) in 389 ms on hadoop113 (executor driver) (10/20)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 16.0 in stage 5.0 (TID 76) in 387 ms on hadoop113 (executor driver) (11/20)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 18.0 in stage 5.0 (TID 78) in 387 ms on hadoop113 (executor driver) (12/20)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 62) in 391 ms on hadoop113 (executor driver) (13/20)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 9.0 in stage 5.0 (TID 69) in 389 ms on hadoop113 (executor driver) (14/20)\n",
      "24/12/04 16:53:31 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 60) in 391 ms on hadoop113 (executor driver) (15/20)\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010e, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,602,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0108, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,603,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb010e : Unable to read additional data from server sessionid 0x100035c67eb010e, likely server has closed socket\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb0108 : Unable to read additional data from server sessionid 0x100035c67eb0108, likely server has closed socket\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb0108\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb00fd, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,605,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb00fc, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,604,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb00fd : Unable to read additional data from server sessionid 0x100035c67eb00fd, likely server has closed socket\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb00fd\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0102, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,608,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb00fc : Unable to read additional data from server sessionid 0x100035c67eb00fc, likely server has closed socket\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb0102 : Unable to read additional data from server sessionid 0x100035c67eb0102, likely server has closed socket\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb010e\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb00fc\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010a, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,607,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb010a\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0103, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,606,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb0102\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb0103\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb0103 : Unable to read additional data from server sessionid 0x100035c67eb0103, likely server has closed socket\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb00fe, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,612,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010c, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,614,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb00fe : Unable to read additional data from server sessionid 0x100035c67eb00fe, likely server has closed socket\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb00fb, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,610,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb00fe\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb00ff, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,609,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb010c : Unable to read additional data from server sessionid 0x100035c67eb010c, likely server has closed socket\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb010c\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb00fb\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb00fb : Unable to read additional data from server sessionid 0x100035c67eb00fb, likely server has closed socket\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0107, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,611,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0100, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,613,0  request:: null response:: null\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb00ff : Unable to read additional data from server sessionid 0x100035c67eb00ff, likely server has closed socket\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb0107\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb0100 : Unable to read additional data from server sessionid 0x100035c67eb0100, likely server has closed socket\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb0107 : Unable to read additional data from server sessionid 0x100035c67eb0107, likely server has closed socket\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb0100\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb00ff\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 44, boot = -840, init = 884, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 45, boot = -837, init = 882, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 43, boot = -841, init = 883, finish = 1\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 42, boot = -834, init = 876, finish = 0\n",
      "24/12/04 16:53:31 INFO PythonRunner: Times: total = 41, boot = -868, init = 909, finish = 0\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Session: 0x100035c67eb0109 closed\n",
      "24/12/04 16:53:31 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb0109\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Session: 0x100035c67eb010e closed\n",
      "24/12/04 16:53:31 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb010e\n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0101, packet:: clientPath:/hbase serverPath:/hbase finished:false header:: 2,8  replyHeader:: 2,614,0  request:: '/hbase,F  response:: v{'meta-region-server,'rs,'splitWAL,'backup-masters,'flush-table-proc,'master-maintenance,'online-snapshot,'master,'switch,'running,'draining,'namespace,'hbaseid,'table} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0105, packet:: clientPath:/hbase serverPath:/hbase finished:false header:: 2,8  replyHeader:: 2,614,0  request:: '/hbase,F  response:: v{'meta-region-server,'rs,'splitWAL,'backup-masters,'flush-table-proc,'master-maintenance,'online-snapshot,'master,'switch,'running,'draining,'namespace,'hbaseid,'table} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010d, packet:: clientPath:/hbase serverPath:/hbase finished:false header:: 2,8  replyHeader:: 2,614,0  request:: '/hbase,F  response:: v{'meta-region-server,'rs,'splitWAL,'backup-masters,'flush-table-proc,'master-maintenance,'online-snapshot,'master,'switch,'running,'draining,'namespace,'hbaseid,'table} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0104, packet:: clientPath:/hbase serverPath:/hbase finished:false header:: 2,8  replyHeader:: 2,614,0  request:: '/hbase,F  response:: v{'meta-region-server,'rs,'splitWAL,'backup-masters,'flush-table-proc,'master-maintenance,'online-snapshot,'master,'switch,'running,'draining,'namespace,'hbaseid,'table} \n",
      "24/12/04 16:53:31 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010b, packet:: clientPath:/hbase serverPath:/hbase finished:false header:: 2,8  replyHeader:: 2,614,0  request:: '/hbase,F  response:: v{'meta-region-server,'rs,'splitWAL,'backup-masters,'flush-table-proc,'master-maintenance,'online-snapshot,'master,'switch,'running,'draining,'namespace,'hbaseid,'table} \n",
      "24/12/04 16:53:31 INFO ZooKeeper: Session: 0x100035c67eb0107 closed\n",
      "24/12/04 16:53:31 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb0107\n",
      "24/12/04 16:53:31 INFO ZooKeeper: Session: 0x100035c67eb0100 closed\n",
      "24/12/04 16:53:31 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb0100\n",
      "24/12/04 16:53:32 INFO ZooKeeper: Session: 0x100035c67eb00fb closed\n",
      "24/12/04 16:53:32 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb00fb\n",
      "24/12/04 16:53:32 INFO ZooKeeper: Session: 0x100035c67eb00ff closed\n",
      "24/12/04 16:53:32 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb00ff\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==========================================>              (15 + 5) / 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/04 16:53:32 INFO ZooKeeper: Session: 0x100035c67eb0103 closed\n",
      "24/12/04 16:53:32 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb0103\n",
      "24/12/04 16:53:32 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb010c\n",
      "24/12/04 16:53:32 INFO ZooKeeper: Session: 0x100035c67eb010c closed\n",
      "24/12/04 16:53:32 INFO ZooKeeper: Session: 0x100035c67eb010a closed\n",
      "24/12/04 16:53:32 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb010a\n",
      "24/12/04 16:53:32 INFO ZooKeeper: Session: 0x100035c67eb00fe closed\n",
      "24/12/04 16:53:32 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb00fe\n",
      "24/12/04 16:53:32 INFO ZooKeeper: Session: 0x100035c67eb00fc closed\n",
      "24/12/04 16:53:32 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb00fc\n",
      "24/12/04 16:53:32 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb0102\n",
      "24/12/04 16:53:32 INFO ZooKeeper: Session: 0x100035c67eb0102 closed\n",
      "24/12/04 16:53:32 INFO ZooKeeper: Session: 0x100035c67eb0106 closed\n",
      "24/12/04 16:53:32 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb0106\n",
      "24/12/04 16:53:32 INFO ZooKeeper: Session: 0x100035c67eb00fd closed\n",
      "24/12/04 16:53:32 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb00fd\n",
      "24/12/04 16:53:32 INFO ZooKeeper: Session: 0x100035c67eb0108 closed\n",
      "24/12/04 16:53:32 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb0108\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010d, packet:: clientPath:/hbase/meta-region-server serverPath:/hbase/meta-region-server finished:false header:: 3,4  replyHeader:: 3,614,0  request:: '/hbase/meta-region-server,F  response:: #ffffffff000146d61737465723a31363030302cffffffebffffff91ffffffb862ffffff976effffffd150425546a15a96861646f6f7031313310ffffff947d18ffffffbcffffffe5ffffffa7fffffffaffffffb832100183,s{32,33,1733275816357,1733275816897,1,0,0,0,56,0,32} \n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0104, packet:: clientPath:/hbase/meta-region-server serverPath:/hbase/meta-region-server finished:false header:: 3,4  replyHeader:: 3,614,0  request:: '/hbase/meta-region-server,F  response:: #ffffffff000146d61737465723a31363030302cffffffebffffff91ffffffb862ffffff976effffffd150425546a15a96861646f6f7031313310ffffff947d18ffffffbcffffffe5ffffffa7fffffffaffffffb832100183,s{32,33,1733275816357,1733275816897,1,0,0,0,56,0,32} \n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0105, packet:: clientPath:/hbase/meta-region-server serverPath:/hbase/meta-region-server finished:false header:: 3,4  replyHeader:: 3,614,0  request:: '/hbase/meta-region-server,F  response:: #ffffffff000146d61737465723a31363030302cffffffebffffff91ffffffb862ffffff976effffffd150425546a15a96861646f6f7031313310ffffff947d18ffffffbcffffffe5ffffffa7fffffffaffffffb832100183,s{32,33,1733275816357,1733275816897,1,0,0,0,56,0,32} \n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0101, packet:: clientPath:/hbase/meta-region-server serverPath:/hbase/meta-region-server finished:false header:: 3,4  replyHeader:: 3,614,0  request:: '/hbase/meta-region-server,F  response:: #ffffffff000146d61737465723a31363030302cffffffebffffff91ffffffb862ffffff976effffffd150425546a15a96861646f6f7031313310ffffff947d18ffffffbcffffffe5ffffffa7fffffffaffffffb832100183,s{32,33,1733275816357,1733275816897,1,0,0,0,56,0,32} \n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010b, packet:: clientPath:/hbase/meta-region-server serverPath:/hbase/meta-region-server finished:false header:: 3,4  replyHeader:: 3,614,0  request:: '/hbase/meta-region-server,F  response:: #ffffffff000146d61737465723a31363030302cffffffebffffff91ffffffb862ffffff976effffffd150425546a15a96861646f6f7031313310ffffff947d18ffffffbcffffffe5ffffffa7fffffffaffffffb832100183,s{32,33,1733275816357,1733275816897,1,0,0,0,56,0,32} \n",
      "24/12/04 16:53:32 DEBUG RpcConnection: Using SIMPLE authentication for service=ClientService, sasl=false\n",
      "24/12/04 16:53:32 DEBUG RpcConnection: Using SIMPLE authentication for service=ClientService, sasl=false\n",
      "24/12/04 16:53:32 DEBUG RpcConnection: Using SIMPLE authentication for service=ClientService, sasl=false\n",
      "24/12/04 16:53:32 DEBUG RpcConnection: Using SIMPLE authentication for service=ClientService, sasl=false\n",
      "24/12/04 16:53:32 DEBUG RpcConnection: Using SIMPLE authentication for service=ClientService, sasl=false\n",
      "24/12/04 16:53:32 DEBUG AbstractByteBuf: -Dorg.apache.hbase.thirdparty.io.netty.buffer.checkAccessible: true\n",
      "24/12/04 16:53:32 DEBUG AbstractByteBuf: -Dorg.apache.hbase.thirdparty.io.netty.buffer.checkBounds: true\n",
      "24/12/04 16:53:32 DEBUG ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector@65d328a8\n",
      "24/12/04 16:53:32 DEBUG DefaultChannelId: -Dio.netty.processId: 14957 (auto-detected)\n",
      "24/12/04 16:53:32 DEBUG NetUtil: -Djava.net.preferIPv4Stack: false\n",
      "24/12/04 16:53:32 DEBUG NetUtil: -Djava.net.preferIPv6Addresses: false\n",
      "24/12/04 16:53:32 DEBUG NetUtilInitializations: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)\n",
      "24/12/04 16:53:32 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 4096\n",
      "24/12/04 16:53:32 DEBUG DefaultChannelId: -Dio.netty.machineId: 02:42:ac:ff:fe:19:01:0d (auto-detected)\n",
      "24/12/04 16:53:32 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 37\n",
      "24/12/04 16:53:32 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 37\n",
      "24/12/04 16:53:32 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192\n",
      "24/12/04 16:53:32 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 9\n",
      "24/12/04 16:53:32 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 4194304\n",
      "24/12/04 16:53:32 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256\n",
      "24/12/04 16:53:32 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64\n",
      "24/12/04 16:53:32 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768\n",
      "24/12/04 16:53:32 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192\n",
      "24/12/04 16:53:32 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimIntervalMillis: 0\n",
      "24/12/04 16:53:32 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.useCacheForAllThreads: false\n",
      "24/12/04 16:53:32 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023\n",
      "24/12/04 16:53:32 DEBUG ByteBufUtil: -Dio.netty.allocator.type: pooled\n",
      "24/12/04 16:53:32 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 0\n",
      "24/12/04 16:53:32 DEBUG ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384\n",
      "24/12/04 16:53:32 DEBUG Recycler: -Dio.netty.recycler.maxCapacityPerThread: 4096\n",
      "24/12/04 16:53:32 DEBUG Recycler: -Dio.netty.recycler.ratio: 8\n",
      "24/12/04 16:53:32 DEBUG Recycler: -Dio.netty.recycler.chunkSize: 32\n",
      "24/12/04 16:53:32 DEBUG Recycler: -Dio.netty.recycler.blocking: false\n",
      "24/12/04 16:53:32 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x1593bff5 to 127.0.0.1:2181\n",
      "24/12/04 16:53:32 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x60f13965 to 127.0.0.1:2181\n",
      "24/12/04 16:53:32 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x78b23790 to 127.0.0.1:2181\n",
      "24/12/04 16:53:32 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x03984772 to 127.0.0.1:2181\n",
      "24/12/04 16:53:32 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x25eb0d12 to 127.0.0.1:2181\n",
      "24/12/04 16:53:32 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:32 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:32 DEBUG ZooKeeper: Closing session: 0x100035c67eb0105\n",
      "24/12/04 16:53:32 DEBUG ZooKeeper: Closing session: 0x100035c67eb0104\n",
      "24/12/04 16:53:32 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:32 DEBUG ZooKeeper: Closing session: 0x100035c67eb010d\n",
      "24/12/04 16:53:32 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:32 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/04 16:53:32 DEBUG ZooKeeper: Closing session: 0x100035c67eb010b\n",
      "24/12/04 16:53:32 DEBUG ZooKeeper: Closing session: 0x100035c67eb0101\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb010b\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb010d\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb0104\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb0105\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Closing client for session: 0x100035c67eb0101\n",
      "24/12/04 16:53:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000015_0\n",
      "24/12/04 16:53:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000007_0\n",
      "24/12/04 16:53:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000019_0\n",
      "24/12/04 16:53:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000003_0\n",
      "24/12/04 16:53:32 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412041653319151691516548704819_0021_m_000011_0\n",
      "24/12/04 16:53:32 INFO Executor: Finished task 3.0 in stage 5.0 (TID 63). 1938 bytes result sent to driver\n",
      "24/12/04 16:53:32 INFO Executor: Finished task 19.0 in stage 5.0 (TID 79). 1938 bytes result sent to driver\n",
      "24/12/04 16:53:32 INFO Executor: Finished task 7.0 in stage 5.0 (TID 67). 1938 bytes result sent to driver\n",
      "24/12/04 16:53:32 INFO Executor: Finished task 11.0 in stage 5.0 (TID 71). 1938 bytes result sent to driver\n",
      "24/12/04 16:53:32 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 4\n",
      "24/12/04 16:53:32 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 3\n",
      "24/12/04 16:53:32 INFO Executor: Finished task 15.0 in stage 5.0 (TID 75). 1938 bytes result sent to driver\n",
      "24/12/04 16:53:32 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 2\n",
      "24/12/04 16:53:32 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 1\n",
      "24/12/04 16:53:32 DEBUG ExecutorMetricsPoller: stageTCMP: (5, 0) -> 0\n",
      "24/12/04 16:53:32 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 63) in 1571 ms on hadoop113 (executor driver) (16/20)\n",
      "24/12/04 16:53:32 INFO TaskSetManager: Finished task 19.0 in stage 5.0 (TID 79) in 1568 ms on hadoop113 (executor driver) (17/20)\n",
      "24/12/04 16:53:32 INFO TaskSetManager: Finished task 7.0 in stage 5.0 (TID 67) in 1570 ms on hadoop113 (executor driver) (18/20)\n",
      "24/12/04 16:53:32 INFO TaskSetManager: Finished task 11.0 in stage 5.0 (TID 71) in 1569 ms on hadoop113 (executor driver) (19/20)\n",
      "24/12/04 16:53:32 INFO TaskSetManager: Finished task 15.0 in stage 5.0 (TID 75) in 1568 ms on hadoop113 (executor driver) (20/20)\n",
      "24/12/04 16:53:32 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/12/04 16:53:32 INFO DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:83) finished in 1.596 s\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010d, packet:: clientPath:null serverPath:null finished:false header:: 4,-11  replyHeader:: 4,615,0  request:: null response:: null\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb010b, packet:: clientPath:null serverPath:null finished:false header:: 4,-11  replyHeader:: 4,616,0  request:: null response:: null\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb010d : Unable to read additional data from server sessionid 0x100035c67eb010d, likely server has closed socket\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb010b\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb010d\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb010b : Unable to read additional data from server sessionid 0x100035c67eb010b, likely server has closed socket\n",
      "24/12/04 16:53:32 DEBUG DAGScheduler: After removal of stage 5, remaining stages = 0\n",
      "24/12/04 16:53:32 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/04 16:53:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0105, packet:: clientPath:null serverPath:null finished:false header:: 4,-11  replyHeader:: 4,617,0  request:: null response:: null\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0104, packet:: clientPath:null serverPath:null finished:false header:: 4,-11  replyHeader:: 4,618,0  request:: null response:: null\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb0105\n",
      "24/12/04 16:53:32 INFO DAGScheduler: Job 5 finished: runJob at SparkHadoopWriter.scala:83, took 1.599644 s\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb0104\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb0104 : Unable to read additional data from server sessionid 0x100035c67eb0104, likely server has closed socket\n",
      "24/12/04 16:53:32 INFO SparkHadoopWriter: Start to commit write Job job_202412041653319151691516548704819_0021.\n",
      "24/12/04 16:53:32 WARN FileOutputCommitter: Output Path is null in commitJob()\n",
      "24/12/04 16:53:32 INFO SparkHadoopWriter: Write Job job_202412041653319151691516548704819_0021 committed. Elapsed time: 0 ms.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/04 16:53:32 DEBUG ClientCnxn: Reading reply sessionid:0x100035c67eb0101, packet:: clientPath:null serverPath:null finished:false header:: 4,-11  replyHeader:: 4,619,0  request:: null response:: null\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: Disconnecting client for session: 0x100035c67eb0101\n",
      "24/12/04 16:53:32 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x100035c67eb0101 : Unable to read additional data from server sessionid 0x100035c67eb0101, likely server has closed socket\n",
      "24/12/04 16:53:32 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.ExecutedCommandExec\",\"num-children\":0,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]}]\n",
      "24/12/04 16:53:32 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#16, city#17, name#18], false\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n"
     ]
    }
   ],
   "source": [
    "#write to hbase\n",
    "goalsDF.write\\\n",
    ".format(\"org.apache.hadoop.hbase.spark\")\\\n",
    ".option(\"hbase.columns.mapping\",\"id STRING :key, name STRING cf:name, city STRING cf:city\")\\\n",
    ".option(\"hbase.namespace\", \"default\")\\\n",
    ".option(\"hbase.table\", \"testspark\")\\\n",
    ".option(\"hbase.spark.use.hbasecontext\", False)\\\n",
    ".save() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "826763ea-68ee-455b-9dd1-893745d313f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/04 16:53:32 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#16, city#17, name#18], false\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:32 DEBUG KafkaRelationVisitor: Checking if provider is KafkaSourceProvider\n",
      "24/12/04 16:53:32 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:32 DEBUG KafkaRelationVisitor: Kafka classes are not available to check whether provider is KafkaSourceProvider\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "24/12/04 16:53:32 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:32 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:32 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:32 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #101 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:32 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #101\n",
      "24/12/04 16:53:32 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:32 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 566.1 KiB, free 364.5 MiB)\n",
      "24/12/04 16:53:32 DEBUG BlockManager: Put block broadcast_7 locally took 3 ms\n",
      "24/12/04 16:53:32 DEBUG BlockManager: Putting block broadcast_7 without replication took 3 ms\n",
      "24/12/04 16:53:32 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #102 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:32 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #102\n",
      "24/12/04 16:53:32 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 2ms\n",
      "24/12/04 16:53:32 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 364.5 MiB)\n",
      "24/12/04 16:53:32 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_7_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:32 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop113:38317 (size: 38.4 KiB, free: 366.1 MiB)\n",
      "24/12/04 16:53:32 DEBUG BlockManagerMaster: Updated info of block broadcast_7_piece0\n",
      "24/12/04 16:53:32 DEBUG BlockManager: Told master about block broadcast_7_piece0\n",
      "24/12/04 16:53:32 DEBUG BlockManager: Put block broadcast_7_piece0 locally took 0 ms\n",
      "24/12/04 16:53:32 DEBUG BlockManager: Putting block broadcast_7_piece0 without replication took 0 ms\n",
      "24/12/04 16:53:32 INFO SparkContext: Created broadcast 7 from broadcast at HBaseContext.scala:71\n",
      "24/12/04 16:53:32 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #103 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:32 DEBUG SparkSQLExecutionContext: Posting event for start 3: io.openlineage.client.OpenLineage$RunEvent@31d9d737\n",
      "24/12/04 16:53:32 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #103\n",
      "24/12/04 16:53:32 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:32 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:31.168Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-ce3b-71ba-98e8-85d4b424caf0\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:32 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:31.168Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-ce3b-71ba-98e8-85d4b424caf0\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:32 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@2529ac26, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:32 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:32 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@2529ac26, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:32 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_save_into_data_source_command.testspark), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306011168}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306012964}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1733306011168, status=STARTED}, dataProcessInstanceProperties={name=01939116-ce3b-71ba-98e8-85d4b424caf0, created={actor=urn:li:corpuser:datahub, time=1733306011168}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-ce3b-71ba-98e8-85d4b424caf0, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306011168, eventFormatter=datahub.event.EventFormatter@5ab6b7db), from {\"eventTime\":\"2024-12-04T09:53:31.168Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-ce3b-71ba-98e8-85d4b424caf0\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:32 INFO DatahubEventEmitter: Collecting lineage successfully in 1 ms\n",
      "24/12/04 16:53:32 INFO AsyncEventQueue: Process of event SparkListenerSQLExecutionStart(3,save at NativeMethodAccessorImpl.java:0,org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "java.lang.reflect.Method.invoke(Method.java:498)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:282)\n",
      "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750),== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand (1)\n",
      "   +- SaveIntoDataSourceCommand (2)\n",
      "         +- LogicalRDD (3)\n",
      "\n",
      "\n",
      "(1) Execute SaveIntoDataSourceCommand\n",
      "Output: []\n",
      "\n",
      "(2) SaveIntoDataSourceCommand\n",
      "Arguments: org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, [hbase.columns.mapping=id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace=default, hbase.table=testspark, hbase.spark.use.hbasecontext=false], ErrorIfExists\n",
      "\n",
      "(3) LogicalRDD\n",
      "Arguments: [id#16, city#17, name#18], false\n",
      "\n",
      ",org.apache.spark.sql.execution.SparkPlanInfo@e60025fe,1733306011168,Map()) by listener DatahubSparkListener took 1.770004586s.\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/04 16:53:32 DEBUG SparkSQLExecutionContext: SparkListenerJobStart - executionId: 3\n",
      "24/12/04 16:53:32 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]\n",
      "24/12/04 16:53:32 DEBUG DataSource: Some paths were ignored:\n",
      "  \n",
      "24/12/04 16:53:32 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.ExecutedCommandExec\",\"num-children\":0,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]}]\n",
      "24/12/04 16:53:32 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#16, city#17, name#18], false\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:32 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#16, city#17, name#18], false\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:32 DEBUG KafkaRelationVisitor: Checking if provider is KafkaSourceProvider\n",
      "24/12/04 16:53:32 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:32 DEBUG KafkaRelationVisitor: Kafka classes are not available to check whether provider is KafkaSourceProvider\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "24/12/04 16:53:32 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:32 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:32 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:32 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #104 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing\n",
      "24/12/04 16:53:32 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #104\n",
      "24/12/04 16:53:32 DEBUG ProtobufRpcEngine2: Call: getListing took 1ms\n",
      "24/12/04 16:53:32 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 566.1 KiB, free 363.9 MiB)\n",
      "24/12/04 16:53:32 DEBUG BlockManager: Put block broadcast_8 locally took 2 ms\n",
      "24/12/04 16:53:32 DEBUG BlockManager: Putting block broadcast_8 without replication took 2 ms\n",
      "24/12/04 16:53:32 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 363.9 MiB)\n",
      "24/12/04 16:53:32 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_8_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:32 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hadoop113:38317 (size: 38.4 KiB, free: 366.0 MiB)\n",
      "24/12/04 16:53:32 DEBUG BlockManagerMaster: Updated info of block broadcast_8_piece0\n",
      "24/12/04 16:53:32 DEBUG BlockManager: Told master about block broadcast_8_piece0\n",
      "24/12/04 16:53:32 DEBUG BlockManager: Put block broadcast_8_piece0 locally took 1 ms\n",
      "24/12/04 16:53:32 DEBUG BlockManager: Putting block broadcast_8_piece0 without replication took 1 ms\n",
      "24/12/04 16:53:32 INFO SparkContext: Created broadcast 8 from broadcast at HBaseContext.scala:71\n",
      "24/12/04 16:53:32 INFO InMemoryFileIndex: It took 14 ms to list leaf files for 1 paths.\n",
      "24/12/04 16:53:32 DEBUG SparkSQLExecutionContext: Posting event for start 3: io.openlineage.client.OpenLineage$RunEvent@5c68684a\n",
      "24/12/04 16:53:32 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:31.332Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-ce3b-71ba-98e8-85d4b424caf0\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":5},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:32 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:31.332Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-ce3b-71ba-98e8-85d4b424caf0\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":5},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:32 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@189b62c7, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:32 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:32 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@189b62c7, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:32 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={jobId=5, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_save_into_data_source_command.testspark), jobInfo={customProperties={jobId=5, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306011332}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306012991}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1733306011332, status=STARTED}, dataProcessInstanceProperties={name=01939116-ce3b-71ba-98e8-85d4b424caf0, created={actor=urn:li:corpuser:datahub, time=1733306011332}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-ce3b-71ba-98e8-85d4b424caf0, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306011332, eventFormatter=datahub.event.EventFormatter@7917c0ff), from {\"eventTime\":\"2024-12-04T09:53:31.332Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-ce3b-71ba-98e8-85d4b424caf0\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":5},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:32 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onJobStart completed successfully in 27 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:32 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/04 16:53:32 DEBUG SparkSQLExecutionContext: SparkListenerJobEnd - executionId: 3\n",
      "24/12/04 16:53:32 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]\n",
      "24/12/04 16:53:32 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.ExecutedCommandExec\",\"num-children\":0,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]}]\n",
      "24/12/04 16:53:32 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#16, city#17, name#18], false\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:32 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#16, city#17, name#18], false\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:32 DEBUG KafkaRelationVisitor: Checking if provider is KafkaSourceProvider\n",
      "24/12/04 16:53:32 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:32 DEBUG KafkaRelationVisitor: Kafka classes are not available to check whether provider is KafkaSourceProvider\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "24/12/04 16:53:32 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:32 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:32 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 566.1 KiB, free 363.3 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_9 locally took 1 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_9 without replication took 1 ms\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$mergeSchemasInParallel$2\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$mergeSchemasInParallel$2) is now cleaned +++\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 363.3 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_9_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:33 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on hadoop113:38317 (size: 38.4 KiB, free: 366.0 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMaster: Updated info of block broadcast_9_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Told master about block broadcast_9_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_9_piece0 locally took 0 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_9_piece0 without replication took 0 ms\n",
      "24/12/04 16:53:33 INFO SparkContext: Created broadcast 9 from broadcast at HBaseContext.scala:71\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: Posting event for end 3: io.openlineage.client.OpenLineage$RunEvent@714e7fdb\n",
      "24/12/04 16:53:33 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:32.931Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-ce3b-71ba-98e8-85d4b424caf0\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:32.931Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-ce3b-71ba-98e8-85d4b424caf0\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@3a7bab41, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@3a7bab41, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_save_into_data_source_command.testspark), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306012931}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306013015}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1733306012931, status=STARTED}, dataProcessInstanceProperties={name=01939116-ce3b-71ba-98e8-85d4b424caf0, created={actor=urn:li:corpuser:datahub, time=1733306012931}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-ce3b-71ba-98e8-85d4b424caf0, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306012931, eventFormatter=datahub.event.EventFormatter@34f7a16d), from {\"eventTime\":\"2024-12-04T09:53:32.931Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-ce3b-71ba-98e8-85d4b424caf0\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Collecting lineage successfully in 1 ms\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onJobEnd completed successfully in 23 ms\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionEnd - executionId: 3\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$collect$2\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.ExecutedCommandExec\",\"num-children\":0,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":16,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":17,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":18,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]}]\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#16, city#17, name#18], false\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#16, city#17, name#18], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@7718aa26, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#16, city#17, name#18], false\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 DEBUG KafkaRelationVisitor: Checking if provider is KafkaSourceProvider\n",
      "24/12/04 16:53:33 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:33 DEBUG KafkaRelationVisitor: Kafka classes are not available to check whether provider is KafkaSourceProvider\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "24/12/04 16:53:33 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:33 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:33 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "24/12/04 16:53:33 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 23 took 0.000042 seconds\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Got job 6 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: submitStage(ResultStage 6 (name=parquet at NativeMethodAccessorImpl.java:0;jobs=6))\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: missing: List()\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[23] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: submitMissingTasks(ResultStage 6)\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 566.1 KiB, free 362.7 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_10 locally took 1 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_10 without replication took 1 ms\n",
      "24/12/04 16:53:33 INFO ZooKeeper: Session: 0x100035c67eb010d closed\n",
      "24/12/04 16:53:33 INFO ZooKeeper: Session: 0x100035c67eb010b closed\n",
      "24/12/04 16:53:33 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb010d\n",
      "24/12/04 16:53:33 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb010b\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 362.7 MiB)\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 103.7 KiB, free 362.6 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_10_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_11 locally took 0 ms\n",
      "24/12/04 16:53:33 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on hadoop113:38317 (size: 38.4 KiB, free: 365.9 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_11 without replication took 0 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMaster: Updated info of block broadcast_10_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Told master about block broadcast_10_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_10_piece0 locally took 1 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_10_piece0 without replication took 1 ms\n",
      "24/12/04 16:53:33 INFO SparkContext: Created broadcast 10 from broadcast at HBaseContext.scala:71\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 362.6 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_11_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:33 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on hadoop113:38317 (size: 37.2 KiB, free: 365.9 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMaster: Updated info of block broadcast_11_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Told master about block broadcast_11_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_11_piece0 locally took 1 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_11_piece0 without replication took 1 ms\n",
      "24/12/04 16:53:33 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: Posting event for end 3: {\"eventTime\":\"2024-12-04T09:53:32.932Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-ce3b-71ba-98e8-85d4b424caf0\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[23] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/12/04 16:53:33 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "24/12/04 16:53:33 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:32.932Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-ce3b-71ba-98e8-85d4b424caf0\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:32.932Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-ce3b-71ba-98e8-85d4b424caf0\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:33 DEBUG TaskSetManager: Epoch for TaskSet 6.0: 0\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@70bb273a, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "24/12/04 16:53:33 DEBUG TaskSetManager: Valid locality levels for TaskSet 6.0: NO_PREF, ANY\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@70bb273a, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_6.0, runningTasks: 0\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_save_into_data_source_command.testspark), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306012932}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306013041}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1733306012932, status=COMPLETE}, dataProcessInstanceProperties={name=01939116-ce3b-71ba-98e8-85d4b424caf0, created={actor=urn:li:corpuser:datahub, time=1733306012932}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-ce3b-71ba-98e8-85d4b424caf0, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306012932, eventFormatter=datahub.event.EventFormatter@fa0b165), from {\"eventTime\":\"2024-12-04T09:53:32.932Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-ce3b-71ba-98e8-85d4b424caf0\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Collecting lineage successfully in 1 ms\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 80) (hadoop113, executor driver, partition 0, PROCESS_LOCAL, 4643 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:33 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "24/12/04 16:53:33 INFO Executor: Running task 0.0 in stage 6.0 (TID 80)\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (6, 0) -> 1\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Getting local block broadcast_11\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Level for block broadcast_11 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "24/12/04 16:53:33 DEBUG RddExecutionContext: setActiveJob within RddExecutionContext org.apache.spark.scheduler.ActiveJob@7baec3d6\n",
      "24/12/04 16:53:33 DEBUG RddExecutionContext: flattenRDDs [MapPartitionsRDD[23] at parquet at NativeMethodAccessorImpl.java:0, ParallelCollectionRDD[22] at parquet at NativeMethodAccessorImpl.java:0]\n",
      "24/12/04 16:53:33 DEBUG RddExecutionContext: find Inputs within RddExecutionContext [MapPartitionsRDD[23] at parquet at NativeMethodAccessorImpl.java:0, ParallelCollectionRDD[22] at parquet at NativeMethodAccessorImpl.java:0]\n",
      "24/12/04 16:53:33 DEBUG RddPathUtils: ParallelCollectionRDD data: List((hdfs://hadoop:9000/tmp/test/part-00000-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet,469))\n",
      "24/12/04 16:53:33 DEBUG RddPathUtils: Found input hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 DEBUG RddPathUtils: ParallelCollectionRDD data: List((hdfs://hadoop:9000/tmp/test/part-00000-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet,469))\n",
      "24/12/04 16:53:33 DEBUG RddPathUtils: Found input hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 DEBUG DecryptionPropertiesFactory: DecryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:33 INFO RddExecutionContext: Config field is not HadoopMapRedWriteConfigUtil or HadoopMapReduceWriteConfigUtil, it's org.apache.spark.rdd.RDD$$Lambda$4717/1302926400\n",
      "24/12/04 16:53:33 INFO RddExecutionContext: Found job conf from RDD Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-rbf-default.xml, hdfs-site.xml, hdfs-rbf-site.xml\n",
      "24/12/04 16:53:33 DEBUG RddExecutionContext: JobConf Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-rbf-default.xml, hdfs-site.xml, hdfs-rbf-site.xml\n",
      "24/12/04 16:53:33 DEBUG RddExecutionContext: Path is null, trying to use old fashioned mapreduce api\n",
      "24/12/04 16:53:33 DEBUG RddPathUtils: ParallelCollectionRDD data: List((hdfs://hadoop:9000/tmp/test/part-00000-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet,469))\n",
      "24/12/04 16:53:33 DEBUG RddPathUtils: Found input hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 INFO RddExecutionContext: Found output path hdfs://hadoop:9000/tmp/test from RDD MapPartitionsRDD[23] at parquet at NativeMethodAccessorImpl.java:0\n",
      "24/12/04 16:53:33 DEBUG RddExecutionContext: start SparkListenerJobStart SparkListenerJobStart(6,1733306013028,WrappedArray(org.apache.spark.scheduler.StageInfo@3a21429c),{spark.rdd.scope={\"id\":\"52\",\"name\":\"collect\"}, spark.rdd.scope.noOverride=true})\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #105 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #105\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getBlockLocations took 0ms\n",
      "24/12/04 16:53:33 DEBUG RddExecutionContext: Posting event for start SparkListenerJobStart(6,1733306013028,WrappedArray(org.apache.spark.scheduler.StageInfo@3a21429c),{spark.rdd.scope={\"id\":\"52\",\"name\":\"collect\"}, spark.rdd.scope.noOverride=true}): io.openlineage.client.OpenLineage$RunEvent@742600f9\n",
      "24/12/04 16:53:33 DEBUG DFSClient: newInfo = LocatedBlocks{;  fileLength=469;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741994_1170; getBlockSize()=469; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741994_1170; getBlockSize()=469; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null}\n",
      "24/12/04 16:53:33 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.028Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-d574-7b65-94f7-37e32d700c15\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":6},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.map_partitions_parallel_collection\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"RDD_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\"}],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\"}]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.028Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-d574-7b65-94f7-37e32d700c15\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":6},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.map_partitions_parallel_collection\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"RDD_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\"}],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\"}]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@5cf50e80, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@5cf50e80, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: File length 469\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={jobId=6, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.map_partitions_parallel_collection), jobInfo={customProperties={jobId=6, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306013028}, name=map_partitions_parallel_collection, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306013063}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1733306013028, status=STARTED}, dataProcessInstanceProperties={name=01939116-d574-7b65-94f7-37e32d700c15, created={actor=urn:li:corpuser:datahub, time=1733306013028}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.map_partitions_parallel_collection), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-d574-7b65-94f7-37e32d700c15, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306013028, eventFormatter=datahub.event.EventFormatter@9d60db4), from {\"eventTime\":\"2024-12-04T09:53:33.028Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-d574-7b65-94f7-37e32d700c15\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":6},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.map_partitions_parallel_collection\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"RDD_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\"}],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\"}]}\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: reading footer index at 461\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Collecting lineage successfully in 1 ms\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onJobStart completed successfully in 22 ms\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "24/12/04 16:53:33 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.25.1.12, datanodeId = DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: read footer length: 457, footer index: 4\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: Finished to read all footer bytes.\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: FileMetaData(version:1, schema:[SchemaElement(name:spark_schema, num_children:3), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:id, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:name, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>)], num_rows:0, row_groups:null, key_value_metadata:[KeyValue(key:org.apache.spark.version, value:3.3.1), KeyValue(key:org.apache.spark.sql.parquet.row.metadata, value:{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]})], created_by:parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94))\n",
      "24/12/04 16:53:33 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(9)\n",
      "24/12/04 16:53:33 DEBUG ContextCleaner: Cleaning broadcast 9\n",
      "24/12/04 16:53:33 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 9\n",
      "24/12/04 16:53:33 DEBUG BlockManagerStorageEndpoint: removing broadcast 9\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Removing broadcast 9\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Removing block broadcast_9_piece0\n",
      "24/12/04 16:53:33 DEBUG MemoryStore: Block broadcast_9_piece0 of size 39339 dropped from memory (free 380220672)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_9_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:33 INFO BlockManagerInfo: Removed broadcast_9_piece0 on hadoop113:38317 in memory (size: 38.4 KiB, free: 365.9 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMaster: Updated info of block broadcast_9_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Told master about block broadcast_9_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Removing block broadcast_9\n",
      "24/12/04 16:53:33 DEBUG MemoryStore: Block broadcast_9 of size 579640 dropped from memory (free 380800312)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 9, response is 0\n",
      "24/12/04 16:53:33 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to hadoop113:36571\n",
      "24/12/04 16:53:33 DEBUG ContextCleaner: Cleaned broadcast 9\n",
      "24/12/04 16:53:33 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(7)\n",
      "24/12/04 16:53:33 DEBUG ContextCleaner: Cleaning broadcast 7\n",
      "24/12/04 16:53:33 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 7\n",
      "24/12/04 16:53:33 DEBUG BlockManagerStorageEndpoint: removing broadcast 7\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Removing broadcast 7\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Removing block broadcast_7_piece0\n",
      "24/12/04 16:53:33 DEBUG MemoryStore: Block broadcast_7_piece0 of size 39339 dropped from memory (free 380839651)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_7_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:33 INFO BlockManagerInfo: Removed broadcast_7_piece0 on hadoop113:38317 in memory (size: 38.4 KiB, free: 366.0 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMaster: Updated info of block broadcast_7_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Told master about block broadcast_7_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Removing block broadcast_7\n",
      "24/12/04 16:53:33 DEBUG MemoryStore: Block broadcast_7 of size 579640 dropped from memory (free 381419291)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 7, response is 0\n",
      "24/12/04 16:53:33 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to hadoop113:36571\n",
      "24/12/04 16:53:33 DEBUG ContextCleaner: Cleaned broadcast 7\n",
      "24/12/04 16:53:33 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(8)\n",
      "24/12/04 16:53:33 DEBUG ContextCleaner: Cleaning broadcast 8\n",
      "24/12/04 16:53:33 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 8\n",
      "24/12/04 16:53:33 DEBUG BlockManagerStorageEndpoint: removing broadcast 8\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Removing broadcast 8\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Removing block broadcast_8_piece0\n",
      "24/12/04 16:53:33 DEBUG MemoryStore: Block broadcast_8_piece0 of size 39339 dropped from memory (free 381458630)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_8_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:33 INFO BlockManagerInfo: Removed broadcast_8_piece0 on hadoop113:38317 in memory (size: 38.4 KiB, free: 366.0 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMaster: Updated info of block broadcast_8_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Told master about block broadcast_8_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Removing block broadcast_8\n",
      "24/12/04 16:53:33 DEBUG MemoryStore: Block broadcast_8 of size 579640 dropped from memory (free 382038270)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 8, response is 0\n",
      "24/12/04 16:53:33 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to hadoop113:36571\n",
      "24/12/04 16:53:33 DEBUG ContextCleaner: Cleaned broadcast 8\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"spark_schema\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"id\" ], [ \"city\" ], [ \"name\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      } ],\n",
      "      \"fieldCount\" : 3,\n",
      "      \"primitive\" : false,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"org.apache.spark.version\" : \"3.3.1\",\n",
      "      \"org.apache.spark.sql.parquet.row.metadata\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\",\n",
      "    \"fileDecryptor\" : null\n",
      "  },\n",
      "  \"blocks\" : [ ]\n",
      "}\n",
      "24/12/04 16:53:33 DEBUG DFSClient: DeadNode detection is not enabled or given block LocatedBlocks{;  fileLength=469;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741994_1170; getBlockSize()=469; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741994_1170; getBlockSize()=469; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null} is null, skip to remove node.\n",
      "24/12/04 16:53:33 INFO ZooKeeper: Session: 0x100035c67eb0101 closed\n",
      "24/12/04 16:53:33 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb0101\n",
      "24/12/04 16:53:33 INFO Executor: Finished task 0.0 in stage 6.0 (TID 80). 1726 bytes result sent to driver\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (6, 0) -> 0\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 80) in 91 ms on hadoop113 (executor driver) (1/1)\n",
      "24/12/04 16:53:33 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:33 INFO DAGScheduler: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0) finished in 0.103 s\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: After removal of stage 6, remaining stages = 0\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/04 16:53:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/04 16:53:33 DEBUG RddExecutionContext: end SparkListenerJobEnd SparkListenerJobEnd(6,1733306013133,JobSucceeded)\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Job 6 finished: parquet at NativeMethodAccessorImpl.java:0, took 0.105646 s\n",
      "24/12/04 16:53:33 DEBUG RddExecutionContext: Posting event for end SparkListenerJobEnd(6,1733306013133,JobSucceeded): io.openlineage.client.OpenLineage$RunEvent@4aa81e3\n",
      "24/12/04 16:53:33 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.133Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-d574-7b65-94f7-37e32d700c15\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.map_partitions_parallel_collection\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"RDD_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\"}],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\"}]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.133Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-d574-7b65-94f7-37e32d700c15\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.map_partitions_parallel_collection\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"RDD_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\"}],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\"}]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@40cf7847, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@40cf7847, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.map_partitions_parallel_collection), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306013133}, name=map_partitions_parallel_collection, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306013134}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1733306013133, status=COMPLETE}, dataProcessInstanceProperties={name=01939116-d574-7b65-94f7-37e32d700c15, created={actor=urn:li:corpuser:datahub, time=1733306013133}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.map_partitions_parallel_collection), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-d574-7b65-94f7-37e32d700c15, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306013133, eventFormatter=datahub.event.EventFormatter@34baefd0), from {\"eventTime\":\"2024-12-04T09:53:33.133Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-d574-7b65-94f7-37e32d700c15\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.map_partitions_parallel_collection\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"RDD_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\"}],\"outputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\"}]}\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onJobEnd completed successfully in 2 ms\n",
      "24/12/04 16:53:33 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/12/04 16:53:33 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/12/04 16:53:33 INFO FileSourceStrategy: Output Data Schema: struct<id: string, city: string, name: string ... 1 more fields>\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart\n",
      "24/12/04 16:53:33 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/04 16:53:33 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:33 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionStart - executionId: 4\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.GlobalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.LocalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.datasources.LogicalRelation\",\"num-children\":0,\"relation\":null,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":48,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":49,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":50,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"isStreaming\":false}]\n",
      "24/12/04 16:53:33 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private int columnartorow_batchIdx_0;\n",
      "/* 010 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[3];\n",
      "/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 012 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];\n",
      "/* 013 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     columnartorow_mutableStateArray_0[0] = inputs[0];\n",
      "/* 023 */\n",
      "/* 024 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 025 */\n",
      "/* 026 */   }\n",
      "/* 027 */\n",
      "/* 028 */   private void columnartorow_nextBatch_0() throws java.io.IOException {\n",
      "/* 029 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {\n",
      "/* 030 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();\n",
      "/* 031 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);\n",
      "/* 032 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());\n",
      "/* 033 */       columnartorow_batchIdx_0 = 0;\n",
      "/* 034 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);\n",
      "/* 035 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);\n",
      "/* 036 */       columnartorow_mutableStateArray_2[2] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(2);\n",
      "/* 037 */\n",
      "/* 038 */     }\n",
      "/* 039 */   }\n",
      "/* 040 */\n",
      "/* 041 */   protected void processNext() throws java.io.IOException {\n",
      "/* 042 */     if (columnartorow_mutableStateArray_1[0] == null) {\n",
      "/* 043 */       columnartorow_nextBatch_0();\n",
      "/* 044 */     }\n",
      "/* 045 */     while ( columnartorow_mutableStateArray_1[0] != null) {\n",
      "/* 046 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();\n",
      "/* 047 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;\n",
      "/* 048 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {\n",
      "/* 049 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;\n",
      "/* 050 */         boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 051 */         UTF8String columnartorow_value_0 = columnartorow_isNull_0 ? null : (columnartorow_mutableStateArray_2[0].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 052 */         boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 053 */         UTF8String columnartorow_value_1 = columnartorow_isNull_1 ? null : (columnartorow_mutableStateArray_2[1].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 054 */         boolean columnartorow_isNull_2 = columnartorow_mutableStateArray_2[2].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 055 */         UTF8String columnartorow_value_2 = columnartorow_isNull_2 ? null : (columnartorow_mutableStateArray_2[2].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 056 */         columnartorow_mutableStateArray_3[0].reset();\n",
      "/* 057 */\n",
      "/* 058 */         columnartorow_mutableStateArray_3[0].zeroOutNullBytes();\n",
      "/* 059 */\n",
      "/* 060 */         if (columnartorow_isNull_0) {\n",
      "/* 061 */           columnartorow_mutableStateArray_3[0].setNullAt(0);\n",
      "/* 062 */         } else {\n",
      "/* 063 */           columnartorow_mutableStateArray_3[0].write(0, columnartorow_value_0);\n",
      "/* 064 */         }\n",
      "/* 065 */\n",
      "/* 066 */         if (columnartorow_isNull_1) {\n",
      "/* 067 */           columnartorow_mutableStateArray_3[0].setNullAt(1);\n",
      "/* 068 */         } else {\n",
      "/* 069 */           columnartorow_mutableStateArray_3[0].write(1, columnartorow_value_1);\n",
      "/* 070 */         }\n",
      "/* 071 */\n",
      "/* 072 */         if (columnartorow_isNull_2) {\n",
      "/* 073 */           columnartorow_mutableStateArray_3[0].setNullAt(2);\n",
      "/* 074 */         } else {\n",
      "/* 075 */           columnartorow_mutableStateArray_3[0].write(2, columnartorow_value_2);\n",
      "/* 076 */         }\n",
      "/* 077 */         append((columnartorow_mutableStateArray_3[0].getRow()));\n",
      "/* 078 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }\n",
      "/* 079 */       }\n",
      "/* 080 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;\n",
      "/* 081 */       columnartorow_mutableStateArray_1[0] = null;\n",
      "/* 082 */       columnartorow_nextBatch_0();\n",
      "/* 083 */     }\n",
      "/* 084 */   }\n",
      "/* 085 */\n",
      "/* 086 */ }\n",
      "\n",
      "24/12/04 16:53:33 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private int columnartorow_batchIdx_0;\n",
      "/* 010 */   private org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[] columnartorow_mutableStateArray_2 = new org.apache.spark.sql.execution.vectorized.OnHeapColumnVector[3];\n",
      "/* 011 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] columnartorow_mutableStateArray_3 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 012 */   private org.apache.spark.sql.vectorized.ColumnarBatch[] columnartorow_mutableStateArray_1 = new org.apache.spark.sql.vectorized.ColumnarBatch[1];\n",
      "/* 013 */   private scala.collection.Iterator[] columnartorow_mutableStateArray_0 = new scala.collection.Iterator[1];\n",
      "/* 014 */\n",
      "/* 015 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 016 */     this.references = references;\n",
      "/* 017 */   }\n",
      "/* 018 */\n",
      "/* 019 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 020 */     partitionIndex = index;\n",
      "/* 021 */     this.inputs = inputs;\n",
      "/* 022 */     columnartorow_mutableStateArray_0[0] = inputs[0];\n",
      "/* 023 */\n",
      "/* 024 */     columnartorow_mutableStateArray_3[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 025 */\n",
      "/* 026 */   }\n",
      "/* 027 */\n",
      "/* 028 */   private void columnartorow_nextBatch_0() throws java.io.IOException {\n",
      "/* 029 */     if (columnartorow_mutableStateArray_0[0].hasNext()) {\n",
      "/* 030 */       columnartorow_mutableStateArray_1[0] = (org.apache.spark.sql.vectorized.ColumnarBatch)columnartorow_mutableStateArray_0[0].next();\n",
      "/* 031 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[1] /* numInputBatches */).add(1);\n",
      "/* 032 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(columnartorow_mutableStateArray_1[0].numRows());\n",
      "/* 033 */       columnartorow_batchIdx_0 = 0;\n",
      "/* 034 */       columnartorow_mutableStateArray_2[0] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(0);\n",
      "/* 035 */       columnartorow_mutableStateArray_2[1] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(1);\n",
      "/* 036 */       columnartorow_mutableStateArray_2[2] = (org.apache.spark.sql.execution.vectorized.OnHeapColumnVector) columnartorow_mutableStateArray_1[0].column(2);\n",
      "/* 037 */\n",
      "/* 038 */     }\n",
      "/* 039 */   }\n",
      "/* 040 */\n",
      "/* 041 */   protected void processNext() throws java.io.IOException {\n",
      "/* 042 */     if (columnartorow_mutableStateArray_1[0] == null) {\n",
      "/* 043 */       columnartorow_nextBatch_0();\n",
      "/* 044 */     }\n",
      "/* 045 */     while ( columnartorow_mutableStateArray_1[0] != null) {\n",
      "/* 046 */       int columnartorow_numRows_0 = columnartorow_mutableStateArray_1[0].numRows();\n",
      "/* 047 */       int columnartorow_localEnd_0 = columnartorow_numRows_0 - columnartorow_batchIdx_0;\n",
      "/* 048 */       for (int columnartorow_localIdx_0 = 0; columnartorow_localIdx_0 < columnartorow_localEnd_0; columnartorow_localIdx_0++) {\n",
      "/* 049 */         int columnartorow_rowIdx_0 = columnartorow_batchIdx_0 + columnartorow_localIdx_0;\n",
      "/* 050 */         boolean columnartorow_isNull_0 = columnartorow_mutableStateArray_2[0].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 051 */         UTF8String columnartorow_value_0 = columnartorow_isNull_0 ? null : (columnartorow_mutableStateArray_2[0].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 052 */         boolean columnartorow_isNull_1 = columnartorow_mutableStateArray_2[1].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 053 */         UTF8String columnartorow_value_1 = columnartorow_isNull_1 ? null : (columnartorow_mutableStateArray_2[1].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 054 */         boolean columnartorow_isNull_2 = columnartorow_mutableStateArray_2[2].isNullAt(columnartorow_rowIdx_0);\n",
      "/* 055 */         UTF8String columnartorow_value_2 = columnartorow_isNull_2 ? null : (columnartorow_mutableStateArray_2[2].getUTF8String(columnartorow_rowIdx_0));\n",
      "/* 056 */         columnartorow_mutableStateArray_3[0].reset();\n",
      "/* 057 */\n",
      "/* 058 */         columnartorow_mutableStateArray_3[0].zeroOutNullBytes();\n",
      "/* 059 */\n",
      "/* 060 */         if (columnartorow_isNull_0) {\n",
      "/* 061 */           columnartorow_mutableStateArray_3[0].setNullAt(0);\n",
      "/* 062 */         } else {\n",
      "/* 063 */           columnartorow_mutableStateArray_3[0].write(0, columnartorow_value_0);\n",
      "/* 064 */         }\n",
      "/* 065 */\n",
      "/* 066 */         if (columnartorow_isNull_1) {\n",
      "/* 067 */           columnartorow_mutableStateArray_3[0].setNullAt(1);\n",
      "/* 068 */         } else {\n",
      "/* 069 */           columnartorow_mutableStateArray_3[0].write(1, columnartorow_value_1);\n",
      "/* 070 */         }\n",
      "/* 071 */\n",
      "/* 072 */         if (columnartorow_isNull_2) {\n",
      "/* 073 */           columnartorow_mutableStateArray_3[0].setNullAt(2);\n",
      "/* 074 */         } else {\n",
      "/* 075 */           columnartorow_mutableStateArray_3[0].write(2, columnartorow_value_2);\n",
      "/* 076 */         }\n",
      "/* 077 */         append((columnartorow_mutableStateArray_3[0].getRow()));\n",
      "/* 078 */         if (shouldStop()) { columnartorow_batchIdx_0 = columnartorow_rowIdx_0 + 1; return; }\n",
      "/* 079 */       }\n",
      "/* 080 */       columnartorow_batchIdx_0 = columnartorow_numRows_0;\n",
      "/* 081 */       columnartorow_mutableStateArray_1[0] = null;\n",
      "/* 082 */       columnartorow_nextBatch_0();\n",
      "/* 083 */     }\n",
      "/* 084 */   }\n",
      "/* 085 */\n",
      "/* 086 */ }\n",
      "\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.CollectLimitExec\",\"num-children\":1,\"limit\":21,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.ColumnarToRowExec\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.InputAdapter\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.FileSourceScanExec\",\"num-children\":0,\"relation\":null,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":48,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":49,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":50,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"requiredSchema\":{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"partitionFilters\":[],\"dataFilters\":[],\"disableBucketedScan\":false}]\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#48,city#49,name#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://hadoop:9000/tmp/test], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,city:string,name:string>\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #106 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #106\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:33 DEBUG HdfsPathDataset: path: hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 INFO HdfsPathDataset: No path_spec_list configuration found. Falling back to creating dataset name with complete uri\n",
      "24/12/04 16:53:33 DEBUG PlanUtils: Path hdfs://hadoop:9000/tmp/test transformed to hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#48,city#49,name#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://hadoop:9000/tmp/test], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,city:string,name:string>\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 INFO RemovePathPatternUtils: Removing path pattern from dataset name /tmp/test\n",
      "24/12/04 16:53:33 DEBUG HdfsPathDataset: path: /tmp/test\n",
      "24/12/04 16:53:33 DEBUG RemovePathPatternUtils: Transformed path is /tmp/test\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: Posting event for start 4: io.openlineage.client.OpenLineage$RunEvent@533b42c2\n",
      "24/12/04 16:53:33 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.174Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.174Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@7282c348, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@7282c348, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Dataset URN: Optional[urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD)], alias_list: {}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Building input with /tmp/test\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306013174}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306013201}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1733306013174, status=STARTED}, dataProcessInstanceProperties={name=01939116-d5f7-7f29-82a1-b6534ebd5626, created={actor=urn:li:corpuser:datahub, time=1733306013174}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-d5f7-7f29-82a1-b6534ebd5626, inSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD), schemaMetadata={platformSchema={com.linkedin.schema.MySqlDDL={tableSchema=[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}}, schemaName=, fields=[{fieldPath=id, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=city, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=name, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}], version=1, hash=, platform=urn:li:dataPlatform:hdfs}, lineage=null)], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306013174, eventFormatter=datahub.event.EventFormatter@156449d4), from {\"eventTime\":\"2024-12-04T09:53:33.174Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/04 16:53:33 INFO CodeGenerator: Code generated in 18.347282 ms\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 356.4 KiB, free 364.0 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_12 locally took 1 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_12 without replication took 1 ms\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 35.1 KiB, free 364.0 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_12_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:33 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on hadoop113:38317 (size: 35.1 KiB, free: 366.0 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMaster: Updated info of block broadcast_12_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Told master about block broadcast_12_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_12_piece0 locally took 1 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_12_piece0 without replication took 1 ms\n",
      "24/12/04 16:53:33 INFO SparkContext: Created broadcast 12 from showString at NativeMethodAccessorImpl.java:0\n",
      "24/12/04 16:53:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/12/04 16:53:33 INFO ZooKeeper: Session: 0x100035c67eb0104 closed\n",
      "24/12/04 16:53:33 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb0104\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerDriverAccumUpdates\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "24/12/04 16:53:33 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 27 took 0.000089 seconds\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Got job 7 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Final stage: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: submitStage(ResultStage 7 (name=showString at NativeMethodAccessorImpl.java:0;jobs=7))\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: missing: List()\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: submitMissingTasks(ResultStage 7)\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: SparkListenerJobStart - executionId: 4\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.GlobalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.LocalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.datasources.LogicalRelation\",\"num-children\":0,\"relation\":null,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":48,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":49,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":50,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"isStreaming\":false}]\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.CollectLimitExec\",\"num-children\":1,\"limit\":21,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.ColumnarToRowExec\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.InputAdapter\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.FileSourceScanExec\",\"num-children\":0,\"relation\":null,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":48,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":49,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":50,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"requiredSchema\":{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"partitionFilters\":[],\"dataFilters\":[],\"disableBucketedScan\":false}]\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#48,city#49,name#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://hadoop:9000/tmp/test], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,city:string,name:string>\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #107 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #107\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:33 DEBUG HdfsPathDataset: path: hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 INFO HdfsPathDataset: No path_spec_list configuration found. Falling back to creating dataset name with complete uri\n",
      "24/12/04 16:53:33 DEBUG PlanUtils: Path hdfs://hadoop:9000/tmp/test transformed to hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#48,city#49,name#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://hadoop:9000/tmp/test], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,city:string,name:string>\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 13.7 KiB, free 363.9 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_13 locally took 0 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_13 without replication took 0 ms\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 363.9 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_13_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:33 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on hadoop113:38317 (size: 6.0 KiB, free: 366.0 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMaster: Updated info of block broadcast_13_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Told master about block broadcast_13_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_13_piece0 locally took 0 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_13_piece0 without replication took 0 ms\n",
      "24/12/04 16:53:33 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/12/04 16:53:33 INFO RemovePathPatternUtils: Removing path pattern from dataset name /tmp/test\n",
      "24/12/04 16:53:33 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "24/12/04 16:53:33 DEBUG TaskSetManager: Epoch for TaskSet 7.0: 0\n",
      "24/12/04 16:53:33 DEBUG HdfsPathDataset: path: /tmp/test\n",
      "24/12/04 16:53:33 DEBUG RemovePathPatternUtils: Transformed path is /tmp/test\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: Posting event for start 4: io.openlineage.client.OpenLineage$RunEvent@1dc4113d\n",
      "24/12/04 16:53:33 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.239Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":7},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.239Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":7},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 DEBUG TaskSetManager: Adding pending tasks took 1 ms\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@2a59c1b8, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@2a59c1b8, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Dataset URN: Optional[urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD)], alias_list: {}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Building input with /tmp/test\n",
      "24/12/04 16:53:33 DEBUG TaskSetManager: Valid locality levels for TaskSet 7.0: ANY\n",
      "24/12/04 16:53:33 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_7.0, runningTasks: 0\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={jobId=7, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), jobInfo={customProperties={jobId=7, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306013239}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306013253}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1733306013239, status=STARTED}, dataProcessInstanceProperties={name=01939116-d5f7-7f29-82a1-b6534ebd5626, created={actor=urn:li:corpuser:datahub, time=1733306013239}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-d5f7-7f29-82a1-b6534ebd5626, inSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD), schemaMetadata={platformSchema={com.linkedin.schema.MySqlDDL={tableSchema=[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}}, schemaName=, fields=[{fieldPath=id, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=city, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=name, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}], version=1, hash=, platform=urn:li:dataPlatform:hdfs}, lineage=null)], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306013239, eventFormatter=datahub.event.EventFormatter@6b266579), from {\"eventTime\":\"2024-12-04T09:53:33.239Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":7},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onJobStart completed successfully in 15 ms\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 81) (hadoop113, executor driver, partition 0, ANY, 4966 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:33 INFO Executor: Running task 0.0 in stage 7.0 (TID 81)\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (7, 0) -> 1\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Getting local block broadcast_13\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Level for block broadcast_13 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "24/12/04 16:53:33 INFO FileScanRDD: Reading File path: hdfs://hadoop:9000/tmp/test/part-00019-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, range: 0-971, partition values: [empty row]\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Getting local block broadcast_12\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Level for block broadcast_12 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #108 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #108\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:33 DEBUG DecryptionPropertiesFactory: DecryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #109 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #109\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getBlockLocations took 0ms\n",
      "24/12/04 16:53:33 DEBUG DFSClient: newInfo = LocatedBlocks{;  fileLength=971;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741995_1171; getBlockSize()=971; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741995_1171; getBlockSize()=971; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null}\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: File length 971\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: reading footer index at 963\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: read footer length: 724, footer index: 239\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: Finished to read all footer bytes.\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: FileMetaData(version:1, schema:[SchemaElement(name:spark_schema, num_children:3), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:id, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:name, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:null, key_value_metadata:[KeyValue(key:org.apache.spark.version, value:3.3.1), KeyValue(key:org.apache.spark.sql.parquet.row.metadata, value:{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]})], created_by:parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94))\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"spark_schema\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"id\" ], [ \"city\" ], [ \"name\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      } ],\n",
      "      \"fieldCount\" : 3,\n",
      "      \"primitive\" : false,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"org.apache.spark.version\" : \"3.3.1\",\n",
      "      \"org.apache.spark.sql.parquet.row.metadata\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\",\n",
      "    \"fileDecryptor\" : null\n",
      "  },\n",
      "  \"blocks\" : [ ]\n",
      "}\n",
      "24/12/04 16:53:33 DEBUG DFSClient: DeadNode detection is not enabled or given block LocatedBlocks{;  fileLength=971;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741995_1171; getBlockSize()=971; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741995_1171; getBlockSize()=971; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null} is null, skip to remove node.\n",
      "24/12/04 16:53:33 DEBUG DecryptionPropertiesFactory: DecryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #110 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #110\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #111 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #111\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getBlockLocations took 0ms\n",
      "24/12/04 16:53:33 DEBUG DFSClient: newInfo = LocatedBlocks{;  fileLength=971;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741995_1171; getBlockSize()=971; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741995_1171; getBlockSize()=971; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null}\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: File length 971\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: reading footer index at 963\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: read footer length: 724, footer index: 239\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: Finished to read all footer bytes.\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: FileMetaData(version:1, schema:[SchemaElement(name:spark_schema, num_children:3), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:id, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:name, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:[RowGroup(columns:[ColumnChunk(file_offset:4, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[id], codec:SNAPPY, num_values:1, total_uncompressed_size:34, total_compressed_size:36, data_page_offset:4, statistics:Statistics(max:36, min:36, null_count:0, max_value:36, min_value:36), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:208, offset_index_length:10, column_index_offset:127, column_index_length:17), ColumnChunk(file_offset:40, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[city], codec:SNAPPY, num_values:1, total_uncompressed_size:40, total_compressed_size:42, data_page_offset:40, statistics:Statistics(max:4D 69 7A 6F 72 61 6D, min:4D 69 7A 6F 72 61 6D, null_count:0, max_value:4D 69 7A 6F 72 61 6D, min_value:4D 69 7A 6F 72 61 6D), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:218, offset_index_length:10, column_index_offset:144, column_index_length:29), ColumnChunk(file_offset:82, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[name], codec:SNAPPY, num_values:1, total_uncompressed_size:43, total_compressed_size:45, data_page_offset:82, statistics:Statistics(max:4C 61 6C 70 65 6B 68 6C 75 61, min:4C 61 6C 70 65 6B 68 6C 75 61, null_count:0, max_value:4C 61 6C 70 65 6B 68 6C 75 61, min_value:4C 61 6C 70 65 6B 68 6C 75 61), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:228, offset_index_length:11, column_index_offset:173, column_index_length:35)], total_byte_size:117, num_rows:1, file_offset:4, total_compressed_size:123, ordinal:0)], key_value_metadata:[KeyValue(key:org.apache.spark.version, value:3.3.1), KeyValue(key:org.apache.spark.sql.parquet.row.metadata, value:{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]})], created_by:parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94), column_orders:[<ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>])\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"spark_schema\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"id\" ], [ \"city\" ], [ \"name\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      } ],\n",
      "      \"fieldCount\" : 3,\n",
      "      \"primitive\" : false,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"org.apache.spark.version\" : \"3.3.1\",\n",
      "      \"org.apache.spark.sql.parquet.row.metadata\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\",\n",
      "    \"fileDecryptor\" : null\n",
      "  },\n",
      "  \"blocks\" : [ {\n",
      "    \"columns\" : [ {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 127,\n",
      "        \"length\" : 17\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 208,\n",
      "        \"length\" : 10\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 36,\n",
      "      \"totalUncompressedSize\" : 34,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"Ng==\",\n",
      "          \"bytes\" : \"Ng==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"Ng==\",\n",
      "          \"bytes\" : \"Ng==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"Ng==\",\n",
      "        \"maxBytes\" : \"Ng==\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 4,\n",
      "      \"startingPos\" : 4,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"id\" ]\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 144,\n",
      "        \"length\" : 29\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 218,\n",
      "        \"length\" : 10\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 42,\n",
      "      \"totalUncompressedSize\" : 40,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"TWl6b3JhbQ==\",\n",
      "          \"bytes\" : \"TWl6b3JhbQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"TWl6b3JhbQ==\",\n",
      "          \"bytes\" : \"TWl6b3JhbQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"TWl6b3JhbQ==\",\n",
      "        \"maxBytes\" : \"TWl6b3JhbQ==\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 40,\n",
      "      \"startingPos\" : 40,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"city\" ]\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 173,\n",
      "        \"length\" : 35\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 228,\n",
      "        \"length\" : 11\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 45,\n",
      "      \"totalUncompressedSize\" : 43,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"TGFscGVraGx1YQ==\",\n",
      "          \"bytes\" : \"TGFscGVraGx1YQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"TGFscGVraGx1YQ==\",\n",
      "          \"bytes\" : \"TGFscGVraGx1YQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"TGFscGVraGx1YQ==\",\n",
      "        \"maxBytes\" : \"TGFscGVraGx1YQ==\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 82,\n",
      "      \"startingPos\" : 82,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"name\" ]\n",
      "    } ],\n",
      "    \"rowCount\" : 1,\n",
      "    \"totalByteSize\" : 117,\n",
      "    \"path\" : null,\n",
      "    \"ordinal\" : 0,\n",
      "    \"startingPos\" : 4,\n",
      "    \"compressedSize\" : 123\n",
      "  } ]\n",
      "}\n",
      "24/12/04 16:53:33 DEBUG ParquetReadSupport: Going to read the following fields from the Parquet file with the following schema:\n",
      "Parquet file schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Parquet clipped schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Parquet requested schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Catalyst requested schema:\n",
      "root\n",
      "-- id: string (nullable = true)\n",
      "-- city: string (nullable = true)\n",
      "-- name: string (nullable = true)\n",
      "\n",
      "       \n",
      "24/12/04 16:53:33 DEBUG ParquetFileFormat: Appending StructType() [empty row]\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 11 bytes\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 17 bytes\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 20 bytes\n",
      "24/12/04 16:53:33 DEBUG DFSClient: DeadNode detection is not enabled or given block LocatedBlocks{;  fileLength=971;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741995_1171; getBlockSize()=971; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741995_1171; getBlockSize()=971; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null} is null, skip to remove node.\n",
      "24/12/04 16:53:33 INFO Executor: Finished task 0.0 in stage 7.0 (TID 81). 1715 bytes result sent to driver\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (7, 0) -> 0\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 81) in 69 ms on hadoop113 (executor driver) (1/1)\n",
      "24/12/04 16:53:33 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:33 INFO DAGScheduler: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 0.080 s\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: After removal of stage 7, remaining stages = 0\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/04 16:53:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: SparkListenerJobEnd - executionId: 4\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Job 7 finished: showString at NativeMethodAccessorImpl.java:0, took 0.084942 s\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.GlobalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.LocalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.datasources.LogicalRelation\",\"num-children\":0,\"relation\":null,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":48,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":49,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":50,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"isStreaming\":false}]\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.CollectLimitExec\",\"num-children\":1,\"limit\":21,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.ColumnarToRowExec\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.InputAdapter\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.FileSourceScanExec\",\"num-children\":0,\"relation\":null,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":48,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":49,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":50,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"requiredSchema\":{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"partitionFilters\":[],\"dataFilters\":[],\"disableBucketedScan\":false}]\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#48,city#49,name#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://hadoop:9000/tmp/test], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,city:string,name:string>\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "24/12/04 16:53:33 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 27 took 0.000043 seconds\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #112 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #112\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Got job 8 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Final stage: ResultStage 8 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: submitStage(ResultStage 8 (name=showString at NativeMethodAccessorImpl.java:0;jobs=8))\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: missing: List()\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: submitMissingTasks(ResultStage 8)\n",
      "24/12/04 16:53:33 DEBUG HdfsPathDataset: path: hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 INFO HdfsPathDataset: No path_spec_list configuration found. Falling back to creating dataset name with complete uri\n",
      "24/12/04 16:53:33 DEBUG PlanUtils: Path hdfs://hadoop:9000/tmp/test transformed to hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 13.7 KiB, free 363.9 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_14 locally took 0 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_14 without replication took 0 ms\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 363.9 MiB)\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#48,city#49,name#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://hadoop:9000/tmp/test], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,city:string,name:string>\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_14_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:33 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on hadoop113:38317 (size: 6.0 KiB, free: 366.0 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMaster: Updated info of block broadcast_14_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Told master about block broadcast_14_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_14_piece0 locally took 0 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_14_piece0 without replication took 0 ms\n",
      "24/12/04 16:53:33 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 8 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/12/04 16:53:33 INFO TaskSchedulerImpl: Adding task set 8.0 with 4 tasks resource profile 0\n",
      "24/12/04 16:53:33 DEBUG TaskSetManager: Epoch for TaskSet 8.0: 0\n",
      "24/12/04 16:53:33 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "24/12/04 16:53:33 DEBUG TaskSetManager: Valid locality levels for TaskSet 8.0: ANY\n",
      "24/12/04 16:53:33 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_8.0, runningTasks: 0\n",
      "24/12/04 16:53:33 INFO RemovePathPatternUtils: Removing path pattern from dataset name /tmp/test\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 82) (hadoop113, executor driver, partition 1, ANY, 4966 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:33 DEBUG HdfsPathDataset: path: /tmp/test\n",
      "24/12/04 16:53:33 DEBUG RemovePathPatternUtils: Transformed path is /tmp/test\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: Posting event for end 4: io.openlineage.client.OpenLineage$RunEvent@6d19125c\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 83) (hadoop113, executor driver, partition 2, ANY, 4966 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 84) (hadoop113, executor driver, partition 3, ANY, 4966 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Starting task 3.0 in stage 8.0 (TID 85) (hadoop113, executor driver, partition 4, ANY, 4966 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:33 INFO Executor: Running task 0.0 in stage 8.0 (TID 82)\n",
      "24/12/04 16:53:33 INFO Executor: Running task 1.0 in stage 8.0 (TID 83)\n",
      "24/12/04 16:53:33 INFO Executor: Running task 3.0 in stage 8.0 (TID 85)\n",
      "24/12/04 16:53:33 INFO Executor: Running task 2.0 in stage 8.0 (TID 84)\n",
      "24/12/04 16:53:33 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.323Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.323Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (8, 0) -> 1\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (8, 0) -> 2\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@13019491, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (8, 0) -> 3\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (8, 0) -> 4\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@13019491, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Getting local block broadcast_14\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Level for block broadcast_14 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Dataset URN: Optional[urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD)], alias_list: {}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Building input with /tmp/test\n",
      "24/12/04 16:53:33 INFO ZooKeeper: Session: 0x100035c67eb0105 closed\n",
      "24/12/04 16:53:33 INFO ClientCnxn: EventThread shut down for session: 0x100035c67eb0105\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306013323}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306013331}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1733306013323, status=STARTED}, dataProcessInstanceProperties={name=01939116-d5f7-7f29-82a1-b6534ebd5626, created={actor=urn:li:corpuser:datahub, time=1733306013323}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-d5f7-7f29-82a1-b6534ebd5626, inSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD), schemaMetadata={platformSchema={com.linkedin.schema.MySqlDDL={tableSchema=[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}}, schemaName=, fields=[{fieldPath=id, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=city, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=name, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}], version=1, hash=, platform=urn:li:dataPlatform:hdfs}, lineage=null)], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306013323, eventFormatter=datahub.event.EventFormatter@66e50703), from {\"eventTime\":\"2024-12-04T09:53:33.323Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onJobEnd completed successfully in 9 ms\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: SparkListenerJobStart - executionId: 4\n",
      "24/12/04 16:53:33 INFO FileScanRDD: Reading File path: hdfs://hadoop:9000/tmp/test/part-00007-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, range: 0-936, partition values: [empty row]\n",
      "24/12/04 16:53:33 INFO FileScanRDD: Reading File path: hdfs://hadoop:9000/tmp/test/part-00003-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, range: 0-964, partition values: [empty row]\n",
      "24/12/04 16:53:33 INFO FileScanRDD: Reading File path: hdfs://hadoop:9000/tmp/test/part-00015-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, range: 0-943, partition values: [empty row]\n",
      "24/12/04 16:53:33 INFO FileScanRDD: Reading File path: hdfs://hadoop:9000/tmp/test/part-00011-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, range: 0-963, partition values: [empty row]\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #113 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #116 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #115 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #114 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.GlobalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.LocalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.datasources.LogicalRelation\",\"num-children\":0,\"relation\":null,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":48,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":49,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":50,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"isStreaming\":false}]\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #113\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #116\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #115\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #114\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:33 DEBUG DecryptionPropertiesFactory: DecryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:33 DEBUG DecryptionPropertiesFactory: DecryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:33 DEBUG DecryptionPropertiesFactory: DecryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:33 DEBUG DecryptionPropertiesFactory: DecryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #117 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #120 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #119 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #118 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #117\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getBlockLocations took 1ms\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #120\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.CollectLimitExec\",\"num-children\":1,\"limit\":21,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.ColumnarToRowExec\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.InputAdapter\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.FileSourceScanExec\",\"num-children\":0,\"relation\":null,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":48,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":49,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":50,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"requiredSchema\":{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"partitionFilters\":[],\"dataFilters\":[],\"disableBucketedScan\":false}]\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getBlockLocations took 1ms\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #118\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #119\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getBlockLocations took 1ms\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getBlockLocations took 1ms\n",
      "24/12/04 16:53:33 DEBUG DFSClient: newInfo = LocatedBlocks{;  fileLength=964;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741992_1168; getBlockSize()=964; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741992_1168; getBlockSize()=964; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null}\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: File length 964\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: reading footer index at 956\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG DFSClient: newInfo = LocatedBlocks{;  fileLength=943;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741996_1172; getBlockSize()=943; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741996_1172; getBlockSize()=943; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null}\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: File length 943\n",
      "24/12/04 16:53:33 DEBUG DFSClient: newInfo = LocatedBlocks{;  fileLength=963;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741993_1169; getBlockSize()=963; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741993_1169; getBlockSize()=963; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null}\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: reading footer index at 935\n",
      "24/12/04 16:53:33 DEBUG DFSClient: newInfo = LocatedBlocks{;  fileLength=936;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741991_1167; getBlockSize()=936; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741991_1167; getBlockSize()=936; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null}\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: File length 963\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: File length 936\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: reading footer index at 955\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: reading footer index at 928\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "24/12/04 16:53:33 DEBUG SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "24/12/04 16:53:33 DEBUG SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\n",
      "24/12/04 16:53:33 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.25.1.12, datanodeId = DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]\n",
      "24/12/04 16:53:33 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.25.1.12, datanodeId = DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]\n",
      "24/12/04 16:53:33 DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /172.25.1.12, datanodeId = DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#48,city#49,name#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://hadoop:9000/tmp/test], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,city:string,name:string>\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: read footer length: 720, footer index: 236\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: read footer length: 720, footer index: 235\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: read footer length: 704, footer index: 224\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: Finished to read all footer bytes.\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #121 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: read footer length: 708, footer index: 227\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: FileMetaData(version:1, schema:[SchemaElement(name:spark_schema, num_children:3), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:id, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:name, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:null, key_value_metadata:[KeyValue(key:org.apache.spark.version, value:3.3.1), KeyValue(key:org.apache.spark.sql.parquet.row.metadata, value:{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]})], created_by:parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94))\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #121\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: Finished to read all footer bytes.\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: Finished to read all footer bytes.\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"spark_schema\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"id\" ], [ \"city\" ], [ \"name\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      } ],\n",
      "      \"fieldCount\" : 3,\n",
      "      \"primitive\" : false,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"org.apache.spark.version\" : \"3.3.1\",\n",
      "      \"org.apache.spark.sql.parquet.row.metadata\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\",\n",
      "    \"fileDecryptor\" : null\n",
      "  },\n",
      "  \"blocks\" : [ ]\n",
      "}\n",
      "24/12/04 16:53:33 DEBUG DFSClient: DeadNode detection is not enabled or given block LocatedBlocks{;  fileLength=964;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741992_1168; getBlockSize()=964; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741992_1168; getBlockSize()=964; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null} is null, skip to remove node.\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: Finished to read all footer bytes.\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: FileMetaData(version:1, schema:[SchemaElement(name:spark_schema, num_children:3), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:id, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:name, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:null, key_value_metadata:[KeyValue(key:org.apache.spark.version, value:3.3.1), KeyValue(key:org.apache.spark.sql.parquet.row.metadata, value:{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]})], created_by:parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94))\n",
      "24/12/04 16:53:33 DEBUG HdfsPathDataset: path: hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 INFO HdfsPathDataset: No path_spec_list configuration found. Falling back to creating dataset name with complete uri\n",
      "24/12/04 16:53:33 DEBUG PlanUtils: Path hdfs://hadoop:9000/tmp/test transformed to hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: FileMetaData(version:1, schema:[SchemaElement(name:spark_schema, num_children:3), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:id, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:name, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:null, key_value_metadata:[KeyValue(key:org.apache.spark.version, value:3.3.1), KeyValue(key:org.apache.spark.sql.parquet.row.metadata, value:{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]})], created_by:parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94))\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: FileMetaData(version:1, schema:[SchemaElement(name:spark_schema, num_children:3), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:id, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:name, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:null, key_value_metadata:[KeyValue(key:org.apache.spark.version, value:3.3.1), KeyValue(key:org.apache.spark.sql.parquet.row.metadata, value:{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]})], created_by:parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94))\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"spark_schema\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"id\" ], [ \"city\" ], [ \"name\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      } ],\n",
      "      \"fieldCount\" : 3,\n",
      "      \"primitive\" : false,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"org.apache.spark.version\" : \"3.3.1\",\n",
      "      \"org.apache.spark.sql.parquet.row.metadata\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\",\n",
      "    \"fileDecryptor\" : null\n",
      "  },\n",
      "  \"blocks\" : [ ]\n",
      "}\n",
      "24/12/04 16:53:33 DEBUG DFSClient: DeadNode detection is not enabled or given block LocatedBlocks{;  fileLength=963;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741993_1169; getBlockSize()=963; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741993_1169; getBlockSize()=963; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null} is null, skip to remove node.\n",
      "24/12/04 16:53:33 DEBUG DecryptionPropertiesFactory: DecryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:33 DEBUG DecryptionPropertiesFactory: DecryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #122 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #123 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"spark_schema\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"id\" ], [ \"city\" ], [ \"name\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      } ],\n",
      "      \"fieldCount\" : 3,\n",
      "      \"primitive\" : false,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"org.apache.spark.version\" : \"3.3.1\",\n",
      "      \"org.apache.spark.sql.parquet.row.metadata\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\",\n",
      "    \"fileDecryptor\" : null\n",
      "  },\n",
      "  \"blocks\" : [ ]\n",
      "}\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"spark_schema\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"id\" ], [ \"city\" ], [ \"name\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      } ],\n",
      "      \"fieldCount\" : 3,\n",
      "      \"primitive\" : false,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"org.apache.spark.version\" : \"3.3.1\",\n",
      "      \"org.apache.spark.sql.parquet.row.metadata\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\",\n",
      "    \"fileDecryptor\" : null\n",
      "  },\n",
      "  \"blocks\" : [ ]\n",
      "}\n",
      "24/12/04 16:53:33 DEBUG DFSClient: DeadNode detection is not enabled or given block LocatedBlocks{;  fileLength=943;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741996_1172; getBlockSize()=943; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741996_1172; getBlockSize()=943; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null} is null, skip to remove node.\n",
      "24/12/04 16:53:33 DEBUG DFSClient: DeadNode detection is not enabled or given block LocatedBlocks{;  fileLength=936;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741991_1167; getBlockSize()=936; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741991_1167; getBlockSize()=936; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null} is null, skip to remove node.\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #122\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #123\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #124 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #125 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n",
      "24/12/04 16:53:33 DEBUG DecryptionPropertiesFactory: DecryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #126 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #125\n",
      "24/12/04 16:53:33 DEBUG DecryptionPropertiesFactory: DecryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #126\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getBlockLocations took 0ms\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #124\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getBlockLocations took 0ms\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#48,city#49,name#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://hadoop:9000/tmp/test], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,city:string,name:string>\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 DEBUG DFSClient: newInfo = LocatedBlocks{;  fileLength=963;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741993_1169; getBlockSize()=963; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741993_1169; getBlockSize()=963; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null}\n",
      "24/12/04 16:53:33 DEBUG DFSClient: newInfo = LocatedBlocks{;  fileLength=964;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741992_1168; getBlockSize()=964; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741992_1168; getBlockSize()=964; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null}\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: File length 963\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #127 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: reading footer index at 955\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: File length 964\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: reading footer index at 956\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #128 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #127\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #128\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getBlockLocations took 0ms\n",
      "24/12/04 16:53:33 DEBUG DFSClient: newInfo = LocatedBlocks{;  fileLength=943;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741996_1172; getBlockSize()=943; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741996_1172; getBlockSize()=943; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null}\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #129 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: File length 943\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: reading footer index at 935\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: read footer length: 720, footer index: 235\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: read footer length: 720, footer index: 236\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #129\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getBlockLocations took 1ms\n",
      "24/12/04 16:53:33 DEBUG DFSClient: newInfo = LocatedBlocks{;  fileLength=936;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741991_1167; getBlockSize()=936; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741991_1167; getBlockSize()=936; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null}\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: File length 936\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: reading footer index at 928\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: read footer length: 708, footer index: 227\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 INFO RemovePathPatternUtils: Removing path pattern from dataset name /tmp/test\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: Finished to read all footer bytes.\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: Finished to read all footer bytes.\n",
      "24/12/04 16:53:33 DEBUG HdfsPathDataset: path: /tmp/test\n",
      "24/12/04 16:53:33 DEBUG RemovePathPatternUtils: Transformed path is /tmp/test\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: Posting event for start 4: io.openlineage.client.OpenLineage$RunEvent@5cf1d385\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: Finished to read all footer bytes.\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: read footer length: 704, footer index: 224\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: FileMetaData(version:1, schema:[SchemaElement(name:spark_schema, num_children:3), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:id, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:name, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:[RowGroup(columns:[ColumnChunk(file_offset:4, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[id], codec:SNAPPY, num_values:1, total_uncompressed_size:34, total_compressed_size:36, data_page_offset:4, statistics:Statistics(max:34, min:34, null_count:0, max_value:34, min_value:34), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:204, offset_index_length:10, column_index_offset:125, column_index_length:17), ColumnChunk(file_offset:40, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[city], codec:SNAPPY, num_values:1, total_uncompressed_size:42, total_compressed_size:44, data_page_offset:40, statistics:Statistics(max:48 79 64 65 72 61 62 61 64, min:48 79 64 65 72 61 62 61 64, null_count:0, max_value:48 79 64 65 72 61 62 61 64, min_value:48 79 64 65 72 61 62 61 64), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:214, offset_index_length:10, column_index_offset:142, column_index_length:33), ColumnChunk(file_offset:84, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[name], codec:SNAPPY, num_values:1, total_uncompressed_size:39, total_compressed_size:41, data_page_offset:84, statistics:Statistics(max:53 68 61 62 62 69 72, min:53 68 61 62 62 69 72, null_count:0, max_value:53 68 61 62 62 69 72, min_value:53 68 61 62 62 69 72), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:224, offset_index_length:11, column_index_offset:175, column_index_length:29)], total_byte_size:115, num_rows:1, file_offset:4, total_compressed_size:121, ordinal:0)], key_value_metadata:[KeyValue(key:org.apache.spark.version, value:3.3.1), KeyValue(key:org.apache.spark.sql.parquet.row.metadata, value:{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]})], created_by:parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94), column_orders:[<ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>])\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: FileMetaData(version:1, schema:[SchemaElement(name:spark_schema, num_children:3), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:id, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:name, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:[RowGroup(columns:[ColumnChunk(file_offset:4, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[id], codec:SNAPPY, num_values:1, total_uncompressed_size:34, total_compressed_size:36, data_page_offset:4, statistics:Statistics(max:32, min:32, null_count:0, max_value:32, min_value:32), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:205, offset_index_length:10, column_index_offset:126, column_index_length:17), ColumnChunk(file_offset:40, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[city], codec:SNAPPY, num_values:1, total_uncompressed_size:42, total_compressed_size:44, data_page_offset:40, statistics:Statistics(max:54 65 6C 61 6E 67 61 6E 61, min:54 65 6C 61 6E 67 61 6E 61, null_count:0, max_value:54 65 6C 61 6E 67 61 6E 61, min_value:54 65 6C 61 6E 67 61 6E 61), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:215, offset_index_length:10, column_index_offset:143, column_index_length:33), ColumnChunk(file_offset:84, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[name], codec:SNAPPY, num_values:1, total_uncompressed_size:40, total_compressed_size:42, data_page_offset:84, statistics:Statistics(max:43 68 68 65 74 72 69, min:43 68 68 65 74 72 69, null_count:0, max_value:43 68 68 65 74 72 69, min_value:43 68 68 65 74 72 69), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:225, offset_index_length:11, column_index_offset:176, column_index_length:29)], total_byte_size:116, num_rows:1, file_offset:4, total_compressed_size:122, ordinal:0)], key_value_metadata:[KeyValue(key:org.apache.spark.version, value:3.3.1), KeyValue(key:org.apache.spark.sql.parquet.row.metadata, value:{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]})], created_by:parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94), column_orders:[<ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>])\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: FileMetaData(version:1, schema:[SchemaElement(name:spark_schema, num_children:3), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:id, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:name, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:[RowGroup(columns:[ColumnChunk(file_offset:4, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[id], codec:SNAPPY, num_values:1, total_uncompressed_size:34, total_compressed_size:36, data_page_offset:4, statistics:Statistics(max:35, min:35, null_count:0, max_value:35, min_value:35), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:196, offset_index_length:10, column_index_offset:123, column_index_length:17), ColumnChunk(file_offset:40, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[city], codec:SNAPPY, num_values:1, total_uncompressed_size:39, total_compressed_size:41, data_page_offset:40, statistics:Statistics(max:4B 65 72 61 6C 61, min:4B 65 72 61 6C 61, null_count:0, max_value:4B 65 72 61 6C 61, min_value:4B 65 72 61 6C 61), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:206, offset_index_length:10, column_index_offset:140, column_index_length:27), ColumnChunk(file_offset:81, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[name], codec:SNAPPY, num_values:1, total_uncompressed_size:40, total_compressed_size:42, data_page_offset:81, statistics:Statistics(max:56 69 6A 61 79 61 6E, min:56 69 6A 61 79 61 6E, null_count:0, max_value:56 69 6A 61 79 61 6E, min_value:56 69 6A 61 79 61 6E), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:216, offset_index_length:11, column_index_offset:167, column_index_length:29)], total_byte_size:113, num_rows:1, file_offset:4, total_compressed_size:119, ordinal:0)], key_value_metadata:[KeyValue(key:org.apache.spark.version, value:3.3.1), KeyValue(key:org.apache.spark.sql.parquet.row.metadata, value:{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]})], created_by:parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94), column_orders:[<ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>])\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: Finished to read all footer bytes.\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: FileMetaData(version:1, schema:[SchemaElement(name:spark_schema, num_children:3), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:id, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:name, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>)], num_rows:1, row_groups:[RowGroup(columns:[ColumnChunk(file_offset:4, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[id], codec:SNAPPY, num_values:1, total_uncompressed_size:34, total_compressed_size:36, data_page_offset:4, statistics:Statistics(max:33, min:33, null_count:0, max_value:33, min_value:33), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:193, offset_index_length:10, column_index_offset:122, column_index_length:17), ColumnChunk(file_offset:40, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[city], codec:SNAPPY, num_values:1, total_uncompressed_size:39, total_compressed_size:41, data_page_offset:40, statistics:Statistics(max:53 69 6B 6B 69 6D, min:53 69 6B 6B 69 6D, null_count:0, max_value:53 69 6B 6B 69 6D, min_value:53 69 6B 6B 69 6D), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:203, offset_index_length:10, column_index_offset:139, column_index_length:27), ColumnChunk(file_offset:81, meta_data:ColumnMetaData(type:BYTE_ARRAY, encodings:[PLAIN, BIT_PACKED, RLE], path_in_schema:[name], codec:SNAPPY, num_values:1, total_uncompressed_size:39, total_compressed_size:41, data_page_offset:81, statistics:Statistics(max:42 68 75 74 69 61, min:42 68 75 74 69 61, null_count:0, max_value:42 68 75 74 69 61, min_value:42 68 75 74 69 61), encoding_stats:[PageEncodingStats(page_type:DATA_PAGE, encoding:PLAIN, count:1)]), offset_index_offset:213, offset_index_length:11, column_index_offset:166, column_index_length:27)], total_byte_size:112, num_rows:1, file_offset:4, total_compressed_size:118, ordinal:0)], key_value_metadata:[KeyValue(key:org.apache.spark.version, value:3.3.1), KeyValue(key:org.apache.spark.sql.parquet.row.metadata, value:{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]})], created_by:parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94), column_orders:[<ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>])\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"spark_schema\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"id\" ], [ \"city\" ], [ \"name\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      } ],\n",
      "      \"fieldCount\" : 3,\n",
      "      \"primitive\" : false,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"org.apache.spark.version\" : \"3.3.1\",\n",
      "      \"org.apache.spark.sql.parquet.row.metadata\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\",\n",
      "    \"fileDecryptor\" : null\n",
      "  },\n",
      "  \"blocks\" : [ {\n",
      "    \"columns\" : [ {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 123,\n",
      "        \"length\" : 17\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 196,\n",
      "        \"length\" : 10\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 36,\n",
      "      \"totalUncompressedSize\" : 34,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"NQ==\",\n",
      "          \"bytes\" : \"NQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"NQ==\",\n",
      "          \"bytes\" : \"NQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"NQ==\",\n",
      "        \"maxBytes\" : \"NQ==\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 4,\n",
      "      \"startingPos\" : 4,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"id\" ]\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 140,\n",
      "        \"length\" : 27\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 206,\n",
      "        \"length\" : 10\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 41,\n",
      "      \"totalUncompressedSize\" : 39,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"S2VyYWxh\",\n",
      "          \"bytes\" : \"S2VyYWxh\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"S2VyYWxh\",\n",
      "          \"bytes\" : \"S2VyYWxh\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"S2VyYWxh\",\n",
      "        \"maxBytes\" : \"S2VyYWxh\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 40,\n",
      "      \"startingPos\" : 40,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"city\" ]\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 167,\n",
      "        \"length\" : 29\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 216,\n",
      "        \"length\" : 11\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 42,\n",
      "      \"totalUncompressedSize\" : 40,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"VmlqYXlhbg==\",\n",
      "          \"bytes\" : \"VmlqYXlhbg==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"VmlqYXlhbg==\",\n",
      "          \"bytes\" : \"VmlqYXlhbg==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"VmlqYXlhbg==\",\n",
      "        \"maxBytes\" : \"VmlqYXlhbg==\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 81,\n",
      "      \"startingPos\" : 81,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"name\" ]\n",
      "    } ],\n",
      "    \"rowCount\" : 1,\n",
      "    \"totalByteSize\" : 113,\n",
      "    \"path\" : null,\n",
      "    \"ordinal\" : 0,\n",
      "    \"startingPos\" : 4,\n",
      "    \"compressedSize\" : 119\n",
      "  } ]\n",
      "}\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"spark_schema\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"id\" ], [ \"city\" ], [ \"name\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      } ],\n",
      "      \"fieldCount\" : 3,\n",
      "      \"primitive\" : false,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"org.apache.spark.version\" : \"3.3.1\",\n",
      "      \"org.apache.spark.sql.parquet.row.metadata\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\",\n",
      "    \"fileDecryptor\" : null\n",
      "  },\n",
      "  \"blocks\" : [ {\n",
      "    \"columns\" : [ {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 125,\n",
      "        \"length\" : 17\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 204,\n",
      "        \"length\" : 10\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 36,\n",
      "      \"totalUncompressedSize\" : 34,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"NA==\",\n",
      "          \"bytes\" : \"NA==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"NA==\",\n",
      "          \"bytes\" : \"NA==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"NA==\",\n",
      "        \"maxBytes\" : \"NA==\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 4,\n",
      "      \"startingPos\" : 4,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"id\" ]\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 142,\n",
      "        \"length\" : 33\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 214,\n",
      "        \"length\" : 10\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 44,\n",
      "      \"totalUncompressedSize\" : 42,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"SHlkZXJhYmFk\",\n",
      "          \"bytes\" : \"SHlkZXJhYmFk\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"SHlkZXJhYmFk\",\n",
      "          \"bytes\" : \"SHlkZXJhYmFk\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"SHlkZXJhYmFk\",\n",
      "        \"maxBytes\" : \"SHlkZXJhYmFk\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 40,\n",
      "      \"startingPos\" : 40,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"city\" ]\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 175,\n",
      "        \"length\" : 29\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 224,\n",
      "        \"length\" : 11\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 41,\n",
      "      \"totalUncompressedSize\" : 39,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"U2hhYmJpcg==\",\n",
      "          \"bytes\" : \"U2hhYmJpcg==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"U2hhYmJpcg==\",\n",
      "          \"bytes\" : \"U2hhYmJpcg==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"U2hhYmJpcg==\",\n",
      "        \"maxBytes\" : \"U2hhYmJpcg==\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 84,\n",
      "      \"startingPos\" : 84,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"name\" ]\n",
      "    } ],\n",
      "    \"rowCount\" : 1,\n",
      "    \"totalByteSize\" : 115,\n",
      "    \"path\" : null,\n",
      "    \"ordinal\" : 0,\n",
      "    \"startingPos\" : 4,\n",
      "    \"compressedSize\" : 121\n",
      "  } ]\n",
      "}\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"spark_schema\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"id\" ], [ \"city\" ], [ \"name\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      } ],\n",
      "      \"fieldCount\" : 3,\n",
      "      \"primitive\" : false,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"org.apache.spark.version\" : \"3.3.1\",\n",
      "      \"org.apache.spark.sql.parquet.row.metadata\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\",\n",
      "    \"fileDecryptor\" : null\n",
      "  },\n",
      "  \"blocks\" : [ {\n",
      "    \"columns\" : [ {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 126,\n",
      "        \"length\" : 17\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 205,\n",
      "        \"length\" : 10\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 36,\n",
      "      \"totalUncompressedSize\" : 34,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"Mg==\",\n",
      "          \"bytes\" : \"Mg==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"Mg==\",\n",
      "          \"bytes\" : \"Mg==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"Mg==\",\n",
      "        \"maxBytes\" : \"Mg==\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 4,\n",
      "      \"startingPos\" : 4,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"id\" ]\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 143,\n",
      "        \"length\" : 33\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 215,\n",
      "        \"length\" : 10\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 44,\n",
      "      \"totalUncompressedSize\" : 42,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"VGVsYW5nYW5h\",\n",
      "          \"bytes\" : \"VGVsYW5nYW5h\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"VGVsYW5nYW5h\",\n",
      "          \"bytes\" : \"VGVsYW5nYW5h\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"VGVsYW5nYW5h\",\n",
      "        \"maxBytes\" : \"VGVsYW5nYW5h\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 40,\n",
      "      \"startingPos\" : 40,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"city\" ]\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 176,\n",
      "        \"length\" : 29\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 225,\n",
      "        \"length\" : 11\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 42,\n",
      "      \"totalUncompressedSize\" : 40,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"Q2hoZXRyaQ==\",\n",
      "          \"bytes\" : \"Q2hoZXRyaQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"Q2hoZXRyaQ==\",\n",
      "          \"bytes\" : \"Q2hoZXRyaQ==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"Q2hoZXRyaQ==\",\n",
      "        \"maxBytes\" : \"Q2hoZXRyaQ==\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 84,\n",
      "      \"startingPos\" : 84,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"name\" ]\n",
      "    } ],\n",
      "    \"rowCount\" : 1,\n",
      "    \"totalByteSize\" : 116,\n",
      "    \"path\" : null,\n",
      "    \"ordinal\" : 0,\n",
      "    \"startingPos\" : 4,\n",
      "    \"compressedSize\" : 122\n",
      "  } ]\n",
      "}\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"spark_schema\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"id\" ], [ \"city\" ], [ \"name\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      } ],\n",
      "      \"fieldCount\" : 3,\n",
      "      \"primitive\" : false,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"org.apache.spark.version\" : \"3.3.1\",\n",
      "      \"org.apache.spark.sql.parquet.row.metadata\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\",\n",
      "    \"fileDecryptor\" : null\n",
      "  },\n",
      "  \"blocks\" : [ {\n",
      "    \"columns\" : [ {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 122,\n",
      "        \"length\" : 17\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 193,\n",
      "        \"length\" : 10\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 36,\n",
      "      \"totalUncompressedSize\" : 34,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"Mw==\",\n",
      "          \"bytes\" : \"Mw==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"Mw==\",\n",
      "          \"bytes\" : \"Mw==\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"Mw==\",\n",
      "        \"maxBytes\" : \"Mw==\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 4,\n",
      "      \"startingPos\" : 4,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"id\" ]\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 139,\n",
      "        \"length\" : 27\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 203,\n",
      "        \"length\" : 10\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 41,\n",
      "      \"totalUncompressedSize\" : 39,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"U2lra2lt\",\n",
      "          \"bytes\" : \"U2lra2lt\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"U2lra2lt\",\n",
      "          \"bytes\" : \"U2lra2lt\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"U2lra2lt\",\n",
      "        \"maxBytes\" : \"U2lra2lt\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 40,\n",
      "      \"startingPos\" : 40,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"city\" ]\n",
      "    }, {\n",
      "      \"rowGroupOrdinal\" : 0,\n",
      "      \"encodingStats\" : {\n",
      "        \"dictionaryEncodings\" : [ ],\n",
      "        \"dataEncodings\" : [ \"PLAIN\" ]\n",
      "      },\n",
      "      \"columnIndexReference\" : {\n",
      "        \"offset\" : 166,\n",
      "        \"length\" : 27\n",
      "      },\n",
      "      \"offsetIndexReference\" : {\n",
      "        \"offset\" : 213,\n",
      "        \"length\" : 11\n",
      "      },\n",
      "      \"bloomFilterOffset\" : -1,\n",
      "      \"dictionaryPageOffset\" : 0,\n",
      "      \"valueCount\" : 1,\n",
      "      \"totalSize\" : 41,\n",
      "      \"totalUncompressedSize\" : 39,\n",
      "      \"statistics\" : {\n",
      "        \"max\" : {\n",
      "          \"bytesUnsafe\" : \"Qmh1dGlh\",\n",
      "          \"bytes\" : \"Qmh1dGlh\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"min\" : {\n",
      "          \"bytesUnsafe\" : \"Qmh1dGlh\",\n",
      "          \"bytes\" : \"Qmh1dGlh\",\n",
      "          \"backingBytesReused\" : true\n",
      "        },\n",
      "        \"minBytes\" : \"Qmh1dGlh\",\n",
      "        \"maxBytes\" : \"Qmh1dGlh\",\n",
      "        \"numNulls\" : 0,\n",
      "        \"numNullsSet\" : true,\n",
      "        \"empty\" : false\n",
      "      },\n",
      "      \"firstDataPageOffset\" : 81,\n",
      "      \"startingPos\" : 81,\n",
      "      \"primitiveType\" : {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      },\n",
      "      \"encodings\" : [ \"PLAIN\", \"BIT_PACKED\", \"RLE\" ],\n",
      "      \"codec\" : \"SNAPPY\",\n",
      "      \"type\" : \"BINARY\",\n",
      "      \"path\" : [ \"name\" ]\n",
      "    } ],\n",
      "    \"rowCount\" : 1,\n",
      "    \"totalByteSize\" : 112,\n",
      "    \"path\" : null,\n",
      "    \"ordinal\" : 0,\n",
      "    \"startingPos\" : 4,\n",
      "    \"compressedSize\" : 118\n",
      "  } ]\n",
      "}\n",
      "24/12/04 16:53:33 DEBUG ParquetReadSupport: Going to read the following fields from the Parquet file with the following schema:\n",
      "Parquet file schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Parquet clipped schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Parquet requested schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Catalyst requested schema:\n",
      "root\n",
      "-- id: string (nullable = true)\n",
      "-- city: string (nullable = true)\n",
      "-- name: string (nullable = true)\n",
      "\n",
      "       \n",
      "24/12/04 16:53:33 DEBUG ParquetReadSupport: Going to read the following fields from the Parquet file with the following schema:\n",
      "Parquet file schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Parquet clipped schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Parquet requested schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Catalyst requested schema:\n",
      "root\n",
      "-- id: string (nullable = true)\n",
      "-- city: string (nullable = true)\n",
      "-- name: string (nullable = true)\n",
      "\n",
      "       \n",
      "24/12/04 16:53:33 DEBUG ParquetReadSupport: Going to read the following fields from the Parquet file with the following schema:\n",
      "Parquet file schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Parquet clipped schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Parquet requested schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Catalyst requested schema:\n",
      "root\n",
      "-- id: string (nullable = true)\n",
      "-- city: string (nullable = true)\n",
      "-- name: string (nullable = true)\n",
      "\n",
      "       \n",
      "24/12/04 16:53:33 DEBUG ParquetReadSupport: Going to read the following fields from the Parquet file with the following schema:\n",
      "Parquet file schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Parquet clipped schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Parquet requested schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Catalyst requested schema:\n",
      "root\n",
      "-- id: string (nullable = true)\n",
      "-- city: string (nullable = true)\n",
      "-- name: string (nullable = true)\n",
      "\n",
      "       \n",
      "24/12/04 16:53:33 DEBUG ParquetFileFormat: Appending StructType() [empty row]\n",
      "24/12/04 16:53:33 DEBUG ParquetFileFormat: Appending StructType() [empty row]\n",
      "24/12/04 16:53:33 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.326Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":8},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileFormat: Appending StructType() [empty row]\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.326Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":8},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileFormat: Appending StructType() [empty row]\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@49a1d723, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@49a1d723, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Dataset URN: Optional[urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD)], alias_list: {}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Building input with /tmp/test\n",
      "24/12/04 16:53:33 DEBUG CodecPool: Got recycled decompressor\n",
      "24/12/04 16:53:33 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 11 bytes\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 11 bytes\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 19 bytes\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 16 bytes\n",
      "24/12/04 16:53:33 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 17 bytes\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 17 bytes\n",
      "24/12/04 16:53:33 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 11 bytes\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 19 bytes\n",
      "24/12/04 16:53:33 DEBUG DFSClient: DeadNode detection is not enabled or given block LocatedBlocks{;  fileLength=943;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741996_1172; getBlockSize()=943; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741996_1172; getBlockSize()=943; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null} is null, skip to remove node.\n",
      "24/12/04 16:53:33 DEBUG DFSClient: DeadNode detection is not enabled or given block LocatedBlocks{;  fileLength=964;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741992_1168; getBlockSize()=964; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741992_1168; getBlockSize()=964; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null} is null, skip to remove node.\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 17 bytes\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 11 bytes\n",
      "24/12/04 16:53:33 DEBUG DFSClient: DeadNode detection is not enabled or given block LocatedBlocks{;  fileLength=963;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741993_1169; getBlockSize()=963; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741993_1169; getBlockSize()=963; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null} is null, skip to remove node.\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 16 bytes\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={jobId=8, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), jobInfo={customProperties={jobId=8, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306013326}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306013347}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1733306013326, status=STARTED}, dataProcessInstanceProperties={name=01939116-d5f7-7f29-82a1-b6534ebd5626, created={actor=urn:li:corpuser:datahub, time=1733306013326}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-d5f7-7f29-82a1-b6534ebd5626, inSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD), schemaMetadata={platformSchema={com.linkedin.schema.MySqlDDL={tableSchema=[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}}, schemaName=, fields=[{fieldPath=id, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=city, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=name, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}], version=1, hash=, platform=urn:li:dataPlatform:hdfs}, lineage=null)], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306013326, eventFormatter=datahub.event.EventFormatter@eafca4c), from {\"eventTime\":\"2024-12-04T09:53:33.326Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":8},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Collecting lineage successfully in 6 ms\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onJobStart completed successfully in 17 ms\n",
      "24/12/04 16:53:33 DEBUG BytesInput$StreamBytesInput: read all 16 bytes\n",
      "24/12/04 16:53:33 DEBUG DFSClient: DeadNode detection is not enabled or given block LocatedBlocks{;  fileLength=936;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741991_1167; getBlockSize()=936; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741991_1167; getBlockSize()=936; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null} is null, skip to remove node.\n",
      "24/12/04 16:53:33 INFO Executor: Finished task 2.0 in stage 8.0 (TID 84). 1707 bytes result sent to driver\n",
      "24/12/04 16:53:33 INFO Executor: Finished task 1.0 in stage 8.0 (TID 83). 1710 bytes result sent to driver\n",
      "24/12/04 16:53:33 INFO Executor: Finished task 0.0 in stage 8.0 (TID 82). 1710 bytes result sent to driver\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (8, 0) -> 3\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (8, 0) -> 2\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (8, 0) -> 1\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 84) in 19 ms on hadoop113 (executor driver) (1/4)\n",
      "24/12/04 16:53:33 INFO Executor: Finished task 3.0 in stage 8.0 (TID 85). 1702 bytes result sent to driver\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (8, 0) -> 0\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 83) in 19 ms on hadoop113 (executor driver) (2/4)\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 1 ms\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 82) in 21 ms on hadoop113 (executor driver) (3/4)\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Finished task 3.0 in stage 8.0 (TID 85) in 20 ms on hadoop113 (executor driver) (4/4)\n",
      "24/12/04 16:53:33 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:33 INFO DAGScheduler: ResultStage 8 (showString at NativeMethodAccessorImpl.java:0) finished in 0.023 s\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: After removal of stage 8, remaining stages = 0\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/04 16:53:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: SparkListenerJobEnd - executionId: 4\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: Event already finished, returning\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onJobEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Job 8 finished: showString at NativeMethodAccessorImpl.java:0, took 0.024301 s\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "24/12/04 16:53:33 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "24/12/04 16:53:33 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 27 took 0.000067 seconds\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Got job 9 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Final stage: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: submitStage(ResultStage 9 (name=showString at NativeMethodAccessorImpl.java:0;jobs=9))\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: missing: List()\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: submitMissingTasks(ResultStage 9)\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: SparkListenerJobStart - executionId: 4\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 13.7 KiB, free 363.9 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_15 locally took 0 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_15 without replication took 0 ms\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.GlobalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.LocalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.datasources.LogicalRelation\",\"num-children\":0,\"relation\":null,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":48,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":49,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":50,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"isStreaming\":false}]\n",
      "24/12/04 16:53:33 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 363.9 MiB)\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.CollectLimitExec\",\"num-children\":1,\"limit\":21,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.ColumnarToRowExec\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.InputAdapter\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.FileSourceScanExec\",\"num-children\":0,\"relation\":null,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":48,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":49,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":50,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"requiredSchema\":{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"partitionFilters\":[],\"dataFilters\":[],\"disableBucketedScan\":false}]\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_15_piece0 for BlockManagerId(driver, hadoop113, 38317, None)\n",
      "24/12/04 16:53:33 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on hadoop113:38317 (size: 6.0 KiB, free: 366.0 MiB)\n",
      "24/12/04 16:53:33 DEBUG BlockManagerMaster: Updated info of block broadcast_15_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Told master about block broadcast_15_piece0\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Put block broadcast_15_piece0 locally took 0 ms\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Putting block broadcast_15_piece0 without replication took 0 ms\n",
      "24/12/04 16:53:33 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#48,city#49,name#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://hadoop:9000/tmp/test], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,city:string,name:string>\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5))\n",
      "24/12/04 16:53:33 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "24/12/04 16:53:33 DEBUG TaskSetManager: Epoch for TaskSet 9.0: 0\n",
      "24/12/04 16:53:33 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "24/12/04 16:53:33 DEBUG TaskSetManager: Valid locality levels for TaskSet 9.0: ANY\n",
      "24/12/04 16:53:33 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_9.0, runningTasks: 0\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 86) (hadoop113, executor driver, partition 5, ANY, 4966 bytes) taskResourceAssignments Map()\n",
      "24/12/04 16:53:33 INFO Executor: Running task 0.0 in stage 9.0 (TID 86)\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #130 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (9, 0) -> 1\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Getting local block broadcast_15\n",
      "24/12/04 16:53:33 DEBUG BlockManager: Level for block broadcast_15 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #130\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:33 DEBUG HdfsPathDataset: path: hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 INFO HdfsPathDataset: No path_spec_list configuration found. Falling back to creating dataset name with complete uri\n",
      "24/12/04 16:53:33 DEBUG PlanUtils: Path hdfs://hadoop:9000/tmp/test transformed to hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 INFO FileScanRDD: Reading File path: hdfs://hadoop:9000/tmp/test/part-00000-5187aa4d-e649-457b-8943-b641f61c8259-c000.snappy.parquet, range: 0-469, partition values: [empty row]\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #131 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #131\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:33 DEBUG DecryptionPropertiesFactory: DecryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #132 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#48,city#49,name#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://hadoop:9000/tmp/test], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,city:string,name:string>\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #132\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getBlockLocations took 1ms\n",
      "24/12/04 16:53:33 DEBUG DFSClient: newInfo = LocatedBlocks{;  fileLength=469;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741994_1170; getBlockSize()=469; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741994_1170; getBlockSize()=469; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null}\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: File length 469\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: reading footer index at 461\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 INFO RemovePathPatternUtils: Removing path pattern from dataset name /tmp/test\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: read footer length: 457, footer index: 4\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG HdfsPathDataset: path: /tmp/test\n",
      "24/12/04 16:53:33 DEBUG RemovePathPatternUtils: Transformed path is /tmp/test\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: Posting event for start 4: io.openlineage.client.OpenLineage$RunEvent@63f17408\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: Finished to read all footer bytes.\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: FileMetaData(version:1, schema:[SchemaElement(name:spark_schema, num_children:3), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:id, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:name, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>)], num_rows:0, row_groups:null, key_value_metadata:[KeyValue(key:org.apache.spark.version, value:3.3.1), KeyValue(key:org.apache.spark.sql.parquet.row.metadata, value:{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]})], created_by:parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94))\n",
      "24/12/04 16:53:33 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.354Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":9},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.354Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":9},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@3f735d67, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"spark_schema\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"id\" ], [ \"city\" ], [ \"name\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      } ],\n",
      "      \"fieldCount\" : 3,\n",
      "      \"primitive\" : false,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"org.apache.spark.version\" : \"3.3.1\",\n",
      "      \"org.apache.spark.sql.parquet.row.metadata\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\",\n",
      "    \"fileDecryptor\" : null\n",
      "  },\n",
      "  \"blocks\" : [ ]\n",
      "}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@3f735d67, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Dataset URN: Optional[urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD)], alias_list: {}\n",
      "24/12/04 16:53:33 DEBUG DFSClient: DeadNode detection is not enabled or given block LocatedBlocks{;  fileLength=469;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741994_1170; getBlockSize()=469; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741994_1170; getBlockSize()=469; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null} is null, skip to remove node.\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Building input with /tmp/test\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={jobId=9, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), jobInfo={customProperties={jobId=9, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306013354}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306013364}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1733306013354, status=STARTED}, dataProcessInstanceProperties={name=01939116-d5f7-7f29-82a1-b6534ebd5626, created={actor=urn:li:corpuser:datahub, time=1733306013354}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-d5f7-7f29-82a1-b6534ebd5626, inSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD), schemaMetadata={platformSchema={com.linkedin.schema.MySqlDDL={tableSchema=[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}}, schemaName=, fields=[{fieldPath=id, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=city, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=name, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}], version=1, hash=, platform=urn:li:dataPlatform:hdfs}, lineage=null)], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306013354, eventFormatter=datahub.event.EventFormatter@2ef3b6da), from {\"eventTime\":\"2024-12-04T09:53:33.354Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":9},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onJobStart completed successfully in 11 ms\n",
      "24/12/04 16:53:33 DEBUG DecryptionPropertiesFactory: DecryptionPropertiesFactory is not configured - name not found in hadoop config\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #133 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #133\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 0ms\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #134 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #134\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getBlockLocations took 1ms\n",
      "24/12/04 16:53:33 DEBUG DFSClient: newInfo = LocatedBlocks{;  fileLength=469;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741994_1170; getBlockSize()=469; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741994_1170; getBlockSize()=469; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null}\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: File length 469\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: reading footer index at 461\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: read footer length: 457, footer index: 4\n",
      "24/12/04 16:53:33 DEBUG DFSClient: Connecting to datanode 172.25.1.12:9866\n",
      "24/12/04 16:53:33 DEBUG ParquetFileReader: Finished to read all footer bytes.\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: FileMetaData(version:1, schema:[SchemaElement(name:spark_schema, num_children:3), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:id, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:city, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>), SchemaElement(type:BYTE_ARRAY, repetition_type:OPTIONAL, name:name, converted_type:UTF8, logicalType:<LogicalType STRING:StringType()>)], num_rows:0, row_groups:[], key_value_metadata:[KeyValue(key:org.apache.spark.version, value:3.3.1), KeyValue(key:org.apache.spark.sql.parquet.row.metadata, value:{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]})], created_by:parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94), column_orders:[<ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>, <ColumnOrder TYPE_ORDER:TypeDefinedOrder()>])\n",
      "24/12/04 16:53:33 DEBUG ParquetMetadataConverter: {\n",
      "  \"fileMetaData\" : {\n",
      "    \"schema\" : {\n",
      "      \"name\" : \"spark_schema\",\n",
      "      \"repetition\" : \"REPEATED\",\n",
      "      \"logicalTypeAnnotation\" : null,\n",
      "      \"id\" : null,\n",
      "      \"fields\" : [ {\n",
      "        \"name\" : \"id\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"city\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      }, {\n",
      "        \"name\" : \"name\",\n",
      "        \"repetition\" : \"OPTIONAL\",\n",
      "        \"logicalTypeAnnotation\" : { },\n",
      "        \"id\" : null,\n",
      "        \"primitive\" : true,\n",
      "        \"decimalMetadata\" : null,\n",
      "        \"typeLength\" : 0,\n",
      "        \"primitiveTypeName\" : \"BINARY\",\n",
      "        \"originalType\" : \"UTF8\"\n",
      "      } ],\n",
      "      \"paths\" : [ [ \"id\" ], [ \"city\" ], [ \"name\" ] ],\n",
      "      \"columns\" : [ {\n",
      "        \"path\" : [ \"id\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"id\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"city\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"city\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      }, {\n",
      "        \"path\" : [ \"name\" ],\n",
      "        \"type\" : \"BINARY\",\n",
      "        \"typeLength\" : 0,\n",
      "        \"maxRepetitionLevel\" : 0,\n",
      "        \"maxDefinitionLevel\" : 1,\n",
      "        \"primitiveType\" : {\n",
      "          \"name\" : \"name\",\n",
      "          \"repetition\" : \"OPTIONAL\",\n",
      "          \"logicalTypeAnnotation\" : { },\n",
      "          \"id\" : null,\n",
      "          \"primitive\" : true,\n",
      "          \"decimalMetadata\" : null,\n",
      "          \"typeLength\" : 0,\n",
      "          \"primitiveTypeName\" : \"BINARY\",\n",
      "          \"originalType\" : \"UTF8\"\n",
      "        }\n",
      "      } ],\n",
      "      \"fieldCount\" : 3,\n",
      "      \"primitive\" : false,\n",
      "      \"originalType\" : null\n",
      "    },\n",
      "    \"keyValueMetaData\" : {\n",
      "      \"org.apache.spark.version\" : \"3.3.1\",\n",
      "      \"org.apache.spark.sql.parquet.row.metadata\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"city\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\"\n",
      "    },\n",
      "    \"createdBy\" : \"parquet-mr version 1.12.2 (build 77e30c8093386ec52c3cfa6c34b7ef3321322c94)\",\n",
      "    \"fileDecryptor\" : null\n",
      "  },\n",
      "  \"blocks\" : [ ]\n",
      "}\n",
      "24/12/04 16:53:33 DEBUG ParquetReadSupport: Going to read the following fields from the Parquet file with the following schema:\n",
      "Parquet file schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Parquet clipped schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Parquet requested schema:\n",
      "message spark_schema {\n",
      "  optional binary id (STRING);\n",
      "  optional binary city (STRING);\n",
      "  optional binary name (STRING);\n",
      "}\n",
      "\n",
      "Catalyst requested schema:\n",
      "root\n",
      "-- id: string (nullable = true)\n",
      "-- city: string (nullable = true)\n",
      "-- name: string (nullable = true)\n",
      "\n",
      "       \n",
      "24/12/04 16:53:33 DEBUG ParquetFileFormat: Appending StructType() [empty row]\n",
      "24/12/04 16:53:33 DEBUG DFSClient: DeadNode detection is not enabled or given block LocatedBlocks{;  fileLength=469;  underConstruction=false;  blocks=[LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741994_1170; getBlockSize()=469; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]}];  lastLocatedBlock=LocatedBlock{BP-71074493-127.0.0.1-1733217510273:blk_1073741994_1170; getBlockSize()=469; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[172.25.1.12:9866,DS-0b947a4d-f204-46b4-8552-ce209f80a8f9,DISK]]; cachedLocs=[]};  isLastBlockComplete=true;  ecPolicy=null} is null, skip to remove node.\n",
      "24/12/04 16:53:33 INFO Executor: Finished task 0.0 in stage 9.0 (TID 86). 1606 bytes result sent to driver\n",
      "24/12/04 16:53:33 DEBUG ExecutorMetricsPoller: stageTCMP: (9, 0) -> 0\n",
      "24/12/04 16:53:33 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 86) in 15 ms on hadoop113 (executor driver) (1/1)\n",
      "24/12/04 16:53:33 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:33 INFO DAGScheduler: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0) finished in 0.017 s\n",
      "24/12/04 16:53:33 DEBUG DAGScheduler: After removal of stage 9, remaining stages = 0\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/04 16:53:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: SparkListenerJobEnd - executionId: 4\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: Event already finished, returning\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: onJobEnd completed successfully in 0 ms\n",
      "24/12/04 16:53:33 INFO DAGScheduler: Job 9 finished: showString at NativeMethodAccessorImpl.java:0, took 0.018761 s\n",
      "24/12/04 16:53:33 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/04 16:53:33 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionEnd - executionId: 4\n",
      "+---+---------+----------+\n",
      "| id|     city|      name|\n",
      "+---+---------+----------+\n",
      "|  6|  Mizoram|Lalpekhlua|\n",
      "|  2|Telangana|   Chhetri|\n",
      "|  4|Hyderabad|   Shabbir|\n",
      "|  5|   Kerala|   Vijayan|\n",
      "|  3|   Sikkim|    Bhutia|\n",
      "+---+---------+----------+\n",
      "\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.GlobalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.LocalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.datasources.LogicalRelation\",\"num-children\":0,\"relation\":null,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":48,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":49,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":50,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"isStreaming\":false}]\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.CollectLimitExec\",\"num-children\":1,\"limit\":21,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.ColumnarToRowExec\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.InputAdapter\",\"num-children\":1,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.FileSourceScanExec\",\"num-children\":0,\"relation\":null,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":48,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":49,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":50,\"jvmId\":\"37d93951-5992-41e9-81e6-ec80e03a096c\"},\"qualifier\":[]}]],\"requiredSchema\":{\"type\":\"struct\",\"fields\":[{\"name\":\"id\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"city\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"partitionFilters\":[],\"dataFilters\":[],\"disableBucketedScan\":false}]\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#48,city#49,name#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://hadoop:9000/tmp/test], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,city:string,name:string>\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root sending #135 org.apache.hadoop.hdfs.protocol.ClientProtocol.getFileInfo\n",
      "24/12/04 16:53:33 DEBUG Client: IPC Client (667649826) connection to hadoop/172.25.1.12:9000 from root got value #135\n",
      "24/12/04 16:53:33 DEBUG ProtobufRpcEngine2: Call: getFileInfo took 1ms\n",
      "24/12/04 16:53:33 DEBUG HdfsPathDataset: path: hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 INFO HdfsPathDataset: No path_spec_list configuration found. Falling back to creating dataset name with complete uri\n",
      "24/12/04 16:53:33 DEBUG PlanUtils: Path hdfs://hadoop:9000/tmp/test transformed to hdfs://hadoop:9000/tmp/test\n",
      "24/12/04 16:53:33 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#48 as string) AS id#57, cast(city#49 as string) AS city#58, cast(name#50 as string) AS name#59]\n",
      "      +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Relation [id#48,city#49,name#50] parquet\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [id#48,city#49,name#50] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[hdfs://hadoop:9000/tmp/test], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,city:string,name:string>\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/04 16:53:33 INFO RemovePathPatternUtils: Removing path pattern from dataset name /tmp/test\n",
      "24/12/04 16:53:33 DEBUG HdfsPathDataset: path: /tmp/test\n",
      "24/12/04 16:53:33 DEBUG RemovePathPatternUtils: Transformed path is /tmp/test\n",
      "24/12/04 16:53:33 DEBUG SparkSQLExecutionContext: Posting event for end 4: {\"eventTime\":\"2024-12-04T09:53:33.375Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.375Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-04T09:53:33.375Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@63f723e3, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/04 16:53:33 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@63f723e3, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Dataset URN: Optional[urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD)], alias_list: {}\n",
      "24/12/04 16:53:33 DEBUG OpenLineageToDataHub: Building input with /tmp/test\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1733306013375}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1733306013386}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1733306013375, status=COMPLETE}, dataProcessInstanceProperties={name=01939116-d5f7-7f29-82a1-b6534ebd5626, created={actor=urn:li:corpuser:datahub, time=1733306013375}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:01939116-d5f7-7f29-82a1-b6534ebd5626, inSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test,PROD), schemaMetadata={platformSchema={com.linkedin.schema.MySqlDDL={tableSchema=[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}}, schemaName=, fields=[{fieldPath=id, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=city, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=name, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}], version=1, hash=, platform=urn:li:dataPlatform:hdfs}, lineage=null)], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1733306013375, eventFormatter=datahub.event.EventFormatter@6fabf7a0), from {\"eventTime\":\"2024-12-04T09:53:33.375Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"01939116-d5f7-7f29-82a1-b6534ebd5626\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"01939116-ca59-7d64-89c0-0f8942dab124\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[{\"namespace\":\"hdfs://hadoop:9000\",\"name\":\"/tmp/test\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"hdfs://hadoop:9000\",\"uri\":\"hdfs://hadoop:9000\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"id\",\"type\":\"string\"},{\"name\":\"city\",\"type\":\"string\"},{\"name\":\"name\",\"type\":\"string\"}]}},\"inputFacets\":{}}],\"outputs\":[]}\n",
      "24/12/04 16:53:33 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/04 16:53:37 DEBUG ExecutorMetricsPoller: removing (1, 0) from stageTCMP\n",
      "24/12/04 16:53:37 DEBUG ExecutorMetricsPoller: removing (2, 0) from stageTCMP\n",
      "24/12/04 16:53:37 DEBUG ExecutorMetricsPoller: removing (7, 0) from stageTCMP\n",
      "24/12/04 16:53:37 DEBUG ExecutorMetricsPoller: removing (5, 0) from stageTCMP\n",
      "24/12/04 16:53:37 DEBUG ExecutorMetricsPoller: removing (4, 0) from stageTCMP\n",
      "24/12/04 16:53:37 DEBUG ExecutorMetricsPoller: removing (8, 0) from stageTCMP\n",
      "24/12/04 16:53:37 DEBUG ExecutorMetricsPoller: removing (9, 0) from stageTCMP\n",
      "24/12/04 16:53:37 DEBUG ExecutorMetricsPoller: removing (0, 0) from stageTCMP\n",
      "24/12/04 16:53:37 DEBUG ExecutorMetricsPoller: removing (6, 0) from stageTCMP\n",
      "24/12/04 16:53:37 DEBUG ExecutorMetricsPoller: removing (3, 0) from stageTCMP\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"hdfs://hadoop:9000/tmp/test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1addcfd1-36d4-4b99-aa3a-33409b9b5568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
