{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b29876-205a-497e-a1b2-87ac1b13b8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/17 10:10:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Initialize Spark session with HBase configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WriteToHbase\") \\\n",
    "    .config(\"spark.jars\", \"/tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,/tmp/hbase-spark-1.1.0-SNAPSHOT.jar\") \\\n",
    "    .config(\"spark.extraListeners\", \"datahub.spark.DatahubSparkListener\")\\\n",
    "    .config(\"spark.datahub.rest.server\", \"http://10.208.164.167:8080\")\\\n",
    "    .config(\"spark.datahub.metadata.dataset.materialize\", \"true\")\\\n",
    "    .config(\"spark.datahub.metadata.dataset.experimental_include_schema_metadata\", \"true\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# .config(\"spark.hadoop.hbase.zookeeper.quorum\", \"localhost1\") -> oke ăn config này nhé (còn đâu khả năng nó ăn mặc định, hoặc là file habse.xml tỏng classpath)\n",
    "# .config(\"spark.hbase.host\", \"localhost:2182\")  -> khong chay \n",
    "# print(os.environ[\"HBASE_HOME\"])\n",
    "# os.environ[\"HBASE_HOME\"] =\"/tmp/a\" -> không work\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55b6b3e-6716-42ee-9e4b-8a08e05b0cf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/17 10:10:08 DEBUG FileSystem: Looking for FS supporting file\n",
      "24/12/17 10:10:08 DEBUG FileSystem: looking for configuration option fs.file.impl\n",
      "24/12/17 10:10:08 DEBUG FileSystem: Looking in service filesystems for implementation class\n",
      "24/12/17 10:10:08 DEBUG FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem\n",
      "24/12/17 10:10:08 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/12/17 10:10:08 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.datahub.metadata.dataset.experimental_include_schema_metadata -> true\n",
      "24/12/17 10:10:08 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.extraListeners -> datahub.spark.DatahubSparkListener\n",
      "24/12/17 10:10:08 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.jars -> /tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,/tmp/hbase-spark-1.1.0-SNAPSHOT.jar\n",
      "24/12/17 10:10:08 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.app.name -> WriteToHbase\n",
      "24/12/17 10:10:08 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.datahub.rest.server -> http://10.208.164.167:8080\n",
      "24/12/17 10:10:08 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.datahub.metadata.dataset.materialize -> true\n",
      "24/12/17 10:10:08 INFO SharedState: Warehouse path is 'file:/spark-warehouse'.\n",
      "24/12/17 10:10:08 DEBUG FsUrlStreamHandlerFactory: Creating handler for protocol jar\n",
      "24/12/17 10:10:08 DEBUG FileSystem: Looking for FS supporting jar\n",
      "24/12/17 10:10:08 DEBUG FileSystem: looking for configuration option fs.jar.impl\n",
      "24/12/17 10:10:08 DEBUG FileSystem: Looking in service filesystems for implementation class\n",
      "24/12/17 10:10:08 DEBUG FsUrlStreamHandlerFactory: Unknown protocol jar, delegating to default implementation\n",
      "24/12/17 10:10:08 DEBUG FsUrlStreamHandlerFactory: Creating handler for protocol file\n",
      "24/12/17 10:10:08 DEBUG FileSystem: Looking for FS supporting file\n",
      "24/12/17 10:10:08 DEBUG FileSystem: looking for configuration option fs.file.impl\n",
      "24/12/17 10:10:08 DEBUG FileSystem: Looking in service filesystems for implementation class\n",
      "24/12/17 10:10:08 DEBUG FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem\n",
      "24/12/17 10:10:08 DEBUG FsUrlStreamHandlerFactory: Found implementation of file: class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem\n",
      "24/12/17 10:10:08 DEBUG FsUrlStreamHandlerFactory: Using handler for protocol file\n",
      "24/12/17 10:10:08 INFO ArgumentParser: Couldn't log config from file, will read it from SparkConf\n",
      "24/12/17 10:10:08 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/17 10:10:08 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/17 10:10:08 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/17 10:10:09 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/17 10:10:09 DEBUG KafkaRelationVisitor: Kafka classes have not been checked yet\n",
      "24/12/17 10:10:09 DEBUG KafkaRelationVisitor: Kafka classes availability: false\n",
      "24/12/17 10:10:09 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/17 10:10:09 DEBUG SparkApplicationExecutionContext: SparkListenerApplicationStart - applicationId: local-1734405008070\n",
      "24/12/17 10:10:09 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional.empty with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:09 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional.empty with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:09 DEBUG SparkApplicationExecutionContext: Posting event for applicationId local-1734405008070 start: io.openlineage.client.OpenLineage$RunEvent@170821e8\n",
      "24/12/17 10:10:09 DEBUG OpenLineageClient: OpenLineageClient will emit lineage event: {\"eventTime\":\"2024-12-17T03:10:07.506Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-2e34-71e9-9f04-2084645b8c30\",\"facets\":{\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"spark_applicationDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"master\":\"local[*]\",\"appName\":\"WriteToHbase\",\"applicationId\":\"local-1734405008070\",\"deployMode\":\"client\",\"driverHost\":\"hadoop113\",\"userName\":\"root\",\"uiWebUrl\":\"http://hadoop113:4040\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"NONE\",\"integration\":\"SPARK\",\"jobType\":\"APPLICATION\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:09 INFO ConsoleTransport: {\"eventTime\":\"2024-12-17T03:10:07.506Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-2e34-71e9-9f04-2084645b8c30\",\"facets\":{\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"spark_applicationDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"master\":\"local[*]\",\"appName\":\"WriteToHbase\",\"applicationId\":\"local-1734405008070\",\"deployMode\":\"client\",\"driverHost\":\"hadoop113\",\"userName\":\"root\",\"uiWebUrl\":\"http://hadoop113:4040\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"NONE\",\"integration\":\"SPARK\",\"jobType\":\"APPLICATION\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:09 DEBUG EventEmitter: Emitting lineage completed successfully: {\"eventTime\":\"2024-12-17T03:10:07.506Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-2e34-71e9-9f04-2084645b8c30\",\"facets\":{\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"spark_applicationDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"master\":\"local[*]\",\"appName\":\"WriteToHbase\",\"applicationId\":\"local-1734405008070\",\"deployMode\":\"client\",\"driverHost\":\"hadoop113\",\"userName\":\"root\",\"uiWebUrl\":\"http://hadoop113:4040\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"NONE\",\"integration\":\"SPARK\",\"jobType\":\"APPLICATION\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:09 INFO DatahubSparkListener: onApplicationStart completed successfully in 649 ms\n",
      "24/12/17 10:10:09 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$pythonToJava$1\n",
      "24/12/17 10:10:09 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$pythonToJava$1) is now cleaned +++\n",
      "24/12/17 10:10:09 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$toJavaArray$1\n",
      "24/12/17 10:10:09 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$toJavaArray$1) is now cleaned +++\n",
      "24/12/17 10:10:09 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$applySchemaToPythonRDD$1\n",
      "24/12/17 10:10:09 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$applySchemaToPythonRDD$1) is now cleaned +++\n",
      "24/12/17 10:10:09 DEBUG CatalystSqlParser: Parsing command: spark_grouping_id\n",
      "24/12/17 10:10:10 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart\n",
      "24/12/17 10:10:10 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/17 10:10:10 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/17 10:10:10 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/17 10:10:10 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionStart - executionId: 0\n",
      "24/12/17 10:10:10 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator rdd_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     rdd_input_0 = inputs[0];\n",
      "/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( rdd_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();\n",
      "/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 028 */       boolean rdd_isNull_0 = rdd_row_0.isNullAt(0);\n",
      "/* 029 */       UTF8String rdd_value_0 = rdd_isNull_0 ?\n",
      "/* 030 */       null : (rdd_row_0.getUTF8String(0));\n",
      "/* 031 */       boolean rdd_isNull_1 = rdd_row_0.isNullAt(1);\n",
      "/* 032 */       UTF8String rdd_value_1 = rdd_isNull_1 ?\n",
      "/* 033 */       null : (rdd_row_0.getUTF8String(1));\n",
      "/* 034 */       boolean rdd_isNull_2 = rdd_row_0.isNullAt(2);\n",
      "/* 035 */       UTF8String rdd_value_2 = rdd_isNull_2 ?\n",
      "/* 036 */       null : (rdd_row_0.getUTF8String(2));\n",
      "/* 037 */       rdd_mutableStateArray_0[0].reset();\n",
      "/* 038 */\n",
      "/* 039 */       rdd_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 040 */\n",
      "/* 041 */       if (rdd_isNull_0) {\n",
      "/* 042 */         rdd_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 043 */       } else {\n",
      "/* 044 */         rdd_mutableStateArray_0[0].write(0, rdd_value_0);\n",
      "/* 045 */       }\n",
      "/* 046 */\n",
      "/* 047 */       if (rdd_isNull_1) {\n",
      "/* 048 */         rdd_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */       } else {\n",
      "/* 050 */         rdd_mutableStateArray_0[0].write(1, rdd_value_1);\n",
      "/* 051 */       }\n",
      "/* 052 */\n",
      "/* 053 */       if (rdd_isNull_2) {\n",
      "/* 054 */         rdd_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 055 */       } else {\n",
      "/* 056 */         rdd_mutableStateArray_0[0].write(2, rdd_value_2);\n",
      "/* 057 */       }\n",
      "/* 058 */       append((rdd_mutableStateArray_0[0].getRow()));\n",
      "/* 059 */       if (shouldStop()) return;\n",
      "/* 060 */     }\n",
      "/* 061 */   }\n",
      "/* 062 */\n",
      "/* 063 */ }\n",
      "\n",
      "24/12/17 10:10:10 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator rdd_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     rdd_input_0 = inputs[0];\n",
      "/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( rdd_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();\n",
      "/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 028 */       boolean rdd_isNull_0 = rdd_row_0.isNullAt(0);\n",
      "/* 029 */       UTF8String rdd_value_0 = rdd_isNull_0 ?\n",
      "/* 030 */       null : (rdd_row_0.getUTF8String(0));\n",
      "/* 031 */       boolean rdd_isNull_1 = rdd_row_0.isNullAt(1);\n",
      "/* 032 */       UTF8String rdd_value_1 = rdd_isNull_1 ?\n",
      "/* 033 */       null : (rdd_row_0.getUTF8String(1));\n",
      "/* 034 */       boolean rdd_isNull_2 = rdd_row_0.isNullAt(2);\n",
      "/* 035 */       UTF8String rdd_value_2 = rdd_isNull_2 ?\n",
      "/* 036 */       null : (rdd_row_0.getUTF8String(2));\n",
      "/* 037 */       rdd_mutableStateArray_0[0].reset();\n",
      "/* 038 */\n",
      "/* 039 */       rdd_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 040 */\n",
      "/* 041 */       if (rdd_isNull_0) {\n",
      "/* 042 */         rdd_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 043 */       } else {\n",
      "/* 044 */         rdd_mutableStateArray_0[0].write(0, rdd_value_0);\n",
      "/* 045 */       }\n",
      "/* 046 */\n",
      "/* 047 */       if (rdd_isNull_1) {\n",
      "/* 048 */         rdd_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */       } else {\n",
      "/* 050 */         rdd_mutableStateArray_0[0].write(1, rdd_value_1);\n",
      "/* 051 */       }\n",
      "/* 052 */\n",
      "/* 053 */       if (rdd_isNull_2) {\n",
      "/* 054 */         rdd_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 055 */       } else {\n",
      "/* 056 */         rdd_mutableStateArray_0[0].write(2, rdd_value_2);\n",
      "/* 057 */       }\n",
      "/* 058 */       append((rdd_mutableStateArray_0[0].getRow()));\n",
      "/* 059 */       if (shouldStop()) return;\n",
      "/* 060 */     }\n",
      "/* 061 */   }\n",
      "/* 062 */\n",
      "/* 063 */ }\n",
      "\n",
      "24/12/17 10:10:10 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.GlobalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.LocalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}]\n",
      "24/12/17 10:10:10 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.CollectLimitExec\",\"num-children\":1,\"limit\":21,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.RDDScanExec\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"name\":\"ExistingRDD\",\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[]}]\n",
      "24/12/17 10:10:10 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#0,city#1,name#2]\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:10 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#0,city#1,name#2]\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:10 DEBUG SparkSQLExecutionContext: Posting event for start 0: io.openlineage.client.OpenLineage$RunEvent@39352075\n",
      "24/12/17 10:10:10 DEBUG OpenLineageClient: OpenLineageClient will emit lineage event: {\"eventTime\":\"2024-12-17T03:10:10.449Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3416-7700-939e-92cd7d1575ab\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-2e34-71e9-9f04-2084645b8c30\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:10 INFO ConsoleTransport: {\"eventTime\":\"2024-12-17T03:10:10.449Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3416-7700-939e-92cd7d1575ab\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-2e34-71e9-9f04-2084645b8c30\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:10 DEBUG EventEmitter: Emitting lineage completed successfully: {\"eventTime\":\"2024-12-17T03:10:10.449Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3416-7700-939e-92cd7d1575ab\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-2e34-71e9-9f04-2084645b8c30\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:10 INFO CodeGenerator: Code generated in 84.861978 ms\n",
      "24/12/17 10:10:10 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "24/12/17 10:10:10 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "24/12/17 10:10:10 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "24/12/17 10:10:10 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "24/12/17 10:10:10 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "24/12/17 10:10:10 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "24/12/17 10:10:10 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/12/17 10:10:10 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 6 took 0.000447 seconds\n",
      "24/12/17 10:10:10 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/17 10:10:10 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/12/17 10:10:10 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/12/17 10:10:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/17 10:10:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/17 10:10:10 DEBUG DAGScheduler: submitStage(ResultStage 0 (name=showString at NativeMethodAccessorImpl.java:0;jobs=0))\n",
      "24/12/17 10:10:10 DEBUG DAGScheduler: missing: List()\n",
      "24/12/17 10:10:10 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/17 10:10:10 DEBUG DAGScheduler: submitMissingTasks(ResultStage 0)\n",
      "24/12/17 10:10:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 366.3 MiB)\n",
      "24/12/17 10:10:10 DEBUG BlockManager: Put block broadcast_0 locally took 14 ms\n",
      "24/12/17 10:10:10 DEBUG BlockManager: Putting block broadcast_0 without replication took 15 ms\n",
      "24/12/17 10:10:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 366.3 MiB)\n",
      "24/12/17 10:10:10 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, hadoop113, 34591, None)\n",
      "24/12/17 10:10:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop113:34591 (size: 6.4 KiB, free: 366.3 MiB)\n",
      "24/12/17 10:10:10 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0\n",
      "24/12/17 10:10:10 DEBUG BlockManager: Told master about block broadcast_0_piece0\n",
      "24/12/17 10:10:10 DEBUG BlockManager: Put block broadcast_0_piece0 locally took 8 ms\n",
      "24/12/17 10:10:10 DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took 8 ms\n",
      "24/12/17 10:10:10 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/17 10:10:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/12/17 10:10:10 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/12/17 10:10:10 DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0\n",
      "24/12/17 10:10:10 DEBUG TaskSetManager: Adding pending tasks took 1 ms\n",
      "24/12/17 10:10:10 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY\n",
      "24/12/17 10:10:10 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0\n",
      "24/12/17 10:10:10 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY\n",
      "24/12/17 10:10:10 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (hadoop113, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:10 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "24/12/17 10:10:10 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/12/17 10:10:10 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 1\n",
      "24/12/17 10:10:10 INFO DatahubSparkListener: sparkEnv: spark.app.id=local-1734405008070\n",
      "spark.app.initial.jar.urls=spark://hadoop113:36133/jars/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,spark://hadoop113:36133/jars/hbase-spark-1.1.0-SNAPSHOT.jar\n",
      "spark.app.name=WriteToHbase\n",
      "spark.app.startTime=1734405007506\n",
      "spark.app.submitTime=1734405007411\n",
      "spark.datahub.metadata.dataset.experimental_include_schema_metadata=true\n",
      "spark.datahub.metadata.dataset.materialize=true\n",
      "spark.datahub.rest.server=http://10.208.164.167:8080\n",
      "spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.driver.host=hadoop113\n",
      "spark.driver.port=36133\n",
      "spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.executor.id=driver\n",
      "spark.extraListeners=datahub.spark.DatahubSparkListener\n",
      "spark.jars=/tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,/tmp/hbase-spark-1.1.0-SNAPSHOT.jar\n",
      "spark.master=local[*]\n",
      "spark.rdd.compress=True\n",
      "spark.repl.local.jars=file:///tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,file:///tmp/hbase-spark-1.1.0-SNAPSHOT.jar\n",
      "spark.serializer.objectStreamReset=100\n",
      "spark.sql.warehouse.dir=file:/spark-warehouse\n",
      "spark.submit.deployMode=client\n",
      "spark.submit.pyFiles=\n",
      "spark.ui.showConsoleProgress=true\n",
      "24/12/17 10:10:10 INFO DatahubSparkListener: Datahub configuration: {\n",
      "    # String: 1-3\n",
      "    \"metadata\" : {\n",
      "        # String: 1-3\n",
      "        \"dataset\" : {\n",
      "            # String: 1\n",
      "            \"experimental_include_schema_metadata\" : \"true\",\n",
      "            # String: 3\n",
      "            \"materialize\" : \"true\"\n",
      "        }\n",
      "    },\n",
      "    # String: 2\n",
      "    \"rest\" : {\n",
      "        # String: 2\n",
      "        \"server\" : \"http://10.208.164.167:8080\"\n",
      "    }\n",
      "}\n",
      "\n",
      "24/12/17 10:10:10 INFO DatahubSparkListener: REST Emitter Configuration: GMS url http://10.208.164.167:8080\n",
      "24/12/17 10:10:10 DEBUG BlockManager: Getting local block broadcast_0\n",
      "24/12/17 10:10:10 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/17 10:10:10 DEBUG DatahubSparkListener: loadDatahubConfig completed successfully in 364 ms\n",
      "24/12/17 10:10:10 INFO ArgumentParser: Couldn't log config from file, will read it from SparkConf\n",
      "24/12/17 10:10:10 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework\n",
      "24/12/17 10:10:10 DEBUG CompositeMeterRegistry: A MeterFilter is being configured after a Meter has been registered to this registry. All MeterFilters should be configured before any Meters are registered. If that is not possible or you have a use case where it should be allowed, let the Micrometer maintainers know at https://github.com/micrometer-metrics/micrometer/issues/4920.\n",
      "java.lang.Thread.getStackTrace(Thread.java:1564)\n",
      "\tat io.micrometer.core.instrument.MeterRegistry$Config.logWarningAboutLateFilter(MeterRegistry.java:844)\n",
      "\tat io.micrometer.core.instrument.MeterRegistry$Config.meterFilter(MeterRegistry.java:830)\n",
      "\tat io.micrometer.core.instrument.MeterRegistry$Config.commonTags(MeterRegistry.java:807)\n",
      "\tat datahub.spark.DatahubSparkListener.initializeMetrics(DatahubSparkListener.java:292)\n",
      "\tat datahub.spark.DatahubSparkListener.initializeContextFactoryIfNotInitialized(DatahubSparkListener.java:339)\n",
      "\tat datahub.spark.DatahubSparkListener.initializeContextFactoryIfNotInitialized(DatahubSparkListener.java:328)\n",
      "\tat datahub.spark.DatahubSparkListener.lambda$initializeContextFactoryIfNotInitialized$1(DatahubSparkListener.java:314)\n",
      "\tat java.util.Optional.ifPresent(Optional.java:159)\n",
      "\tat datahub.spark.DatahubSparkListener.initializeContextFactoryIfNotInitialized(DatahubSparkListener.java:314)\n",
      "\tat datahub.spark.DatahubSparkListener.onJobStart(DatahubSparkListener.java:221)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "24/12/17 10:10:10 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/17 10:10:10 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/17 10:10:10 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/17 10:10:11 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/17 10:10:11 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/17 10:10:11 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: SparkListenerJobStart - executionId: 0\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.GlobalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.LocalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}]\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.CollectLimitExec\",\"num-children\":1,\"limit\":21,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.RDDScanExec\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"name\":\"ExistingRDD\",\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[]}]\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#0,city#1,name#2]\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#0,city#1,name#2]\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: Posting event for start 0: io.openlineage.client.OpenLineage$RunEvent@114aaa5e\n",
      "24/12/17 10:10:11 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:10.619Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":0},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:10.619Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":0},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@5a8db47e, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:11 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToHbase, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@5a8db47e, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:11 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), dataFlowInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, name=WriteToHbase}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.collect_limit), jobInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, created={time=1734405010619}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734405011119}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734405010619, status=STARTED}, dataProcessInstanceProperties={name=0193d298-3638-7fe3-9e16-b57e1d5ad8d9, created={actor=urn:li:corpuser:datahub, time=1734405010619}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193d298-3638-7fe3-9e16-b57e1d5ad8d9, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734405010619, eventFormatter=datahub.event.EventFormatter@1c1e9c63), from {\"eventTime\":\"2024-12-17T03:10:10.619Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":0},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 INFO DatahubEventEmitter: Collecting lineage successfully in 127 ms\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onJobStart completed successfully in 524 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 325, boot = 278, init = 47, finish = 0\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1832 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 0\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 487 ms on hadoop113 (executor driver) (1/1)\n",
      "24/12/17 10:10:11 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/12/17 10:10:11 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 57781\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 1 ms\n",
      "24/12/17 10:10:11 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 0.595 s\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: After removal of stage 0, remaining stages = 0\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/17 10:10:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: SparkListenerJobEnd - executionId: 0\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 0.616207 s\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.GlobalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.LocalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}]\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.CollectLimitExec\",\"num-children\":1,\"limit\":21,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.RDDScanExec\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"name\":\"ExistingRDD\",\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[]}]\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#0,city#1,name#2]\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#0,city#1,name#2]\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "24/12/17 10:10:11 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 6 took 0.000122 seconds\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: Posting event for end 0: io.openlineage.client.OpenLineage$RunEvent@eacf3af\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: submitStage(ResultStage 1 (name=showString at NativeMethodAccessorImpl.java:0;jobs=1))\n",
      "24/12/17 10:10:11 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:11.225Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: missing: List()\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:11.225Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: submitMissingTasks(ResultStage 1)\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@37c44306, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:11 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToHbase, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@37c44306, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:11 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, name=WriteToHbase}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.collect_limit), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, created={time=1734405011225}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734405011237}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734405011225, status=COMPLETE}, dataProcessInstanceProperties={name=0193d298-3638-7fe3-9e16-b57e1d5ad8d9, created={actor=urn:li:corpuser:datahub, time=1734405011225}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193d298-3638-7fe3-9e16-b57e1d5ad8d9, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734405011225, eventFormatter=datahub.event.EventFormatter@3fc4f5ab), from {\"eventTime\":\"2024-12-17T03:10:11.225Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onJobEnd completed successfully in 12 ms\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: SparkListenerJobStart - executionId: 0\n",
      "24/12/17 10:10:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.3 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Put block broadcast_1 locally took 0 ms\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Putting block broadcast_1 without replication took 0 ms\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.GlobalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.LocalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}]\n",
      "24/12/17 10:10:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 366.3 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, hadoop113, 34591, None)\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.CollectLimitExec\",\"num-children\":1,\"limit\":21,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.RDDScanExec\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"name\":\"ExistingRDD\",\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[]}]\n",
      "24/12/17 10:10:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop113:34591 (size: 6.4 KiB, free: 366.3 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Told master about block broadcast_1_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Put block broadcast_1_piece0 locally took 1 ms\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Putting block broadcast_1_piece0 without replication took 1 ms\n",
      "24/12/17 10:10:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/12/17 10:10:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#0,city#1,name#2]\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:11 DEBUG TaskSetManager: Epoch for TaskSet 1.0: 0\n",
      "24/12/17 10:10:11 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "24/12/17 10:10:11 DEBUG TaskSetManager: Valid locality levels for TaskSet 1.0: NO_PREF, ANY\n",
      "24/12/17 10:10:11 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 0\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (hadoop113, executor driver, partition 1, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (hadoop113, executor driver, partition 2, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (hadoop113, executor driver, partition 3, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#0,city#1,name#2]\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (hadoop113, executor driver, partition 4, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "24/12/17 10:10:11 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 1\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: Posting event for start 0: io.openlineage.client.OpenLineage$RunEvent@1d779946\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 2\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 3\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 4\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Getting local block broadcast_1\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "24/12/17 10:10:11 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:11.236Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":1},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:11.236Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":1},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@4bd07721, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:11 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToHbase, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@4bd07721, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:11 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), dataFlowInfo={customProperties={jobId=1, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, name=WriteToHbase}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.collect_limit), jobInfo={customProperties={jobId=1, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, created={time=1734405011236}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734405011259}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734405011236, status=STARTED}, dataProcessInstanceProperties={name=0193d298-3638-7fe3-9e16-b57e1d5ad8d9, created={actor=urn:li:corpuser:datahub, time=1734405011236}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193d298-3638-7fe3-9e16-b57e1d5ad8d9, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734405011236, eventFormatter=datahub.event.EventFormatter@72813503), from {\"eventTime\":\"2024-12-17T03:10:11.236Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":1},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 INFO DatahubEventEmitter: Collecting lineage successfully in 13 ms\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onJobStart completed successfully in 24 ms\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 44, boot = -59, init = 103, finish = 0\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 3\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 62 ms on hadoop113 (executor driver) (1/4)\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 1 ms\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 46, boot = 3, init = 43, finish = 0\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 48, boot = 5, init = 43, finish = 0\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 2\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 69 ms on hadoop113 (executor driver) (2/4)\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 1\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 73 ms on hadoop113 (executor driver) (3/4)\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 51, boot = 7, init = 43, finish = 1\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 1850 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 0\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 87 ms on hadoop113 (executor driver) (4/4)\n",
      "24/12/17 10:10:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.097 s\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/17 10:10:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: SparkListenerJobEnd - executionId: 0\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.099481 s\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: Event already finished, returning\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onJobEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "24/12/17 10:10:11 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 6 took 0.000067 seconds\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 15 output partitions\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: submitStage(ResultStage 2 (name=showString at NativeMethodAccessorImpl.java:0;jobs=2))\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: SparkListenerJobStart - executionId: 0\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: missing: List()\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: submitMissingTasks(ResultStage 2)\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.GlobalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.LocalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}]\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.CollectLimitExec\",\"num-children\":1,\"limit\":21,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.RDDScanExec\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"name\":\"ExistingRDD\",\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[]}]\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#0,city#1,name#2]\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:11 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.1 KiB, free 366.3 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Put block broadcast_2 locally took 0 ms\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Putting block broadcast_2 without replication took 0 ms\n",
      "24/12/17 10:10:11 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 366.2 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, hadoop113, 34591, None)\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#0,city#1,name#2]\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:11 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop113:34591 (size: 6.4 KiB, free: 366.3 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMaster: Updated info of block broadcast_2_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Told master about block broadcast_2_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Put block broadcast_2_piece0 locally took 1 ms\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Putting block broadcast_2_piece0 without replication took 1 ms\n",
      "24/12/17 10:10:11 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Submitting 15 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19))\n",
      "24/12/17 10:10:11 INFO TaskSchedulerImpl: Adding task set 2.0 with 15 tasks resource profile 0\n",
      "24/12/17 10:10:11 DEBUG TaskSetManager: Epoch for TaskSet 2.0: 0\n",
      "24/12/17 10:10:11 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "24/12/17 10:10:11 DEBUG TaskSetManager: Valid locality levels for TaskSet 2.0: NO_PREF, ANY\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: Posting event for start 0: io.openlineage.client.OpenLineage$RunEvent@5abf1f4a\n",
      "24/12/17 10:10:11 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2.0, runningTasks: 0\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5) (hadoop113, executor driver, partition 5, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6) (hadoop113, executor driver, partition 6, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 7) (hadoop113, executor driver, partition 7, PROCESS_LOCAL, 4482 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 8) (hadoop113, executor driver, partition 8, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 9) (hadoop113, executor driver, partition 9, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 10) (hadoop113, executor driver, partition 10, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:11.34Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":2},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 11) (hadoop113, executor driver, partition 11, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:11.34Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":2},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@758a490c, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 12) (hadoop113, executor driver, partition 12, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToHbase, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@758a490c, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 13) (hadoop113, executor driver, partition 13, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), dataFlowInfo={customProperties={jobId=2, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, name=WriteToHbase}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.collect_limit), jobInfo={customProperties={jobId=2, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, created={time=1734405011340}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734405011350}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734405011340, status=STARTED}, dataProcessInstanceProperties={name=0193d298-3638-7fe3-9e16-b57e1d5ad8d9, created={actor=urn:li:corpuser:datahub, time=1734405011340}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193d298-3638-7fe3-9e16-b57e1d5ad8d9, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734405011340, eventFormatter=datahub.event.EventFormatter@3e6957e9), from {\"eventTime\":\"2024-12-17T03:10:11.34Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":2},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onJobStart completed successfully in 10 ms\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 14) (hadoop113, executor driver, partition 14, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 15) (hadoop113, executor driver, partition 15, PROCESS_LOCAL, 4483 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 16) (hadoop113, executor driver, partition 16, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 17) (hadoop113, executor driver, partition 17, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 18) (hadoop113, executor driver, partition 18, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 19) (hadoop113, executor driver, partition 19, PROCESS_LOCAL, 4487 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "24/12/17 10:10:11 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 2.0 in stage 2.0 (TID 7)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 3.0 in stage 2.0 (TID 8)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 4.0 in stage 2.0 (TID 9)\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 1\n",
      "24/12/17 10:10:11 INFO Executor: Running task 5.0 in stage 2.0 (TID 10)\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 2\n",
      "24/12/17 10:10:11 INFO Executor: Running task 6.0 in stage 2.0 (TID 11)\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 3\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 4\n",
      "24/12/17 10:10:11 INFO Executor: Running task 8.0 in stage 2.0 (TID 13)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 7.0 in stage 2.0 (TID 12)\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 5\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 6\n",
      "24/12/17 10:10:11 INFO Executor: Running task 10.0 in stage 2.0 (TID 15)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 9.0 in stage 2.0 (TID 14)\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 7\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 8\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Getting local block broadcast_2\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 13.0 in stage 2.0 (TID 18)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 12.0 in stage 2.0 (TID 17)\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 10\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 9\n",
      "24/12/17 10:10:11 INFO Executor: Running task 14.0 in stage 2.0 (TID 19)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 11.0 in stage 2.0 (TID 16)\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 11\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 12\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 13\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 14\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 15\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 42, boot = -53, init = 95, finish = 0\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 4.0 in stage 2.0 (TID 9). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 14\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 9) in 56 ms on hadoop113 (executor driver) (1/15)\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 43, boot = -29, init = 72, finish = 0\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 45, boot = -28, init = 73, finish = 0\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 45, boot = -29, init = 74, finish = 0\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 2.0 in stage 2.0 (TID 7). 1842 bytes result sent to driver\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 13\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 12\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 11\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 7) in 60 ms on hadoop113 (executor driver) (2/15)\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 61 ms on hadoop113 (executor driver) (3/15)\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 61 ms on hadoop113 (executor driver) (4/15)\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 1 ms\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 46, boot = 4, init = 42, finish = 0\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 3.0 in stage 2.0 (TID 8). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 10\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 8) in 66 ms on hadoop113 (executor driver) (5/15)\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 48, boot = 5, init = 43, finish = 0\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 1 ms\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 9.0 in stage 2.0 (TID 14). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 9\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 14) in 70 ms on hadoop113 (executor driver) (6/15)\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 53, boot = 9, init = 44, finish = 0\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 13.0 in stage 2.0 (TID 18). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 54, boot = 12, init = 42, finish = 0\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 8\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 18) in 70 ms on hadoop113 (executor driver) (7/15)\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 12.0 in stage 2.0 (TID 17). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 7\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 17) in 72 ms on hadoop113 (executor driver) (8/15)\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 57, boot = 15, init = 42, finish = 0\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 14.0 in stage 2.0 (TID 19). 1855 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 6\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 19) in 75 ms on hadoop113 (executor driver) (9/15)\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 62, boot = 19, init = 42, finish = 1\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 65, boot = 22, init = 42, finish = 1\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 11.0 in stage 2.0 (TID 16). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 5\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 69, boot = 27, init = 42, finish = 0\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 69, boot = 25, init = 44, finish = 0\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 16) in 83 ms on hadoop113 (executor driver) (10/15)\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 10.0 in stage 2.0 (TID 15). 1847 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 4\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 7.0 in stage 2.0 (TID 12). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 1 ms\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 3\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 5.0 in stage 2.0 (TID 10). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 2\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 71, boot = 28, init = 43, finish = 0\n",
      "24/12/17 10:10:11 INFO PythonRunner: Times: total = 73, boot = 31, init = 42, finish = 0\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 12) in 88 ms on hadoop113 (executor driver) (11/15)\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 10) in 89 ms on hadoop113 (executor driver) (12/15)\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 6.0 in stage 2.0 (TID 11). 1850 bytes result sent to driver\n",
      "24/12/17 10:10:11 INFO Executor: Finished task 8.0 in stage 2.0 (TID 13). 1789 bytes result sent to driver\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 1\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 0\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 1 ms\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 11) in 94 ms on hadoop113 (executor driver) (13/15)\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 15) in 94 ms on hadoop113 (executor driver) (14/15)\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 13) in 94 ms on hadoop113 (executor driver) (15/15)\n",
      "24/12/17 10:10:11 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:11 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.105 s\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: After removal of stage 2, remaining stages = 0\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/17 10:10:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: SparkListenerJobEnd - executionId: 0\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: Event already finished, returning\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: onJobEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.109084 s\n",
      "24/12/17 10:10:11 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:11 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:11 INFO CodeGenerator: Code generated in 10.506899 ms\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionEnd - executionId: 0\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.GlobalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.catalyst.plans.logical.LocalLimit\",\"num-children\":1,\"limitExpr\":[{\"class\":\"org.apache.spark.sql.catalyst.expressions.Literal\",\"num-children\":0,\"value\":\"21\",\"dataType\":\"integer\"}],\"child\":0},{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}]\n",
      "+---+---------+----------+\n",
      "| id|     city|      name|\n",
      "+---+---------+----------+\n",
      "|  2|Telangana|   Chhetri|\n",
      "|  3|   Sikkim|    Bhutia|\n",
      "|  4|Hyderabad|   Shabbir|\n",
      "|  5|   Kerala|   Vijayan|\n",
      "|  6|  Mizoram|Lalpekhlua|\n",
      "+---+---------+----------+\n",
      "\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.CollectLimitExec\",\"num-children\":1,\"limit\":21,\"child\":0},{\"class\":\"org.apache.spark.sql.execution.WholeStageCodegenExec\",\"num-children\":1,\"child\":0,\"codegenStageId\":1},{\"class\":\"org.apache.spark.sql.execution.RDDScanExec\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"name\":\"ExistingRDD\",\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[]}]\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#0,city#1,name#2]\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n"
     ]
    }
   ],
   "source": [
    "data = [('2','Telangana','Chhetri'),\n",
    "  ('3','Sikkim','Bhutia'),\n",
    "  ('4','Hyderabad','Shabbir'),\n",
    "  ('5','Kerala','Vijayan'),\n",
    "  ('6','Mizoram','Lalpekhlua')\n",
    "  ]\n",
    "columns = [\"id\",\"city\",\"name\"]\n",
    "goalsDF = spark.createDataFrame(data=data, schema = columns)\n",
    "goalsDF.show()\n",
    "\n",
    "# hdfs \n",
    "# goalsDF.write.mode(\"overwrite\").parquet(\"hdfs://hadoop:9000/tmp/test\")\n",
    "#spark.read.parquet(\"hdfs://hadoop:9000/tmp/test\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a54d69-0bdd-4cd1-ae31-23bbea063efd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: string, city: string, name: string\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- Project [cast(id#0 as string) AS id#9, cast(city#1 as string) AS city#10, cast(name#2 as string) AS name#11]\n",
      "      +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 21\n",
      "+- LocalLimit 21\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "CollectLimit 21\n",
      "+- *(1) Scan ExistingRDD[id#0,city#1,name#2]\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: Posting event for end 0: {\"eventTime\":\"2024-12-17T03:10:11.477Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:11.477Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:11.477Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@28433274, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:11 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToHbase, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/17 10:10:11 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@28433274, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:11 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, name=WriteToHbase}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.collect_limit), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, created={time=1734405011477}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734405011488}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734405011477, status=COMPLETE}, dataProcessInstanceProperties={name=0193d298-3638-7fe3-9e16-b57e1d5ad8d9, created={actor=urn:li:corpuser:datahub, time=1734405011477}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193d298-3638-7fe3-9e16-b57e1d5ad8d9, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734405011477, eventFormatter=datahub.event.EventFormatter@5c61fe8a), from {\"eventTime\":\"2024-12-17T03:10:11.477Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193d298-3638-7fe3-9e16-b57e1d5ad8d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:11 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/17 10:10:11 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart\n",
      "24/12/17 10:10:11 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/17 10:10:11 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/17 10:10:11 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/17 10:10:11 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionStart - executionId: 1\n",
      "24/12/17 10:10:11 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "24/12/17 10:10:11 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:11 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:11 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:11 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 566.1 KiB, free 365.7 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Put block broadcast_3 locally took 8 ms\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Putting block broadcast_3 without replication took 8 ms\n",
      "24/12/17 10:10:11 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 365.7 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, hadoop113, 34591, None)\n",
      "24/12/17 10:10:11 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop113:34591 (size: 38.4 KiB, free: 366.2 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMaster: Updated info of block broadcast_3_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Told master about block broadcast_3_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Put block broadcast_3_piece0 locally took 2 ms\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Putting block broadcast_3_piece0 without replication took 2 ms\n",
      "24/12/17 10:10:11 INFO SparkContext: Created broadcast 3 from broadcast at HBaseContext.scala:71\n",
      "24/12/17 10:10:11 INFO HBaseRelation: newtable\n",
      "is not defined or no larger than 3, skip the create table\n",
      "24/12/17 10:10:11 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator rdd_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     rdd_input_0 = inputs[0];\n",
      "/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( rdd_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();\n",
      "/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 028 */       boolean rdd_isNull_0 = rdd_row_0.isNullAt(0);\n",
      "/* 029 */       UTF8String rdd_value_0 = rdd_isNull_0 ?\n",
      "/* 030 */       null : (rdd_row_0.getUTF8String(0));\n",
      "/* 031 */       boolean rdd_isNull_1 = rdd_row_0.isNullAt(1);\n",
      "/* 032 */       UTF8String rdd_value_1 = rdd_isNull_1 ?\n",
      "/* 033 */       null : (rdd_row_0.getUTF8String(1));\n",
      "/* 034 */       boolean rdd_isNull_2 = rdd_row_0.isNullAt(2);\n",
      "/* 035 */       UTF8String rdd_value_2 = rdd_isNull_2 ?\n",
      "/* 036 */       null : (rdd_row_0.getUTF8String(2));\n",
      "/* 037 */       rdd_mutableStateArray_0[0].reset();\n",
      "/* 038 */\n",
      "/* 039 */       rdd_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 040 */\n",
      "/* 041 */       if (rdd_isNull_0) {\n",
      "/* 042 */         rdd_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 043 */       } else {\n",
      "/* 044 */         rdd_mutableStateArray_0[0].write(0, rdd_value_0);\n",
      "/* 045 */       }\n",
      "/* 046 */\n",
      "/* 047 */       if (rdd_isNull_1) {\n",
      "/* 048 */         rdd_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */       } else {\n",
      "/* 050 */         rdd_mutableStateArray_0[0].write(1, rdd_value_1);\n",
      "/* 051 */       }\n",
      "/* 052 */\n",
      "/* 053 */       if (rdd_isNull_2) {\n",
      "/* 054 */         rdd_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 055 */       } else {\n",
      "/* 056 */         rdd_mutableStateArray_0[0].write(2, rdd_value_2);\n",
      "/* 057 */       }\n",
      "/* 058 */       append((rdd_mutableStateArray_0[0].getRow()));\n",
      "/* 059 */       if (shouldStop()) return;\n",
      "/* 060 */     }\n",
      "/* 061 */   }\n",
      "/* 062 */\n",
      "/* 063 */ }\n",
      "\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$rdd$1\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$insert$11\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$insert$11) is now cleaned +++\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(24)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 24\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 24\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(4)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 4\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 4\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(56)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 56\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 56\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(78)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 78\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 78\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(32)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 32\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 32\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(84)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 84\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 84\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(86)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 86\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 86\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(30)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 30\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 30\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(45)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 45\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 45\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(67)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 67\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 67\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(31)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 31\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 31\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(63)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 63\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 63\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(71)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 71\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 71\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(12)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 12\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 12\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(77)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 77\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 77\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(21)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 21\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 21\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(29)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 29\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 29\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(38)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 38\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 38\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(51)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 51\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 51\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(54)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 54\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 54\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(14)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 14\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 14\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(35)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 35\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 35\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(1)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning broadcast 1\n",
      "24/12/17 10:10:11 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 1\n",
      "24/12/17 10:10:11 DEBUG HadoopMapRedWriteConfigUtil: Saving as hadoop file of type (LongWritable, Text)\n",
      "24/12/17 10:10:11 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "24/12/17 10:10:11 DEBUG FileCommitProtocol: Creating committer org.apache.spark.internal.io.HadoopMapRedCommitProtocol; job 11; output=null; dynamic=false\n",
      "24/12/17 10:10:11 DEBUG BlockManagerStorageEndpoint: removing broadcast 1\n",
      "24/12/17 10:10:11 DEBUG FileCommitProtocol: Falling back to (String, String) constructor\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Removing broadcast 1\n",
      "24/12/17 10:10:11 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Removing block broadcast_1\n",
      "24/12/17 10:10:11 DEBUG MemoryStore: Block broadcast_1 of size 12440 dropped from memory (free 383429872)\n",
      "24/12/17 10:10:11 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7f4e7981}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context JobContextImpl{jobId=job_202412171010114913797296662819386_0011}\n",
      "24/12/17 10:10:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:11 WARN FileOutputCommitter: Output Path is null in setupJob()\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Removing block broadcast_1_piece0\n",
      "24/12/17 10:10:11 DEBUG MemoryStore: Block broadcast_1_piece0 of size 6554 dropped from memory (free 383436426)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, hadoop113, 34591, None)\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$write$1\n",
      "24/12/17 10:10:11 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$write$1) is now cleaned +++\n",
      "24/12/17 10:10:11 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hadoop113:34591 in memory (size: 6.4 KiB, free: 366.2 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Told master about block broadcast_1_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 1, response is 0\n",
      "24/12/17 10:10:11 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to hadoop113:36133\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned broadcast 1\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(39)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 39\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 39\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(53)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 53\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 53\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(69)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 69\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 69\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(22)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 22\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 22\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(19)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 19\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 19\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(26)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 26\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 26\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(73)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 73\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 73\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(87)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 87\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 87\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(66)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 66\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 66\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(74)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 74\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 74\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(75)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 75\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 75\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(57)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 57\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 57\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(5)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 5\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 5\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(8)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 8\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 8\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(82)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 82\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 82\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(34)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 34\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 34\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(46)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 46\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 46\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(20)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 20\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 20\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(2)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning broadcast 2\n",
      "24/12/17 10:10:11 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 2\n",
      "24/12/17 10:10:11 DEBUG BlockManagerStorageEndpoint: removing broadcast 2\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Removing broadcast 2\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Removing block broadcast_2_piece0\n",
      "24/12/17 10:10:11 DEBUG MemoryStore: Block broadcast_2_piece0 of size 6555 dropped from memory (free 383442981)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, hadoop113, 34591, None)\n",
      "24/12/17 10:10:11 INFO BlockManagerInfo: Removed broadcast_2_piece0 on hadoop113:34591 in memory (size: 6.4 KiB, free: 366.3 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMaster: Updated info of block broadcast_2_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Told master about block broadcast_2_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Removing block broadcast_2\n",
      "24/12/17 10:10:11 DEBUG MemoryStore: Block broadcast_2 of size 12440 dropped from memory (free 383455421)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 2, response is 0\n",
      "24/12/17 10:10:11 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to hadoop113:36133\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned broadcast 2\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(11)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 11\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 11\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(49)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 49\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 49\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(28)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 28\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 28\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(52)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 52\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 52\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(13)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 13\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 13\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(2)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 2\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 2\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(36)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 36\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 36\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(80)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 80\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 80\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(40)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 40\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 40\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(83)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 83\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 83\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(10)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 10\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 10\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(76)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 76\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 76\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(60)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 60\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 60\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(25)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 25\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 25\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(50)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 50\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 50\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(17)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 17\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 17\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(65)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 65\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 65\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(42)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 42\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 42\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(43)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 43\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 43\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(70)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 70\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 70\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(3)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 3\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 3\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(47)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 47\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 47\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(55)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 55\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 55\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(6)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 6\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 6\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(16)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 16\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 16\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(33)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 33\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 33\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(27)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 27\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 27\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(79)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 79\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 79\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(23)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 23\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 23\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(81)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 81\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 81\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(41)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 41\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 41\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(7)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 7\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 7\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(72)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 72\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 72\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(62)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 62\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 62\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(1)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 1\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 1\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(48)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 48\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 48\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(59)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 59\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 59\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(37)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 37\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 37\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(61)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 61\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 61\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(9)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 9\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 9\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(15)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 15\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 15\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(68)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 68\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 68\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(58)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 58\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 58\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanBroadcast(0)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning broadcast 0\n",
      "24/12/17 10:10:11 DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast 0\n",
      "24/12/17 10:10:11 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
      "24/12/17 10:10:11 DEBUG BlockManagerStorageEndpoint: removing broadcast 0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Removing broadcast 0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Removing block broadcast_0_piece0\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 11 took 0.000094 seconds\n",
      "24/12/17 10:10:11 DEBUG MemoryStore: Block broadcast_0_piece0 of size 6548 dropped from memory (free 383461969)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, hadoop113, 34591, None)\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/17 10:10:11 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hadoop113:34591 in memory (size: 6.4 KiB, free: 366.3 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Told master about block broadcast_0_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Removing block broadcast_0\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Got job 3 (runJob at SparkHadoopWriter.scala:83) with 20 output partitions\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:83)\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/17 10:10:11 DEBUG MemoryStore: Block broadcast_0 of size 12440 dropped from memory (free 383474409)\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: submitStage(ResultStage 3 (name=runJob at SparkHadoopWriter.scala:83;jobs=3))\n",
      "24/12/17 10:10:11 DEBUG BlockManagerStorageEndpoint: Done removing broadcast 0, response is 0\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: missing: List()\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at map at DefaultSource.scala:277), which has no missing parents\n",
      "24/12/17 10:10:11 DEBUG DAGScheduler: submitMissingTasks(ResultStage 3)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerStorageEndpoint: Sent response: 0 to hadoop113:36133\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned broadcast 0\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(64)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 64\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 64\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(18)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 18\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 18\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(44)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 44\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 44\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Got cleaning task CleanAccum(85)\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaning accumulator 85\n",
      "24/12/17 10:10:11 DEBUG ContextCleaner: Cleaned accumulator 85\n",
      "24/12/17 10:10:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 256.1 KiB, free 365.5 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Put block broadcast_4 locally took 1 ms\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Putting block broadcast_4 without replication took 1 ms\n",
      "24/12/17 10:10:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 90.6 KiB, free 365.4 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, hadoop113, 34591, None)\n",
      "24/12/17 10:10:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop113:34591 (size: 90.6 KiB, free: 366.2 MiB)\n",
      "24/12/17 10:10:11 DEBUG BlockManagerMaster: Updated info of block broadcast_4_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Told master about block broadcast_4_piece0\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Put block broadcast_4_piece0 locally took 1 ms\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Putting block broadcast_4_piece0 without replication took 1 ms\n",
      "24/12/17 10:10:11 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/17 10:10:11 INFO DAGScheduler: Submitting 20 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at map at DefaultSource.scala:277) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "24/12/17 10:10:11 INFO TaskSchedulerImpl: Adding task set 3.0 with 20 tasks resource profile 0\n",
      "24/12/17 10:10:11 DEBUG TaskSetManager: Epoch for TaskSet 3.0: 0\n",
      "24/12/17 10:10:11 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "24/12/17 10:10:11 DEBUG TaskSetManager: Valid locality levels for TaskSet 3.0: NO_PREF, ANY\n",
      "24/12/17 10:10:11 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_3.0, runningTasks: 0\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 20) (hadoop113, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 21) (hadoop113, executor driver, partition 1, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 22) (hadoop113, executor driver, partition 2, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 23) (hadoop113, executor driver, partition 3, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 24) (hadoop113, executor driver, partition 4, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 25) (hadoop113, executor driver, partition 5, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 26) (hadoop113, executor driver, partition 6, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 27) (hadoop113, executor driver, partition 7, PROCESS_LOCAL, 4482 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 28) (hadoop113, executor driver, partition 8, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 29) (hadoop113, executor driver, partition 9, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 30) (hadoop113, executor driver, partition 10, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 31) (hadoop113, executor driver, partition 11, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 32) (hadoop113, executor driver, partition 12, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 33) (hadoop113, executor driver, partition 13, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 34) (hadoop113, executor driver, partition 14, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 35) (hadoop113, executor driver, partition 15, PROCESS_LOCAL, 4483 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 36) (hadoop113, executor driver, partition 16, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 37) (hadoop113, executor driver, partition 17, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 38) (hadoop113, executor driver, partition 18, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO TaskSetManager: Starting task 19.0 in stage 3.0 (TID 39) (hadoop113, executor driver, partition 19, PROCESS_LOCAL, 4487 bytes) taskResourceAssignments Map()\n",
      "24/12/17 10:10:11 INFO Executor: Running task 0.0 in stage 3.0 (TID 20)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 1.0 in stage 3.0 (TID 21)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 2.0 in stage 3.0 (TID 22)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 3.0 in stage 3.0 (TID 23)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 5.0 in stage 3.0 (TID 25)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 4.0 in stage 3.0 (TID 24)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 6.0 in stage 3.0 (TID 26)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 7.0 in stage 3.0 (TID 27)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 9.0 in stage 3.0 (TID 29)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 8.0 in stage 3.0 (TID 28)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 11.0 in stage 3.0 (TID 31)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 14.0 in stage 3.0 (TID 34)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 13.0 in stage 3.0 (TID 33)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 10.0 in stage 3.0 (TID 30)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 12.0 in stage 3.0 (TID 32)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 15.0 in stage 3.0 (TID 35)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 16.0 in stage 3.0 (TID 36)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 17.0 in stage 3.0 (TID 37)\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 1\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 2\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 3\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 5\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 4\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 6\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 7\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 8\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 9\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 10\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 11\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 12\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 13\n",
      "24/12/17 10:10:11 INFO Executor: Running task 18.0 in stage 3.0 (TID 38)\n",
      "24/12/17 10:10:11 INFO Executor: Running task 19.0 in stage 3.0 (TID 39)\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 14\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 15\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Getting local block broadcast_4\n",
      "24/12/17 10:10:11 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 16\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 17\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 18\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 19\n",
      "24/12/17 10:10:11 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 20\n",
      "24/12/17 10:10:11 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:11 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:11 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000001_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5bdc5449}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000001_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000018_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7c5ed1dd}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000018_0, status=''}\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000015_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4984e910}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000015_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000011_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5208a419}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000011_0, status=''}\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000010_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@45d73cce}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000010_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000005_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3e7689a0}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000005_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000017_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@603312b5}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000017_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000004_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@18bc5c0e}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000004_0, status=''}\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000012_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4470ddcc}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000012_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000019_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5a2ebecf}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000019_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000007_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@5994260}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000007_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000002_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@16296710}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000002_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000009_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@3473a1a7}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000009_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000008_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2dd5637f}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000008_0, status=''}\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000014_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@495bda6a}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000014_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000003_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2c6dc21d}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000003_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000013_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4707132b}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000013_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000006_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@79ec528f}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000006_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000016_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@4ed0c756}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000016_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, StructField(id,StringType,true), StructField(city,StringType,true), StructField(name,StringType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     boolean isNull_2 = i.isNullAt(0);\n",
      "/* 039 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 040 */     null : (i.getUTF8String(0));\n",
      "/* 041 */     boolean isNull_1 = true;\n",
      "/* 042 */     java.lang.String value_1 = null;\n",
      "/* 043 */     if (!isNull_2) {\n",
      "/* 044 */       isNull_1 = false;\n",
      "/* 045 */       if (!isNull_1) {\n",
      "/* 046 */\n",
      "/* 047 */         Object funcResult_0 = null;\n",
      "/* 048 */         funcResult_0 = value_2.toString();\n",
      "/* 049 */         value_1 = (java.lang.String) funcResult_0;\n",
      "/* 050 */\n",
      "/* 051 */       }\n",
      "/* 052 */     }\n",
      "/* 053 */     if (isNull_1) {\n",
      "/* 054 */       values_0[0] = null;\n",
      "/* 055 */     } else {\n",
      "/* 056 */       values_0[0] = value_1;\n",
      "/* 057 */     }\n",
      "/* 058 */\n",
      "/* 059 */     boolean isNull_4 = i.isNullAt(1);\n",
      "/* 060 */     UTF8String value_4 = isNull_4 ?\n",
      "/* 061 */     null : (i.getUTF8String(1));\n",
      "/* 062 */     boolean isNull_3 = true;\n",
      "/* 063 */     java.lang.String value_3 = null;\n",
      "/* 064 */     if (!isNull_4) {\n",
      "/* 065 */       isNull_3 = false;\n",
      "/* 066 */       if (!isNull_3) {\n",
      "/* 067 */\n",
      "/* 068 */         Object funcResult_1 = null;\n",
      "/* 069 */         funcResult_1 = value_4.toString();\n",
      "/* 070 */         value_3 = (java.lang.String) funcResult_1;\n",
      "/* 071 */\n",
      "/* 072 */       }\n",
      "/* 073 */     }\n",
      "/* 074 */     if (isNull_3) {\n",
      "/* 075 */       values_0[1] = null;\n",
      "/* 076 */     } else {\n",
      "/* 077 */       values_0[1] = value_3;\n",
      "/* 078 */     }\n",
      "/* 079 */\n",
      "/* 080 */     boolean isNull_6 = i.isNullAt(2);\n",
      "/* 081 */     UTF8String value_6 = isNull_6 ?\n",
      "/* 082 */     null : (i.getUTF8String(2));\n",
      "/* 083 */     boolean isNull_5 = true;\n",
      "/* 084 */     java.lang.String value_5 = null;\n",
      "/* 085 */     if (!isNull_6) {\n",
      "/* 086 */       isNull_5 = false;\n",
      "/* 087 */       if (!isNull_5) {\n",
      "/* 088 */\n",
      "/* 089 */         Object funcResult_2 = null;\n",
      "/* 090 */         funcResult_2 = value_6.toString();\n",
      "/* 091 */         value_5 = (java.lang.String) funcResult_2;\n",
      "/* 092 */\n",
      "/* 093 */       }\n",
      "/* 094 */     }\n",
      "/* 095 */     if (isNull_5) {\n",
      "/* 096 */       values_0[2] = null;\n",
      "/* 097 */     } else {\n",
      "/* 098 */       values_0[2] = value_5;\n",
      "/* 099 */     }\n",
      "/* 100 */\n",
      "/* 101 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 102 */\n",
      "/* 103 */     return value_0;\n",
      "/* 104 */   }\n",
      "/* 105 */\n",
      "/* 106 */ }\n",
      "\n",
      "24/12/17 10:10:12 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/17 10:10:12 DEBUG PathOutputCommitter: Instantiating committer FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7bdc7973}; outputPath=null, workPath=null, algorithmVersion=0, skipCleanup=false, ignoreCleanupFailures=false} with output path null and job context TaskAttemptContextImpl{JobContextImpl{jobId=job_202412171010114913797296662819386_0011}; taskId=attempt_202412171010114913797296662819386_0011_m_000000_0, status=''}\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/17 10:10:12 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@3507265f]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@1f7d44be]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@5fdf2545]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@6ee5766d]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@302fc4bb]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@3253d8e0]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@13000778]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@ad2c135]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@8f65b0f]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@5f4afb6b]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@45992f47]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@3ca619d3]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@571a351d]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@1d3e10ed]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@4626e988]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@438a0dcf]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@68144eb2]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@233e4f4a]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@c096ba3]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG UserGroupInformation: PrivilegedAction [as: root (auth:SIMPLE)][action: org.apache.hadoop.hbase.client.ConnectionFactory$$Lambda$2952/368180307@340ea260]\n",
      "java.lang.Exception\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1875)\n",
      "\tat org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:325)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:230)\n",
      "\tat org.apache.hadoop.hbase.client.ConnectionFactory.createConnection(ConnectionFactory.java:130)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat$TableRecordWriter.<init>(TableOutputFormat.java:74)\n",
      "\tat org.apache.hadoop.hbase.mapred.TableOutputFormat.getRecordWriter(TableOutputFormat.java:116)\n",
      "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.initWriter(SparkHadoopWriter.scala:238)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.executeTask(SparkHadoopWriter.scala:126)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$1(SparkHadoopWriter.scala:88)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x5399227c to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x4aaac5ce to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x49ba0100 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x32e6bc39 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x75b9d609 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x061239b6 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x71c34912 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x2dd35eac to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x1beeab32 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x7c9d1e4b to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x571c85fb to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x04b5a71e to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x11e1fa00 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x3a1d23d1 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x6e32f673 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x39d274c5 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x3d17d912 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x351b12d5 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x7f620b43 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Connect 0x75501885 to 127.0.0.1:2181 with session timeout=90000ms, retries 30, retry interval 1000ms, keepAlive=60000ms\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:zookeeper.version=3.5.7-f0fdd52973d373ffd9c86b81d99842dc2c7f660e, built on 02/10/2020 11:30 GMT\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:host.name=hadoop113\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:java.version=1.8.0_432\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:java.vendor=Private Build\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:java.class.path=/usr/local/lib/python3.8/dist-packages/pyspark/conf:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-rbac-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-sql_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/zookeeper-jute-3.6.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-compress-1.21.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-xml_2.12-1.2.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-beeline-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-exec-2.3.9-core.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/joda-time-2.10.13.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-batch-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-dbcp-1.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-reflect-2.12.15.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-kubernetes_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/xbean-asm9-shaded-4.20.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arrow-memory-core-7.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hk2-utils-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/lz4-java-1.8.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/log4j-core-2.17.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/JTransforms-3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/curator-client-2.13.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-shaded-guava-1.1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/log4j-1.2-api-2.17.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-policy-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/flatbuffers-java-1.12.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-hive_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/HikariCP-2.5.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-jackson-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hk2-api-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/shims-0.9.25.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-extensions-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/antlr4-runtime-4.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/breeze-macros_2.12-1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-lang3-3.12.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/shapeless_2.12-2.3.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/datanucleus-api-jdo-4.2.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-launcher_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-text-1.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/generex-1.0.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-container-servlet-core-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-apiextensions-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-cli-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/javax.jdo-3.2.0-m3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jdo-api-3.0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jpam-1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/stax-api-1.0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-metrics-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/javolution-5.5.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/protobuf-java-2.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-autoscaling-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-databind-2.13.4.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arrow-memory-netty-7.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-networking-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-resolver-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-catalyst_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-compiler-2.12.15.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-serde-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/istack-commons-runtime-3.0.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/libfb303-0.9.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/avro-ipc-1.11.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-client-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-jmx-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-codec-1.15.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-core-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jline-2.14.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-collection-compat_2.12-2.1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.validation-api-2.0.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/core-1.1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/slf4j-api-1.7.32.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-logging-1.1.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/javassist-3.25.0-GA.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-events-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/zstd-jni-1.5.2-1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-hadoop-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/snakeyaml-1.31.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spire-macros_2.12-0.17.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-all-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-scheduling-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/zookeeper-3.6.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-cli-1.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/threeten-extra-1.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/snappy-java-1.1.8.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-tcnative-classes-2.0.48.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-network-common_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/curator-recipes-2.13.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-lang-2.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-parser-combinators_2.12-1.1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arpack_combined_all-0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/py4j-0.10.9.5.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-discovery-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jsr305-3.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-service-rpc-3.1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-datatype-jsr310-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-graphx_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spire_2.12-0.17.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/okhttp-3.12.12.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/bonecp-0.8.0.RELEASE.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jcl-over-slf4j-1.7.32.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/orc-shims-1.7.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/transaction-api-1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spire-platform_2.12-0.17.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/guava-14.0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/chill-java-0.10.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/minlog-1.3.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-buffer-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-pool-1.5.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/opencsv-2.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/orc-mapreduce-1.7.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/aopalliance-repackaged-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arrow-vector-7.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-common-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-library-2.12.15.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/univocity-parsers-2.9.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-unsafe_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-handler-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/chill_2.12-0.10.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-node-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-container-servlet-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-common-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json4s-jackson_2.12-3.7.0-M11.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arpack-2.2.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/JLargeArrays-1.5.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-math3-3.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-core-asl-1.9.13.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/datanucleus-core-4.1.17.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.inject-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/janino-3.0.16.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-api-3.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-server-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arrow-format-7.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-compiler-3.0.16.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-mllib-local_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/xz-1.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-dataformat-yaml-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.servlet-api-4.0.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json4s-ast_2.12-3.7.0-M11.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-collections-3.2.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-classes-kqueue-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/pickle-1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jul-to-slf4j-1.7.32.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/avro-1.11.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/RoaringBitmap-0.9.25.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jaxb-runtime-2.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/super-csv-2.2.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/httpcore-4.4.14.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-yarn_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-encoding-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json4s-core_2.12-3.7.0-M11.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/lapack-2.2.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.annotation-api-1.3.5.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-flowcontrol-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/orc-core-1.7.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-unix-common-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-module-scala_2.12-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/compress-lzf-1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/objenesis-3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-core-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/httpclient-4.5.13.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-yarn-server-web-proxy-3.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/log4j-api-2.17.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-shims-scheduler-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/automaton-1.11-8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/curator-framework-2.13.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ST4-4.0.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-kvstore_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-certificates-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-client-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/avro-mapred-1.11.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kryo-shaded-4.0.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-core-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/zjsonpatch-0.3.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jodd-core-3.5.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-crypto-1.1.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-sketch_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-shims-common-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/velocity-1.5.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-annotations-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-coordination-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-storage-api-2.7.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-format-structures-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-vector-code-gen-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/oro-2.0.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-metastore-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-common-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jta-1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-graphite-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-admissionregistration-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/osgi-resource-locator-1.0.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-collections4-4.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-io-2.11.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/derby-10.14.2.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spire-util_2.12-0.17.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/blas-2.2.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-shims-0.23-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-json-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/paranamer-2.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/aircompressor-0.21.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-mesos_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-column-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-codec-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/antlr-runtime-3.5.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-storageclass-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-jvm-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-network-shuffle_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/leveldbjni-all-1.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/stream-2.9.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/cats-kernel_2.12-2.1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/gson-2.2.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-repl_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/breeze_2.12-1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-shims-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/annotations-17.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/mesos-1.4.3-shaded-protobuf.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.ws.rs-api-2.1.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-mllib_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/okio-1.14.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/tink-1.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-classes-epoll-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-hive-thriftserver_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-apps-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-hk2-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-mapper-asl-1.9.13.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/algebra_2.12-2.0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/log4j-slf4j-impl-2.17.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-llap-common-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json-1.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/audience-annotations-0.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-jdbc-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.xml.bind-api-2.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/activation-1.1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-common-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/libthrift-0.12.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-core_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/datanucleus-rdbms-4.1.19.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/rocksdbjni-6.20.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/logging-interceptor-3.12.12.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json4s-scalap_2.12-3.7.0-M11.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-tags_2.12-3.3.1-tests.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-streaming_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hk2-locator-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-common-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-tags_2.12-3.3.1.jar\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:java.compiler=<NA>\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:os.name=Linux\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:os.arch=amd64\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:os.version=6.8.0-50-generic\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:user.name=root\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:user.home=/root\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:user.dir=/\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:os.memory.free=826MB\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:os.memory.max=957MB\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Client environment:os.memory.total=957MB\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3016/1911549447@1fe61be6\n",
      "24/12/17 10:10:12 INFO X509Util: Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42456, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42428, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42458, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42426, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42454, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42430, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301c5, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301c3, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301c7, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301c2, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301c4, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301c6, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 DEBUG SaslServerPrincipal: Canonicalized address to localhost\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42488, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42476, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42474, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42492, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42494, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42640, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42482, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42486, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42478, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42630, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42490, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42644, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42484, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:42480, server: localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Session establishment request sent on localhost/127.0.0.1:2181\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301c8, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301c9, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301ca, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301cb, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301cc, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301cd, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301ce, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301cf, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301d0, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301d2, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301d1, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301d4, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301d3, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10004ff096301d5, negotiated timeout = 90000\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301ca, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d4, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d5, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c8, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c6, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301cc, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301cb, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301cf, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d2, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c3, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c5, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c2, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d0, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d3, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c9, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d1, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c4, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c7, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301ce, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301cd, packet:: clientPath:/hbase/hbaseid serverPath:/hbase/hbaseid finished:false header:: 1,4  replyHeader:: 1,1590,0  request:: '/hbase/hbaseid,F  response:: #ffffffff000146d61737465723a313630303014ffffff9efffffff5ffffffe41762ffffff832350425546a2431333739383935312d343330652d346166642d393664662d316137633335633266346330,s{13,632,1733275814277,1734314929563,1,0,0,0,67,0,13} \n",
      "24/12/17 10:10:12 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework\n",
      "24/12/17 10:10:12 DEBUG ResourceLeakDetector: -Dorg.apache.hbase.thirdparty.io.netty.leakDetection.level: simple\n",
      "24/12/17 10:10:12 DEBUG ResourceLeakDetector: -Dorg.apache.hbase.thirdparty.io.netty.leakDetection.targetRecords: 4\n",
      "24/12/17 10:10:12 DEBUG ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector@257abad4\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent0: -Dio.netty.noUnsafe: false\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent0: Java version: 8\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent0: sun.misc.Unsafe.theUnsafe: available\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent0: sun.misc.Unsafe.copyMemory: available\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent0: sun.misc.Unsafe.storeFence: available\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent0: java.nio.Buffer.address: available\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent0: direct buffer constructor: available\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent0: java.nio.Bits.unaligned: available, true\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent0: jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable prior to Java9\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent0: java.nio.DirectByteBuffer.<init>(long, int): available\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent: sun.misc.Unsafe: available\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent: -Dio.netty.tmpdir: /tmp (java.io.tmpdir)\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent: -Dio.netty.bitMode: 64 (sun.arch.data.model)\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent: -Dio.netty.maxDirectMemory: 954728448 bytes\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent: -Dio.netty.uninitializedArrayAllocationThreshold: -1\n",
      "24/12/17 10:10:12 DEBUG CleanerJava6: java.nio.ByteBuffer.cleaner(): available\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent: -Dio.netty.noPreferDirect: false\n",
      "24/12/17 10:10:12 DEBUG PlatformDependent: org.jctools-core.MpscChunkedArrayQueue: available\n",
      "24/12/17 10:10:12 DEBUG ClassSize: Using Unsafe to estimate memory layout\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@3b792617, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@4bb716ae, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@9fdcd03, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@e50e7b2, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@48e11519, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@c7c1d4d, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@66b3d8b, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@3ae98525, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@298c3db8, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@6f1d62d2, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@4a9f3afb, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@441f404b, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@2a5b44f1, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@49d5da1a, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@2e2d37a6, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@62f51cc5, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@5e9eefd6, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@285a1253, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@581aef0e, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Codec=org.apache.hadoop.hbase.codec.KeyValueCodec@69304a7a, compressor=null, tcpKeepAlive=true, tcpNoDelay=true, connectTO=10000, readTO=20000, writeTO=60000, minIdleTimeBeforeClose=120000, maxRetries=0, fallbackAllowed=false, bind address=null\n",
      "24/12/17 10:10:12 DEBUG MultithreadEventLoopGroup: -Dio.netty.eventLoopThreads: 40\n",
      "24/12/17 10:10:12 DEBUG InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024\n",
      "24/12/17 10:10:12 DEBUG InternalThreadLocalMap: -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096\n",
      "24/12/17 10:10:12 DEBUG NioEventLoop: -Dio.netty.noKeySetOptimization: false\n",
      "24/12/17 10:10:12 DEBUG NioEventLoop: -Dio.netty.selectorAutoRebuildThreshold: 512\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 44, boot = -536, init = 580, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 81, boot = -521, init = 602, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 43, boot = -537, init = 580, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 76, boot = -511, init = 587, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 74, boot = 7, init = 67, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 73, boot = -538, init = 611, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 111, boot = -509, init = 620, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 63, boot = 13, init = 50, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 64, boot = 20, init = 44, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 75, boot = -535, init = 610, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 43, boot = -535, init = 578, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 42, boot = -539, init = 581, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 83, boot = 18, init = 64, finish = 1\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 49, boot = -531, init = 580, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 78, boot = -514, init = 592, finish = 0\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x3a1d23d1 to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x7c9d1e4b to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x04b5a71e to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x1beeab32 to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x6e32f673 to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x11e1fa00 to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x7f620b43 to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301c3\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301d0\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x75501885 to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301c8\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x4aaac5ce to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301d5\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301c8\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x32e6bc39 to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x3d17d912 to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x061239b6 to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x71c34912 to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301c5\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301ca\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x49ba0100 to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301c5\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301ca\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301ce\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301ce\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301cb\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301cd\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301d5\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301cd\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301d0\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301c6\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301c6\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301c7\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301c7\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301c3\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301d4\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301d4\n",
      "24/12/17 10:10:12 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x351b12d5 to 127.0.0.1:2181\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301c9\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301d2\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301c9\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301d2\n",
      "24/12/17 10:10:12 DEBUG ZooKeeper: Closing session: 0x10004ff096301cf\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301cf\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301cb\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000000_0\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000016_0\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301ce, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1591,0  request:: null response:: null\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000005_0\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000010_0\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301ce : Unable to read additional data from server sessionid 0x10004ff096301ce, likely server has closed socket\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000006_0\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000002_0\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301ce\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000013_0\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000008_0\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000001_0\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c5, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1592,0  request:: null response:: null\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301cd, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1593,0  request:: null response:: null\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301cd\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000018_0\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301cd : Unable to read additional data from server sessionid 0x10004ff096301cd, likely server has closed socket\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000012_0\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301c5\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000014_0\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000017_0\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000004_0\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301c5 : Unable to read additional data from server sessionid 0x10004ff096301c5, likely server has closed socket\n",
      "24/12/17 10:10:12 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000009_0\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 5.0 in stage 3.0 (TID 25). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 8.0 in stage 3.0 (TID 28). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 18.0 in stage 3.0 (TID 38). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 6.0 in stage 3.0 (TID 26). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 12.0 in stage 3.0 (TID 32). 1895 bytes result sent to driver\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 2.0 in stage 3.0 (TID 22). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 19\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 18\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 16.0 in stage 3.0 (TID 36). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 17\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 16\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 15\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c7, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1598,0  request:: null response:: null\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d5, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1594,0  request:: null response:: null\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d4, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1597,0  request:: null response:: null\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301d4\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 1.0 in stage 3.0 (TID 21). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301c7\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 13.0 in stage 3.0 (TID 33). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 10.0 in stage 3.0 (TID 30). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c6, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1596,0  request:: null response:: null\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 14\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301c6\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 13\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d0, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1600,0  request:: null response:: null\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d2, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1601,0  request:: null response:: null\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c8, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1595,0  request:: null response:: null\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301d0\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301ca, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1603,0  request:: null response:: null\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 9.0 in stage 3.0 (TID 29). 1895 bytes result sent to driver\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301d2 : Unable to read additional data from server sessionid 0x10004ff096301d2, likely server has closed socket\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301d0 : Unable to read additional data from server sessionid 0x10004ff096301d0, likely server has closed socket\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301cb, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1605,0  request:: null response:: null\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c9, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1602,0  request:: null response:: null\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c3, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1599,0  request:: null response:: null\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301d2\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301cb\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301c9 : Unable to read additional data from server sessionid 0x10004ff096301c9, likely server has closed socket\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301ca\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301c3 : Unable to read additional data from server sessionid 0x10004ff096301c3, likely server has closed socket\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301c9\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301d5\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 14.0 in stage 3.0 (TID 34). 1895 bytes result sent to driver\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 12\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 4.0 in stage 3.0 (TID 24). 1895 bytes result sent to driver\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301c3\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 11\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 17.0 in stage 3.0 (TID 37). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301cf, packet:: clientPath:null serverPath:null finished:false header:: 2,-11  replyHeader:: 2,1604,0  request:: null response:: null\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301d5 : Unable to read additional data from server sessionid 0x10004ff096301d5, likely server has closed socket\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301cb : Unable to read additional data from server sessionid 0x10004ff096301cb, likely server has closed socket\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301cf\n",
      "24/12/17 10:10:12 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_3.0, runningTasks: 19\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301cf : Unable to read additional data from server sessionid 0x10004ff096301cf, likely server has closed socket\n",
      "24/12/17 10:10:12 INFO Executor: Finished task 0.0 in stage 3.0 (TID 20). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301c8\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 10\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301c8 : Unable to read additional data from server sessionid 0x10004ff096301c8, likely server has closed socket\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 9\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 8\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 7\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 6\n",
      "24/12/17 10:10:12 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 5\n",
      "24/12/17 10:10:12 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 25) in 526 ms on hadoop113 (executor driver) (1/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 26) in 527 ms on hadoop113 (executor driver) (2/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 22) in 528 ms on hadoop113 (executor driver) (3/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 32) in 527 ms on hadoop113 (executor driver) (4/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 38) in 527 ms on hadoop113 (executor driver) (5/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 36) in 527 ms on hadoop113 (executor driver) (6/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 21) in 531 ms on hadoop113 (executor driver) (7/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 28) in 531 ms on hadoop113 (executor driver) (8/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 33) in 531 ms on hadoop113 (executor driver) (9/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 30) in 531 ms on hadoop113 (executor driver) (10/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 34) in 531 ms on hadoop113 (executor driver) (11/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 24) in 532 ms on hadoop113 (executor driver) (12/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 37) in 532 ms on hadoop113 (executor driver) (13/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 29) in 533 ms on hadoop113 (executor driver) (14/20)\n",
      "24/12/17 10:10:12 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 20) in 535 ms on hadoop113 (executor driver) (15/20)\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 47, boot = -539, init = 586, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 76, boot = 13, init = 62, finish = 1\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 48, boot = -525, init = 573, finish = 0\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 43, boot = -509, init = 551, finish = 1\n",
      "24/12/17 10:10:12 INFO PythonRunner: Times: total = 78, boot = -527, init = 605, finish = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==========================================>              (15 + 5) / 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/17 10:10:12 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301cd\n",
      "24/12/17 10:10:12 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301ce\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Session: 0x10004ff096301cd closed\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d1, packet:: clientPath:/hbase serverPath:/hbase finished:false header:: 2,8  replyHeader:: 2,1605,0  request:: '/hbase,F  response:: v{'meta-region-server,'rs,'splitWAL,'backup-masters,'flush-table-proc,'master-maintenance,'online-snapshot,'switch,'master,'running,'draining,'namespace,'hbaseid,'table} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c2, packet:: clientPath:/hbase serverPath:/hbase finished:false header:: 2,8  replyHeader:: 2,1605,0  request:: '/hbase,F  response:: v{'meta-region-server,'rs,'splitWAL,'backup-masters,'flush-table-proc,'master-maintenance,'online-snapshot,'switch,'master,'running,'draining,'namespace,'hbaseid,'table} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301cc, packet:: clientPath:/hbase serverPath:/hbase finished:false header:: 2,8  replyHeader:: 2,1605,0  request:: '/hbase,F  response:: v{'meta-region-server,'rs,'splitWAL,'backup-masters,'flush-table-proc,'master-maintenance,'online-snapshot,'switch,'master,'running,'draining,'namespace,'hbaseid,'table} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d3, packet:: clientPath:/hbase serverPath:/hbase finished:false header:: 2,8  replyHeader:: 2,1605,0  request:: '/hbase,F  response:: v{'meta-region-server,'rs,'splitWAL,'backup-masters,'flush-table-proc,'master-maintenance,'online-snapshot,'switch,'master,'running,'draining,'namespace,'hbaseid,'table} \n",
      "24/12/17 10:10:12 INFO ZooKeeper: Session: 0x10004ff096301ce closed\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c4, packet:: clientPath:/hbase serverPath:/hbase finished:false header:: 2,8  replyHeader:: 2,1605,0  request:: '/hbase,F  response:: v{'meta-region-server,'rs,'splitWAL,'backup-masters,'flush-table-proc,'master-maintenance,'online-snapshot,'switch,'master,'running,'draining,'namespace,'hbaseid,'table} \n",
      "24/12/17 10:10:12 INFO ZooKeeper: Session: 0x10004ff096301c3 closed\n",
      "24/12/17 10:10:12 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301c3\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Session: 0x10004ff096301c8 closed\n",
      "24/12/17 10:10:12 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301c8\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Session: 0x10004ff096301c9 closed\n",
      "24/12/17 10:10:12 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301c9\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Session: 0x10004ff096301cf closed\n",
      "24/12/17 10:10:12 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301cf\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Session: 0x10004ff096301d2 closed\n",
      "24/12/17 10:10:12 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301d2\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Session: 0x10004ff096301cb closed\n",
      "24/12/17 10:10:12 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301cb\n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301cc, packet:: clientPath:/hbase/meta-region-server serverPath:/hbase/meta-region-server finished:false header:: 3,4  replyHeader:: 3,1605,0  request:: '/hbase/meta-region-server,F  response:: #ffffffff000146d61737465723a31363030303cffffff89ffffffaf37ffffffae6a72ffffff8450425546a15a96861646f6f7031313310ffffff947d18ffffffc2ffffffc3ffffffe4ffffffe9ffffffbc32100183,s{32,639,1733275816357,1734314945234,3,0,0,0,56,0,32} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c4, packet:: clientPath:/hbase/meta-region-server serverPath:/hbase/meta-region-server finished:false header:: 3,4  replyHeader:: 3,1605,0  request:: '/hbase/meta-region-server,F  response:: #ffffffff000146d61737465723a31363030303cffffff89ffffffaf37ffffffae6a72ffffff8450425546a15a96861646f6f7031313310ffffff947d18ffffffc2ffffffc3ffffffe4ffffffe9ffffffbc32100183,s{32,639,1733275816357,1734314945234,3,0,0,0,56,0,32} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c2, packet:: clientPath:/hbase/meta-region-server serverPath:/hbase/meta-region-server finished:false header:: 3,4  replyHeader:: 3,1605,0  request:: '/hbase/meta-region-server,F  response:: #ffffffff000146d61737465723a31363030303cffffff89ffffffaf37ffffffae6a72ffffff8450425546a15a96861646f6f7031313310ffffff947d18ffffffc2ffffffc3ffffffe4ffffffe9ffffffbc32100183,s{32,639,1733275816357,1734314945234,3,0,0,0,56,0,32} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d3, packet:: clientPath:/hbase/meta-region-server serverPath:/hbase/meta-region-server finished:false header:: 3,4  replyHeader:: 3,1605,0  request:: '/hbase/meta-region-server,F  response:: #ffffffff000146d61737465723a31363030303cffffff89ffffffaf37ffffffae6a72ffffff8450425546a15a96861646f6f7031313310ffffff947d18ffffffc2ffffffc3ffffffe4ffffffe9ffffffbc32100183,s{32,639,1733275816357,1734314945234,3,0,0,0,56,0,32} \n",
      "24/12/17 10:10:12 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d1, packet:: clientPath:/hbase/meta-region-server serverPath:/hbase/meta-region-server finished:false header:: 3,4  replyHeader:: 3,1605,0  request:: '/hbase/meta-region-server,F  response:: #ffffffff000146d61737465723a31363030303cffffff89ffffffaf37ffffffae6a72ffffff8450425546a15a96861646f6f7031313310ffffff947d18ffffffc2ffffffc3ffffffe4ffffffe9ffffffbc32100183,s{32,639,1733275816357,1734314945234,3,0,0,0,56,0,32} \n",
      "24/12/17 10:10:12 INFO ZooKeeper: Session: 0x10004ff096301d5 closed\n",
      "24/12/17 10:10:12 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301d5\n",
      "24/12/17 10:10:12 DEBUG RpcConnection: Using SIMPLE authentication for service=ClientService, sasl=false\n",
      "24/12/17 10:10:12 DEBUG RpcConnection: Using SIMPLE authentication for service=ClientService, sasl=false\n",
      "24/12/17 10:10:12 DEBUG RpcConnection: Using SIMPLE authentication for service=ClientService, sasl=false\n",
      "24/12/17 10:10:12 DEBUG RpcConnection: Using SIMPLE authentication for service=ClientService, sasl=false\n",
      "24/12/17 10:10:12 DEBUG RpcConnection: Using SIMPLE authentication for service=ClientService, sasl=false\n",
      "24/12/17 10:10:12 INFO ZooKeeper: Session: 0x10004ff096301ca closed\n",
      "24/12/17 10:10:12 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301ca\n",
      "24/12/17 10:10:12 DEBUG AbstractByteBuf: -Dorg.apache.hbase.thirdparty.io.netty.buffer.checkAccessible: true\n",
      "24/12/17 10:10:12 DEBUG AbstractByteBuf: -Dorg.apache.hbase.thirdparty.io.netty.buffer.checkBounds: true\n",
      "24/12/17 10:10:12 DEBUG ResourceLeakDetectorFactory: Loaded default ResourceLeakDetector: org.apache.hbase.thirdparty.io.netty.util.ResourceLeakDetector@229c8870\n",
      "24/12/17 10:10:12 DEBUG DefaultChannelId: -Dio.netty.processId: 26149 (auto-detected)\n",
      "24/12/17 10:10:12 DEBUG NetUtil: -Djava.net.preferIPv4Stack: false\n",
      "24/12/17 10:10:12 DEBUG NetUtil: -Djava.net.preferIPv6Addresses: false\n",
      "24/12/17 10:10:12 DEBUG NetUtilInitializations: Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%lo)\n",
      "24/12/17 10:10:12 DEBUG NetUtil: /proc/sys/net/core/somaxconn: 4096\n",
      "24/12/17 10:10:12 DEBUG DefaultChannelId: -Dio.netty.machineId: 02:42:ac:ff:fe:19:01:0d (auto-detected)\n",
      "24/12/17 10:10:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numHeapArenas: 39\n",
      "24/12/17 10:10:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.numDirectArenas: 37\n",
      "24/12/17 10:10:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.pageSize: 8192\n",
      "24/12/17 10:10:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxOrder: 9\n",
      "24/12/17 10:10:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.chunkSize: 4194304\n",
      "24/12/17 10:10:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.smallCacheSize: 256\n",
      "24/12/17 10:10:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.normalCacheSize: 64\n",
      "24/12/17 10:10:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedBufferCapacity: 32768\n",
      "24/12/17 10:10:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimInterval: 8192\n",
      "24/12/17 10:10:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.cacheTrimIntervalMillis: 0\n",
      "24/12/17 10:10:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.useCacheForAllThreads: false\n",
      "24/12/17 10:10:13 DEBUG PooledByteBufAllocator: -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023\n",
      "24/12/17 10:10:13 DEBUG ByteBufUtil: -Dio.netty.allocator.type: pooled\n",
      "24/12/17 10:10:13 DEBUG ByteBufUtil: -Dio.netty.threadLocalDirectBufferSize: 0\n",
      "24/12/17 10:10:13 DEBUG ByteBufUtil: -Dio.netty.maxThreadLocalCharBufferSize: 16384\n",
      "24/12/17 10:10:13 INFO ZooKeeper: Session: 0x10004ff096301d0 closed\n",
      "24/12/17 10:10:13 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301d0\n",
      "24/12/17 10:10:13 DEBUG Recycler: -Dio.netty.recycler.maxCapacityPerThread: 4096\n",
      "24/12/17 10:10:13 DEBUG Recycler: -Dio.netty.recycler.ratio: 8\n",
      "24/12/17 10:10:13 DEBUG Recycler: -Dio.netty.recycler.chunkSize: 32\n",
      "24/12/17 10:10:13 DEBUG Recycler: -Dio.netty.recycler.blocking: false\n",
      "24/12/17 10:10:13 INFO ZooKeeper: Session: 0x10004ff096301c6 closed\n",
      "24/12/17 10:10:13 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301c6\n",
      "24/12/17 10:10:13 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x39d274c5 to 127.0.0.1:2181\n",
      "24/12/17 10:10:13 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x571c85fb to 127.0.0.1:2181\n",
      "24/12/17 10:10:13 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x5399227c to 127.0.0.1:2181\n",
      "24/12/17 10:10:13 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x2dd35eac to 127.0.0.1:2181\n",
      "24/12/17 10:10:13 DEBUG ReadOnlyZKClient: Close zookeeper connection 0x75b9d609 to 127.0.0.1:2181\n",
      "24/12/17 10:10:13 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:13 DEBUG ZooKeeper: Closing session: 0x10004ff096301c2\n",
      "24/12/17 10:10:13 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:13 DEBUG ZooKeeper: Closing session: 0x10004ff096301cc\n",
      "24/12/17 10:10:13 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301c2\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301cc\n",
      "24/12/17 10:10:13 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:13 DEBUG ZooKeeper: Closing session: 0x10004ff096301c4\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301c4\n",
      "24/12/17 10:10:13 DEBUG AbstractRpcClient: Stopping rpc client\n",
      "24/12/17 10:10:13 DEBUG ZooKeeper: Closing session: 0x10004ff096301d1\n",
      "24/12/17 10:10:13 DEBUG ZooKeeper: Closing session: 0x10004ff096301d3\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301d1\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Closing client for session: 0x10004ff096301d3\n",
      "24/12/17 10:10:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000003_0\n",
      "24/12/17 10:10:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000015_0\n",
      "24/12/17 10:10:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000019_0\n",
      "24/12/17 10:10:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000007_0\n",
      "24/12/17 10:10:13 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412171010114913797296662819386_0011_m_000011_0\n",
      "24/12/17 10:10:13 INFO Executor: Finished task 3.0 in stage 3.0 (TID 23). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:13 INFO Executor: Finished task 7.0 in stage 3.0 (TID 27). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:13 INFO Executor: Finished task 19.0 in stage 3.0 (TID 39). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:13 INFO Executor: Finished task 15.0 in stage 3.0 (TID 35). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:13 INFO Executor: Finished task 11.0 in stage 3.0 (TID 31). 1938 bytes result sent to driver\n",
      "24/12/17 10:10:13 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 4\n",
      "24/12/17 10:10:13 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 3\n",
      "24/12/17 10:10:13 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 2\n",
      "24/12/17 10:10:13 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 1\n",
      "24/12/17 10:10:13 DEBUG ExecutorMetricsPoller: stageTCMP: (3, 0) -> 0\n",
      "24/12/17 10:10:13 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 23) in 1350 ms on hadoop113 (executor driver) (16/20)\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d1, packet:: clientPath:null serverPath:null finished:false header:: 4,-11  replyHeader:: 4,1609,0  request:: null response:: null\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301d3, packet:: clientPath:null serverPath:null finished:false header:: 4,-11  replyHeader:: 4,1606,0  request:: null response:: null\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c2, packet:: clientPath:null serverPath:null finished:false header:: 4,-11  replyHeader:: 4,1608,0  request:: null response:: null\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301c4, packet:: clientPath:null serverPath:null finished:false header:: 4,-11  replyHeader:: 4,1610,0  request:: null response:: null\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301d1\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Reading reply sessionid:0x10004ff096301cc, packet:: clientPath:null serverPath:null finished:false header:: 4,-11  replyHeader:: 4,1607,0  request:: null response:: null\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301d3\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301d1 : Unable to read additional data from server sessionid 0x10004ff096301d1, likely server has closed socket\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301c2\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301d3 : Unable to read additional data from server sessionid 0x10004ff096301d3, likely server has closed socket\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301c2 : Unable to read additional data from server sessionid 0x10004ff096301c2, likely server has closed socket\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301c4\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: Disconnecting client for session: 0x10004ff096301cc\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301c4 : Unable to read additional data from server sessionid 0x10004ff096301c4, likely server has closed socket\n",
      "24/12/17 10:10:13 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 27) in 1350 ms on hadoop113 (executor driver) (17/20)\n",
      "24/12/17 10:10:13 DEBUG ClientCnxn: An exception was thrown while closing send thread for session 0x10004ff096301cc : Unable to read additional data from server sessionid 0x10004ff096301cc, likely server has closed socket\n",
      "24/12/17 10:10:13 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 35) in 1349 ms on hadoop113 (executor driver) (18/20)\n",
      "24/12/17 10:10:13 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 31) in 1350 ms on hadoop113 (executor driver) (19/20)\n",
      "24/12/17 10:10:13 INFO TaskSetManager: Finished task 19.0 in stage 3.0 (TID 39) in 1349 ms on hadoop113 (executor driver) (20/20)\n",
      "24/12/17 10:10:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/12/17 10:10:13 INFO DAGScheduler: ResultStage 3 (runJob at SparkHadoopWriter.scala:83) finished in 1.385 s\n",
      "24/12/17 10:10:13 DEBUG DAGScheduler: After removal of stage 3, remaining stages = 0\n",
      "24/12/17 10:10:13 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/17 10:10:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/12/17 10:10:13 INFO DAGScheduler: Job 3 finished: runJob at SparkHadoopWriter.scala:83, took 1.388595 s\n",
      "24/12/17 10:10:13 INFO SparkHadoopWriter: Start to commit write Job job_202412171010114913797296662819386_0011.\n",
      "24/12/17 10:10:13 WARN FileOutputCommitter: Output Path is null in commitJob()\n",
      "24/12/17 10:10:13 INFO SparkHadoopWriter: Write Job job_202412171010114913797296662819386_0011 committed. Elapsed time: 0 ms.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.ExecutedCommandExec\",\"num-children\":0,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]}]\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#0, city#1, name#2], false\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#0, city#1, name#2], false\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:13 DEBUG KafkaRelationVisitor: Checking if provider is KafkaSourceProvider\n",
      "24/12/17 10:10:13 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/17 10:10:13 DEBUG KafkaRelationVisitor: Kafka classes are not available to check whether provider is KafkaSourceProvider\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "24/12/17 10:10:13 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:13 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:13 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:13 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 566.1 KiB, free 364.8 MiB)\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Put block broadcast_5 locally took 4 ms\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Putting block broadcast_5 without replication took 4 ms\n",
      "24/12/17 10:10:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 364.8 MiB)\n",
      "24/12/17 10:10:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_5_piece0 for BlockManagerId(driver, hadoop113, 34591, None)\n",
      "24/12/17 10:10:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop113:34591 (size: 38.4 KiB, free: 366.1 MiB)\n",
      "24/12/17 10:10:13 DEBUG BlockManagerMaster: Updated info of block broadcast_5_piece0\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Told master about block broadcast_5_piece0\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Put block broadcast_5_piece0 locally took 0 ms\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Putting block broadcast_5_piece0 without replication took 0 ms\n",
      "24/12/17 10:10:13 INFO SparkContext: Created broadcast 5 from broadcast at HBaseContext.scala:71\n",
      "24/12/17 10:10:13 DEBUG SparkSQLExecutionContext: Posting event for start 1: io.openlineage.client.OpenLineage$RunEvent@462af0d6\n",
      "24/12/17 10:10:13 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:11.542Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3857-75bf-b3ae-bd2d2ba1a34a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:13 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:11.542Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3857-75bf-b3ae-bd2d2ba1a34a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:13 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@1908d683, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:13 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToHbase, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/17 10:10:13 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@1908d683, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:13 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, name=WriteToHbase}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.execute_save_into_data_source_command.testspark), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, created={time=1734405011542}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734405013222}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734405011542, status=STARTED}, dataProcessInstanceProperties={name=0193d298-3857-75bf-b3ae-bd2d2ba1a34a, created={actor=urn:li:corpuser:datahub, time=1734405011542}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193d298-3857-75bf-b3ae-bd2d2ba1a34a, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734405011542, eventFormatter=datahub.event.EventFormatter@6b7e9c05), from {\"eventTime\":\"2024-12-17T03:10:11.542Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193d298-3857-75bf-b3ae-bd2d2ba1a34a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:13 INFO DatahubEventEmitter: Collecting lineage successfully in 1 ms\n",
      "24/12/17 10:10:13 INFO AsyncEventQueue: Process of event SparkListenerSQLExecutionStart(1,save at NativeMethodAccessorImpl.java:0,org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "java.lang.reflect.Method.invoke(Method.java:498)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:282)\n",
      "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750),== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand (1)\n",
      "   +- SaveIntoDataSourceCommand (2)\n",
      "         +- LogicalRDD (3)\n",
      "\n",
      "\n",
      "(1) Execute SaveIntoDataSourceCommand\n",
      "Output: []\n",
      "\n",
      "(2) SaveIntoDataSourceCommand\n",
      "Arguments: org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, [hbase.columns.mapping=id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace=default, hbase.table=testspark, hbase.spark.use.hbasecontext=false], ErrorIfExists\n",
      "\n",
      "(3) LogicalRDD\n",
      "Arguments: [id#0, city#1, name#2], false\n",
      "\n",
      ",org.apache.spark.sql.execution.SparkPlanInfo@e60025fe,1734405011542,Map()) by listener DatahubSparkListener took 1.680278564s.\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/17 10:10:13 DEBUG SparkSQLExecutionContext: SparkListenerJobStart - executionId: 1\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.ExecutedCommandExec\",\"num-children\":0,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]}]\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#0, city#1, name#2], false\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#0, city#1, name#2], false\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:13 DEBUG KafkaRelationVisitor: Checking if provider is KafkaSourceProvider\n",
      "24/12/17 10:10:13 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/17 10:10:13 DEBUG KafkaRelationVisitor: Kafka classes are not available to check whether provider is KafkaSourceProvider\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "24/12/17 10:10:13 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:13 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:13 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:13 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 566.1 KiB, free 364.2 MiB)\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Put block broadcast_6 locally took 1 ms\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Putting block broadcast_6 without replication took 1 ms\n",
      "24/12/17 10:10:13 INFO ZooKeeper: Session: 0x10004ff096301d4 closed\n",
      "24/12/17 10:10:13 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301d4\n",
      "24/12/17 10:10:13 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 364.2 MiB)\n",
      "24/12/17 10:10:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_6_piece0 for BlockManagerId(driver, hadoop113, 34591, None)\n",
      "24/12/17 10:10:13 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop113:34591 (size: 38.4 KiB, free: 366.1 MiB)\n",
      "24/12/17 10:10:13 DEBUG BlockManagerMaster: Updated info of block broadcast_6_piece0\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Told master about block broadcast_6_piece0\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Put block broadcast_6_piece0 locally took 0 ms\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Putting block broadcast_6_piece0 without replication took 0 ms\n",
      "24/12/17 10:10:13 INFO SparkContext: Created broadcast 6 from broadcast at HBaseContext.scala:71\n",
      "24/12/17 10:10:13 DEBUG SparkSQLExecutionContext: Posting event for start 1: io.openlineage.client.OpenLineage$RunEvent@1e93d68a\n",
      "24/12/17 10:10:13 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:11.784Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"0193d298-3857-75bf-b3ae-bd2d2ba1a34a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":3},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:13 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:11.784Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"0193d298-3857-75bf-b3ae-bd2d2ba1a34a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":3},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:13 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@257dda09, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:13 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToHbase, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/17 10:10:13 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@257dda09, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:13 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), dataFlowInfo={customProperties={jobId=3, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, name=WriteToHbase}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.execute_save_into_data_source_command.testspark), jobInfo={customProperties={jobId=3, spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, created={time=1734405011784}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734405013248}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734405011784, status=STARTED}, dataProcessInstanceProperties={name=0193d298-3857-75bf-b3ae-bd2d2ba1a34a, created={actor=urn:li:corpuser:datahub, time=1734405011784}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193d298-3857-75bf-b3ae-bd2d2ba1a34a, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734405011784, eventFormatter=datahub.event.EventFormatter@4d199bdb), from {\"eventTime\":\"2024-12-17T03:10:11.784Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"0193d298-3857-75bf-b3ae-bd2d2ba1a34a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":3},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:13 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onJobStart completed successfully in 25 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/17 10:10:13 DEBUG SparkSQLExecutionContext: SparkListenerJobEnd - executionId: 1\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.ExecutedCommandExec\",\"num-children\":0,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]}]\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#0, city#1, name#2], false\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#0, city#1, name#2], false\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:13 DEBUG KafkaRelationVisitor: Checking if provider is KafkaSourceProvider\n",
      "24/12/17 10:10:13 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/17 10:10:13 DEBUG KafkaRelationVisitor: Kafka classes are not available to check whether provider is KafkaSourceProvider\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "24/12/17 10:10:13 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:13 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:13 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:13 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 566.1 KiB, free 363.6 MiB)\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Put block broadcast_7 locally took 1 ms\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Putting block broadcast_7 without replication took 1 ms\n",
      "24/12/17 10:10:13 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 363.6 MiB)\n",
      "24/12/17 10:10:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_7_piece0 for BlockManagerId(driver, hadoop113, 34591, None)\n",
      "24/12/17 10:10:13 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on hadoop113:34591 (size: 38.4 KiB, free: 366.1 MiB)\n",
      "24/12/17 10:10:13 DEBUG BlockManagerMaster: Updated info of block broadcast_7_piece0\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Told master about block broadcast_7_piece0\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Put block broadcast_7_piece0 locally took 1 ms\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Putting block broadcast_7_piece0 without replication took 1 ms\n",
      "24/12/17 10:10:13 INFO SparkContext: Created broadcast 7 from broadcast at HBaseContext.scala:71\n",
      "24/12/17 10:10:13 DEBUG SparkSQLExecutionContext: Posting event for end 1: io.openlineage.client.OpenLineage$RunEvent@52223f32\n",
      "24/12/17 10:10:13 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:13.172Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"0193d298-3857-75bf-b3ae-bd2d2ba1a34a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:13 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:13.172Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"0193d298-3857-75bf-b3ae-bd2d2ba1a34a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:13 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@6957eaeb, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:13 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToHbase, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/17 10:10:13 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@6957eaeb, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:13 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, name=WriteToHbase}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.execute_save_into_data_source_command.testspark), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, created={time=1734405013172}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734405013273}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734405013172, status=STARTED}, dataProcessInstanceProperties={name=0193d298-3857-75bf-b3ae-bd2d2ba1a34a, created={actor=urn:li:corpuser:datahub, time=1734405013172}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193d298-3857-75bf-b3ae-bd2d2ba1a34a, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734405013172, eventFormatter=datahub.event.EventFormatter@42e043da), from {\"eventTime\":\"2024-12-17T03:10:13.172Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"0193d298-3857-75bf-b3ae-bd2d2ba1a34a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:13 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: onJobEnd completed successfully in 25 ms\n",
      "24/12/17 10:10:13 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd\n",
      "24/12/17 10:10:13 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionEnd - executionId: 1\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.ExecutedCommandExec\",\"num-children\":0,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"id\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"city\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"5c8e9f13-2997-4228-9187-09547e0dd1aa\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]}]\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#0, city#1, name#2], false\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:13 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "   +- LogicalRDD [id#0, city#1, name#2], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.hadoop.hbase.spark.DefaultSource@6065c09e, Map(hbase.columns.mapping -> id STRING :key, name STRING cf:name, city STRING cf:city, hbase.namespace -> default, hbase.table -> testspark, hbase.spark.use.hbasecontext -> false), ErrorIfExists\n",
      "         +- LogicalRDD [id#0, city#1, name#2], false\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/17 10:10:13 DEBUG KafkaRelationVisitor: Checking if provider is KafkaSourceProvider\n",
      "24/12/17 10:10:13 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/17 10:10:13 DEBUG KafkaRelationVisitor: Kafka classes are not available to check whether provider is KafkaSourceProvider\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "24/12/17 10:10:13 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:13 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:13 DEBUG CatalystSqlParser: Parsing command: STRING\n",
      "24/12/17 10:10:13 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 566.1 KiB, free 363.0 MiB)\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Put block broadcast_8 locally took 1 ms\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Putting block broadcast_8 without replication took 1 ms\n",
      "24/12/17 10:10:13 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 363.0 MiB)\n",
      "24/12/17 10:10:13 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_8_piece0 for BlockManagerId(driver, hadoop113, 34591, None)\n",
      "24/12/17 10:10:13 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on hadoop113:34591 (size: 38.4 KiB, free: 366.0 MiB)\n",
      "24/12/17 10:10:13 DEBUG BlockManagerMaster: Updated info of block broadcast_8_piece0\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Told master about block broadcast_8_piece0\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Put block broadcast_8_piece0 locally took 0 ms\n",
      "24/12/17 10:10:13 DEBUG BlockManager: Putting block broadcast_8_piece0 without replication took 0 ms\n",
      "24/12/17 10:10:13 INFO SparkContext: Created broadcast 8 from broadcast at HBaseContext.scala:71\n",
      "24/12/17 10:10:13 DEBUG SparkSQLExecutionContext: Posting event for end 1: {\"eventTime\":\"2024-12-17T03:10:13.174Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193d298-3857-75bf-b3ae-bd2d2ba1a34a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:13 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:13.174Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193d298-3857-75bf-b3ae-bd2d2ba1a34a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:13 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-17T03:10:13.174Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193d298-3857-75bf-b3ae-bd2d2ba1a34a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:13 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@e785cd6, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:13 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToHbase, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/17 10:10:13 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@e785cd6, Properties: {spark.master=local[*], spark.app.name=WriteToHbase}\n",
      "24/12/17 10:10:13 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, name=WriteToHbase}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.execute_save_into_data_source_command.testspark), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToHbase, processingEngine=spark}, created={time=1734405013174}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToHbase,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734405013299}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734405013174, status=COMPLETE}, dataProcessInstanceProperties={name=0193d298-3857-75bf-b3ae-bd2d2ba1a34a, created={actor=urn:li:corpuser:datahub, time=1734405013174}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase,default),write_to_hbase.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193d298-3857-75bf-b3ae-bd2d2ba1a34a, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734405013174, eventFormatter=datahub.event.EventFormatter@3d1cc02c), from {\"eventTime\":\"2024-12-17T03:10:13.174Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193d298-3857-75bf-b3ae-bd2d2ba1a34a\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193d298-3635-734c-b877-489d4a83159b\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/17 10:10:13 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/17 10:10:13 INFO ZooKeeper: Session: 0x10004ff096301c7 closed\n",
      "24/12/17 10:10:13 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301c7\n",
      "24/12/17 10:10:13 INFO ZooKeeper: Session: 0x10004ff096301c5 closed\n",
      "24/12/17 10:10:13 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301c5\n",
      "24/12/17 10:10:13 INFO ZooKeeper: Session: 0x10004ff096301cc closed\n",
      "24/12/17 10:10:13 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301cc\n",
      "24/12/17 10:10:13 INFO ZooKeeper: Session: 0x10004ff096301c4 closed\n",
      "24/12/17 10:10:13 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301c4\n",
      "24/12/17 10:10:13 INFO ZooKeeper: Session: 0x10004ff096301c2 closed\n",
      "24/12/17 10:10:13 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301c2\n",
      "24/12/17 10:10:13 INFO ZooKeeper: Session: 0x10004ff096301d3 closed\n",
      "24/12/17 10:10:13 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301d3\n",
      "24/12/17 10:10:13 INFO ZooKeeper: Session: 0x10004ff096301d1 closed\n",
      "24/12/17 10:10:13 INFO ClientCnxn: EventThread shut down for session: 0x10004ff096301d1\n",
      "24/12/17 10:10:22 DEBUG ExecutorMetricsPoller: removing (1, 0) from stageTCMP\n",
      "24/12/17 10:10:22 DEBUG ExecutorMetricsPoller: removing (2, 0) from stageTCMP\n",
      "24/12/17 10:10:22 DEBUG ExecutorMetricsPoller: removing (0, 0) from stageTCMP\n",
      "24/12/17 10:10:22 DEBUG ExecutorMetricsPoller: removing (3, 0) from stageTCMP\n"
     ]
    }
   ],
   "source": [
    "#write to hbase\n",
    "\n",
    "# hbase:001:0> create 'testspark', 'cf'\n",
    "# Created table testspark\n",
    "# Took 2.3158 seconds                                                                                                                                                                           \n",
    "# => Hbase::Table - testspark\n",
    "# hbase:003:0> scan 'testspark'\n",
    "\n",
    "# sao k co ip hbase ma no van chay dc k nhi :D\n",
    "# .option(\"hbase.zookeeper.quorum\", \"localhost:2182\") \\\n",
    "goalsDF.write\\\n",
    ".format(\"org.apache.hadoop.hbase.spark\")\\\n",
    ".option(\"hbase.columns.mapping\",\"id STRING :key, name STRING cf:name, city STRING cf:city\")\\\n",
    ".option(\"hbase.namespace\", \"default\")\\\n",
    ".option(\"hbase.table\", \"testspark\")\\\n",
    ".option(\"hbase.spark.use.hbasecontext\", False)\\\n",
    ".save() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
