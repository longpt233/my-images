{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b29876-205a-497e-a1b2-87ac1b13b8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/20 17:28:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Initialize Spark session with HBase configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WriteToHbase11\") \\\n",
    "    .config(\"spark.jars\", \"/tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,/tmp/hbase-spark-1.1.0-SNAPSHOT.jar\") \\\n",
    "    .config(\"spark.extraListeners\", \"datahub.spark.DatahubSparkListener\")\\\n",
    "    .config(\"spark.datahub.rest.server\", \"http://10.208.164.167:8080\")\\\n",
    "    .config(\"spark.datahub.metadata.dataset.materialize\", \"true\")\\\n",
    "    .config(\"spark.datahub.metadata.dataset.experimental_include_schema_metadata\", \"true\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "    #  /tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,/tmp/hbase-spark-1.1.0-SNAPSHOT.jar\n",
    "    \n",
    "\n",
    "# .config(\"spark.hadoop.hbase.zookeeper.quorum\", \"localhost1\") -> oke ăn config này nhé (còn đâu khả năng nó ăn mặc định, hoặc là file habse.xml tỏng classpath)\n",
    "# .config(\"spark.hbase.host\", \"localhost:2182\")  -> khong chay \n",
    "# print(os.environ[\"HBASE_HOME\"])\n",
    "# os.environ[\"HBASE_HOME\"] =\"/tmp/a\" -> không work\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55b6b3e-6716-42ee-9e4b-8a08e05b0cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/20 17:28:47 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/12/20 17:28:47 INFO SharedState: Warehouse path is 'file:/spark-warehouse'.\n",
      "24/12/20 17:28:47 INFO ArgumentParser: Couldn't log config from file, will read it from SparkConf\n",
      "24/12/20 17:28:47 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/20 17:28:47 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/20 17:28:47 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/20 17:28:47 INFO ConsoleTransport: {\"eventTime\":\"2024-12-20T10:28:46.185Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193e39c-d97a-780d-a3d1-cf20766b7d77\",\"facets\":{\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"spark_applicationDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"master\":\"local[*]\",\"appName\":\"WriteToHbase11\",\"applicationId\":\"local-1734690526732\",\"deployMode\":\"client\",\"driverHost\":\"hadoop113\",\"userName\":\"root\",\"uiWebUrl\":\"http://hadoop113:4040\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"NONE\",\"integration\":\"SPARK\",\"jobType\":\"APPLICATION\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:47 INFO DatahubSparkListener: onApplicationStart completed successfully in 586 ms\n",
      "24/12/20 17:28:49 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/20 17:28:49 INFO ConsoleTransport: {\"eventTime\":\"2024-12-20T10:28:49.07Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193e39c-df35-716b-8a37-848068d539d9\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-d97a-780d-a3d1-cf20766b7d77\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:49 INFO CodeGenerator: Code generated in 83.1112 ms\n",
      "24/12/20 17:28:49 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/20 17:28:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 12.1 KiB, free 366.3 MiB)\n",
      "24/12/20 17:28:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 366.3 MiB)\n",
      "24/12/20 17:28:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop113:35417 (size: 6.4 KiB, free: 366.3 MiB)\n",
      "24/12/20 17:28:49 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "24/12/20 17:28:49 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (hadoop113, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/12/20 17:28:49 INFO DatahubSparkListener: sparkEnv: spark.app.id=local-1734690526732\n",
      "spark.app.initial.jar.urls=spark://hadoop113:34433/jars/hbase-spark-1.1.0-SNAPSHOT.jar,spark://hadoop113:34433/jars/acryl-spark-lineage-5206f9d-SNAPSHOT.jar\n",
      "spark.app.name=WriteToHbase11\n",
      "spark.app.startTime=1734690526185\n",
      "spark.app.submitTime=1734690526007\n",
      "spark.datahub.metadata.dataset.experimental_include_schema_metadata=true\n",
      "spark.datahub.metadata.dataset.materialize=true\n",
      "spark.datahub.rest.server=http://10.208.164.167:8080\n",
      "spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.driver.host=hadoop113\n",
      "spark.driver.port=34433\n",
      "spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.executor.id=driver\n",
      "spark.extraListeners=datahub.spark.DatahubSparkListener\n",
      "spark.jars=/tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,/tmp/hbase-spark-1.1.0-SNAPSHOT.jar\n",
      "spark.master=local[*]\n",
      "spark.rdd.compress=True\n",
      "spark.repl.local.jars=file:///tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,file:///tmp/hbase-spark-1.1.0-SNAPSHOT.jar\n",
      "spark.serializer.objectStreamReset=100\n",
      "spark.sql.warehouse.dir=file:/spark-warehouse\n",
      "spark.submit.deployMode=client\n",
      "spark.submit.pyFiles=\n",
      "spark.ui.showConsoleProgress=true\n",
      "24/12/20 17:28:49 INFO DatahubSparkListener: Datahub configuration: {\n",
      "    # String: 1-3\n",
      "    \"metadata\" : {\n",
      "        # String: 1-3\n",
      "        \"dataset\" : {\n",
      "            # String: 1\n",
      "            \"experimental_include_schema_metadata\" : \"true\",\n",
      "            # String: 3\n",
      "            \"materialize\" : \"true\"\n",
      "        }\n",
      "    },\n",
      "    # String: 2\n",
      "    \"rest\" : {\n",
      "        # String: 2\n",
      "        \"server\" : \"http://10.208.164.167:8080\"\n",
      "    }\n",
      "}\n",
      "\n",
      "24/12/20 17:28:49 INFO DatahubSparkListener: REST Emitter Configuration: GMS url http://10.208.164.167:8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/20 17:28:49 INFO ArgumentParser: Couldn't log config from file, will read it from SparkConf\n",
      "24/12/20 17:28:49 WARN CompositeMeterRegistry: A MeterFilter is being configured after a Meter has been registered to this registry. All MeterFilters should be configured before any Meters are registered. If that is not possible or you have a use case where it should be allowed, let the Micrometer maintainers know at https://github.com/micrometer-metrics/micrometer/issues/4920. Enable DEBUG level logging on this logger to see a stack trace of the call configuring this MeterFilter.\n",
      "24/12/20 17:28:49 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/20 17:28:49 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/20 17:28:49 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-20T10:28:49.244Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193e39c-e16d-7a25-9008-40559eacb573\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":0},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@3849bac8, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@3849bac8, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: alo alaoala io.openlineage.client.OpenLineage$RunEvent@79700884\n",
      "24/12/20 17:28:49 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), jobInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529244}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690529750}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734690529244, status=STARTED}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690529244}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690529244, eventFormatter=datahub.event.EventFormatter@16baf7bc), from {\"eventTime\":\"2024-12-20T10:28:49.244Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193e39c-e16d-7a25-9008-40559eacb573\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":0},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:49 INFO DatahubEventEmitter: Collecting lineage successfully in 122 ms\n",
      "24/12/20 17:28:49 INFO PythonRunner: Times: total = 288, boot = 240, init = 48, finish = 0\n",
      "24/12/20 17:28:49 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1832 bytes result sent to driver\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 458 ms on hadoop113 (executor driver) (1/1)\n",
      "24/12/20 17:28:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/12/20 17:28:49 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 54497\n",
      "24/12/20 17:28:49 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 0.586 s\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/20 17:28:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 0.617027 s\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-20T10:28:49.845Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193e39c-e16d-7a25-9008-40559eacb573\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@209eb194, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@209eb194, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: alo alaoala io.openlineage.client.OpenLineage$RunEvent@5c12ac95\n",
      "24/12/20 17:28:49 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529845}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690529852}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690529845}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690529845, eventFormatter=datahub.event.EventFormatter@410d6341), from {\"eventTime\":\"2024-12-20T10:28:49.845Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193e39c-e16d-7a25-9008-40559eacb573\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:49 INFO DatahubEventEmitter: Collecting lineage successfully in 3 ms\n",
      "24/12/20 17:28:49 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 4 output partitions\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/20 17:28:49 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.1 KiB, free 366.3 MiB)\n",
      "24/12/20 17:28:49 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 366.3 MiB)\n",
      "24/12/20 17:28:49 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on hadoop113:35417 (size: 6.4 KiB, free: 366.3 MiB)\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-20T10:28:49.855Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193e39c-e16d-7a25-9008-40559eacb573\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":1},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@78688c32, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:49 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@78688c32, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: alo alaoala io.openlineage.client.OpenLineage$RunEvent@3157e1ae\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/12/20 17:28:49 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (hadoop113, executor driver, partition 1, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (hadoop113, executor driver, partition 2, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (hadoop113, executor driver, partition 3, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (hadoop113, executor driver, partition 4, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={jobId=1, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), jobInfo={customProperties={jobId=1, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529855}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690529860}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734690529855, status=STARTED}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690529855}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690529855, eventFormatter=datahub.event.EventFormatter@42958b22), from {\"eventTime\":\"2024-12-20T10:28:49.855Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193e39c-e16d-7a25-9008-40559eacb573\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":1},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:49 INFO DatahubEventEmitter: Collecting lineage successfully in 9 ms\n",
      "24/12/20 17:28:49 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)\n",
      "24/12/20 17:28:49 INFO PythonRunner: Times: total = 43, boot = -58, init = 101, finish = 0\n",
      "24/12/20 17:28:49 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 1850 bytes result sent to driver\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 64 ms on hadoop113 (executor driver) (1/4)\n",
      "24/12/20 17:28:49 INFO PythonRunner: Times: total = 49, boot = 6, init = 43, finish = 0\n",
      "24/12/20 17:28:49 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 70 ms on hadoop113 (executor driver) (2/4)\n",
      "24/12/20 17:28:49 INFO PythonRunner: Times: total = 48, boot = 5, init = 43, finish = 0\n",
      "24/12/20 17:28:49 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 73 ms on hadoop113 (executor driver) (3/4)\n",
      "24/12/20 17:28:49 INFO PythonRunner: Times: total = 52, boot = 9, init = 43, finish = 0\n",
      "24/12/20 17:28:49 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 78 ms on hadoop113 (executor driver) (4/4)\n",
      "24/12/20 17:28:49 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/12/20 17:28:49 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.088 s\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/20 17:28:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.091069 s\n",
      "24/12/20 17:28:49 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 15 output partitions\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/20 17:28:49 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.1 KiB, free 366.3 MiB)\n",
      "24/12/20 17:28:49 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 366.2 MiB)\n",
      "24/12/20 17:28:49 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on hadoop113:35417 (size: 6.4 KiB, free: 366.3 MiB)\n",
      "24/12/20 17:28:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-20T10:28:49.95Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193e39c-e16d-7a25-9008-40559eacb573\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":2},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@62349002, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@62349002, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:49 INFO DAGScheduler: Submitting 15 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19))\n",
      "24/12/20 17:28:49 INFO OpenLineageToDataHub: alo alaoala io.openlineage.client.OpenLineage$RunEvent@60cccc58\n",
      "24/12/20 17:28:49 INFO TaskSchedulerImpl: Adding task set 2.0 with 15 tasks resource profile 0\n",
      "24/12/20 17:28:49 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={jobId=2, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), jobInfo={customProperties={jobId=2, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529950}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690529957}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734690529950, status=STARTED}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690529950}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690529950, eventFormatter=datahub.event.EventFormatter@3e0de9a5), from {\"eventTime\":\"2024-12-20T10:28:49.95Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193e39c-e16d-7a25-9008-40559eacb573\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":2},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:49 INFO DatahubEventEmitter: Collecting lineage successfully in 3 ms\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5) (hadoop113, executor driver, partition 5, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6) (hadoop113, executor driver, partition 6, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 7) (hadoop113, executor driver, partition 7, PROCESS_LOCAL, 4482 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 8) (hadoop113, executor driver, partition 8, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 4.0 in stage 2.0 (TID 9) (hadoop113, executor driver, partition 9, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 10) (hadoop113, executor driver, partition 10, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 11) (hadoop113, executor driver, partition 11, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 7.0 in stage 2.0 (TID 12) (hadoop113, executor driver, partition 12, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 8.0 in stage 2.0 (TID 13) (hadoop113, executor driver, partition 13, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 9.0 in stage 2.0 (TID 14) (hadoop113, executor driver, partition 14, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 10.0 in stage 2.0 (TID 15) (hadoop113, executor driver, partition 15, PROCESS_LOCAL, 4483 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 16) (hadoop113, executor driver, partition 16, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 17) (hadoop113, executor driver, partition 17, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 13.0 in stage 2.0 (TID 18) (hadoop113, executor driver, partition 18, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 19) (hadoop113, executor driver, partition 19, PROCESS_LOCAL, 4487 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:49 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 2.0 in stage 2.0 (TID 7)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 3.0 in stage 2.0 (TID 8)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 4.0 in stage 2.0 (TID 9)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 5.0 in stage 2.0 (TID 10)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 6.0 in stage 2.0 (TID 11)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 7.0 in stage 2.0 (TID 12)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 8.0 in stage 2.0 (TID 13)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 9.0 in stage 2.0 (TID 14)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 11.0 in stage 2.0 (TID 16)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 12.0 in stage 2.0 (TID 17)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 10.0 in stage 2.0 (TID 15)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 13.0 in stage 2.0 (TID 18)\n",
      "24/12/20 17:28:49 INFO Executor: Running task 14.0 in stage 2.0 (TID 19)\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 41, boot = -34, init = 75, finish = 0\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 2.0 in stage 2.0 (TID 7). 1842 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 7) in 57 ms on hadoop113 (executor driver) (1/15)\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 44, boot = -44, init = 88, finish = 0\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 3.0 in stage 2.0 (TID 8). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 8) in 59 ms on hadoop113 (executor driver) (2/15)\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 45, boot = -27, init = 72, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 44, boot = -26, init = 70, finish = 0\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 61 ms on hadoop113 (executor driver) (3/15)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 61 ms on hadoop113 (executor driver) (4/15)\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 49, boot = 6, init = 43, finish = 0\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 8.0 in stage 2.0 (TID 13). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 8.0 in stage 2.0 (TID 13) in 65 ms on hadoop113 (executor driver) (5/15)\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 46, boot = 4, init = 42, finish = 0\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 9.0 in stage 2.0 (TID 14). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 9.0 in stage 2.0 (TID 14) in 68 ms on hadoop113 (executor driver) (6/15)\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 52, boot = 10, init = 42, finish = 0\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 7.0 in stage 2.0 (TID 12). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 7.0 in stage 2.0 (TID 12) in 71 ms on hadoop113 (executor driver) (7/15)\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 54, boot = 13, init = 41, finish = 0\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 5.0 in stage 2.0 (TID 10). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 10) in 74 ms on hadoop113 (executor driver) (8/15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 61, boot = 18, init = 43, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 62, boot = 20, init = 42, finish = 0\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 12.0 in stage 2.0 (TID 17). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 6.0 in stage 2.0 (TID 11). 1850 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 12.0 in stage 2.0 (TID 17) in 78 ms on hadoop113 (executor driver) (9/15)\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 64, boot = 22, init = 42, finish = 0\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 11) in 80 ms on hadoop113 (executor driver) (10/15)\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 4.0 in stage 2.0 (TID 9). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 60, boot = 18, init = 42, finish = 0\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 4.0 in stage 2.0 (TID 9) in 83 ms on hadoop113 (executor driver) (11/15)\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 10.0 in stage 2.0 (TID 15). 1847 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 63, boot = 19, init = 44, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 64, boot = 22, init = 42, finish = 0\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 10.0 in stage 2.0 (TID 15) in 85 ms on hadoop113 (executor driver) (12/15)\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 65, boot = 23, init = 42, finish = 0\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 13.0 in stage 2.0 (TID 18). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 11.0 in stage 2.0 (TID 16). 1789 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 14.0 in stage 2.0 (TID 19). 1855 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 13.0 in stage 2.0 (TID 18) in 88 ms on hadoop113 (executor driver) (13/15)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 11.0 in stage 2.0 (TID 16) in 91 ms on hadoop113 (executor driver) (14/15)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 14.0 in stage 2.0 (TID 19) in 91 ms on hadoop113 (executor driver) (15/15)\n",
      "24/12/20 17:28:50 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/12/20 17:28:50 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.103 s\n",
      "24/12/20 17:28:50 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/20 17:28:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/12/20 17:28:50 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.107356 s\n",
      "24/12/20 17:28:50 INFO CodeGenerator: Code generated in 10.498644 ms\n",
      "24/12/20 17:28:50 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-20T10:28:50.084Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193e39c-e16d-7a25-9008-40559eacb573\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:50 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@32a58ccf, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:50 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@32a58ccf, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:50 INFO OpenLineageToDataHub: alo alaoala io.openlineage.client.OpenLineage$RunEvent@63b40bff\n",
      "24/12/20 17:28:50 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690530084}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690530088}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690530084, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690530084}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690530084, eventFormatter=datahub.event.EventFormatter@69d7d4e4), from {\"eventTime\":\"2024-12-20T10:28:50.084Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193e39c-e16d-7a25-9008-40559eacb573\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.collect_limit\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:50 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "+---+---------+----------+\n",
      "| id|     city|      name|\n",
      "+---+---------+----------+\n",
      "|  2|Telangana|   Chhetri|\n",
      "|  3|   Sikkim|    Bhutia|\n",
      "|  4|Hyderabad|   Shabbir|\n",
      "|  5|   Kerala|   Vijayan|\n",
      "|  6|  Mizoram|Lalpekhlua|\n",
      "+---+---------+----------+\n",
      "\n",
      "24/12/20 17:28:50 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "24/12/20 17:28:50 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 566.1 KiB, free 365.1 MiB)\n",
      "24/12/20 17:28:50 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 566.1 KiB, free 365.1 MiB)\n",
      "24/12/20 17:28:50 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 365.1 MiB)\n",
      "24/12/20 17:28:50 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 365.1 MiB)\n",
      "24/12/20 17:28:50 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on hadoop113:35417 (size: 38.4 KiB, free: 366.2 MiB)\n",
      "24/12/20 17:28:50 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on hadoop113:35417 (size: 38.4 KiB, free: 366.2 MiB)\n",
      "24/12/20 17:28:50 INFO SparkContext: Created broadcast 4 from broadcast at HBaseContext.scala:71\n",
      "24/12/20 17:28:50 INFO SparkContext: Created broadcast 3 from broadcast at HBaseContext.scala:71\n",
      "24/12/20 17:28:50 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-20T10:28:50.143Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193e39c-e360-7099-abfc-6d471184fdb3\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:50 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@199218e3, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:50 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@199218e3, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:50 INFO OpenLineageToDataHub: alo alaoala io.openlineage.client.OpenLineage$RunEvent@38f839f8\n",
      "24/12/20 17:28:50 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690530143}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690530273}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734690530143, status=STARTED}, dataProcessInstanceProperties={name=0193e39c-e360-7099-abfc-6d471184fdb3, created={actor=urn:li:corpuser:datahub, time=1734690530143}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e360-7099-abfc-6d471184fdb3, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690530143, eventFormatter=datahub.event.EventFormatter@412ba378), from {\"eventTime\":\"2024-12-20T10:28:50.143Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193e39c-e360-7099-abfc-6d471184fdb3\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:50 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/20 17:28:50 INFO HBaseRelation: newtable\n",
      "is not defined or no larger than 3, skip the create table\n",
      "24/12/20 17:28:50 INFO deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 WARN FileOutputCommitter: Output Path is null in setupJob()\n",
      "24/12/20 17:28:50 INFO SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
      "24/12/20 17:28:50 INFO DAGScheduler: Got job 3 (runJob at SparkHadoopWriter.scala:83) with 20 output partitions\n",
      "24/12/20 17:28:50 INFO DAGScheduler: Final stage: ResultStage 3 (runJob at SparkHadoopWriter.scala:83)\n",
      "24/12/20 17:28:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/20 17:28:50 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/20 17:28:50 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[11] at map at DefaultSource.scala:277), which has no missing parents\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "24/12/20 17:28:50 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 566.1 KiB, free 364.5 MiB)\n",
      "24/12/20 17:28:50 INFO BlockManagerInfo: Removed broadcast_0_piece0 on hadoop113:35417 in memory (size: 6.4 KiB, free: 366.2 MiB)\n",
      "24/12/20 17:28:50 INFO BlockManagerInfo: Removed broadcast_2_piece0 on hadoop113:35417 in memory (size: 6.4 KiB, free: 366.2 MiB)\n",
      "24/12/20 17:28:50 INFO BlockManagerInfo: Removed broadcast_1_piece0 on hadoop113:35417 in memory (size: 6.4 KiB, free: 366.2 MiB)\n",
      "24/12/20 17:28:50 INFO BlockManagerInfo: Removed broadcast_4_piece0 on hadoop113:35417 in memory (size: 38.4 KiB, free: 366.3 MiB)\n",
      "24/12/20 17:28:50 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 365.1 MiB)\n",
      "24/12/20 17:28:50 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 256.1 KiB, free 364.9 MiB)\n",
      "24/12/20 17:28:50 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on hadoop113:35417 (size: 38.4 KiB, free: 366.2 MiB)\n",
      "24/12/20 17:28:50 INFO SparkContext: Created broadcast 5 from broadcast at HBaseContext.scala:71\n",
      "24/12/20 17:28:50 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 90.6 KiB, free 364.8 MiB)\n",
      "24/12/20 17:28:50 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on hadoop113:35417 (size: 90.6 KiB, free: 366.1 MiB)\n",
      "24/12/20 17:28:50 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/20 17:28:50 INFO DAGScheduler: Submitting 20 missing tasks from ResultStage 3 (MapPartitionsRDD[11] at map at DefaultSource.scala:277) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "24/12/20 17:28:50 INFO TaskSchedulerImpl: Adding task set 3.0 with 20 tasks resource profile 0\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 20) (hadoop113, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 21) (hadoop113, executor driver, partition 1, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 22) (hadoop113, executor driver, partition 2, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 23) (hadoop113, executor driver, partition 3, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 24) (hadoop113, executor driver, partition 4, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 25) (hadoop113, executor driver, partition 5, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 6.0 in stage 3.0 (TID 26) (hadoop113, executor driver, partition 6, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 7.0 in stage 3.0 (TID 27) (hadoop113, executor driver, partition 7, PROCESS_LOCAL, 4482 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 8.0 in stage 3.0 (TID 28) (hadoop113, executor driver, partition 8, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 9.0 in stage 3.0 (TID 29) (hadoop113, executor driver, partition 9, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 10.0 in stage 3.0 (TID 30) (hadoop113, executor driver, partition 10, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 11.0 in stage 3.0 (TID 31) (hadoop113, executor driver, partition 11, PROCESS_LOCAL, 4486 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 12.0 in stage 3.0 (TID 32) (hadoop113, executor driver, partition 12, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 13.0 in stage 3.0 (TID 33) (hadoop113, executor driver, partition 13, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-20T10:28:50.344Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"0193e39c-e360-7099-abfc-6d471184fdb3\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":3},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 14.0 in stage 3.0 (TID 34) (hadoop113, executor driver, partition 14, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@6ccd085b, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 15.0 in stage 3.0 (TID 35) (hadoop113, executor driver, partition 15, PROCESS_LOCAL, 4483 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@6ccd085b, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 16.0 in stage 3.0 (TID 36) (hadoop113, executor driver, partition 16, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO OpenLineageToDataHub: alo alaoala io.openlineage.client.OpenLineage$RunEvent@17f7f455\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 17.0 in stage 3.0 (TID 37) (hadoop113, executor driver, partition 17, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 18.0 in stage 3.0 (TID 38) (hadoop113, executor driver, partition 18, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Starting task 19.0 in stage 3.0 (TID 39) (hadoop113, executor driver, partition 19, PROCESS_LOCAL, 4487 bytes) taskResourceAssignments Map()\n",
      "24/12/20 17:28:50 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={jobId=3, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), jobInfo={customProperties={jobId=3, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690530344}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690530403}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734690530344, status=STARTED}, dataProcessInstanceProperties={name=0193e39c-e360-7099-abfc-6d471184fdb3, created={actor=urn:li:corpuser:datahub, time=1734690530344}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e360-7099-abfc-6d471184fdb3, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690530344, eventFormatter=datahub.event.EventFormatter@58afb3d0), from {\"eventTime\":\"2024-12-20T10:28:50.344Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"0193e39c-e360-7099-abfc-6d471184fdb3\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_jobDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"jobId\":3},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:50 INFO DatahubEventEmitter: Collecting lineage successfully in 3 ms\n",
      "24/12/20 17:28:50 INFO Executor: Running task 0.0 in stage 3.0 (TID 20)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 1.0 in stage 3.0 (TID 21)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 2.0 in stage 3.0 (TID 22)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 3.0 in stage 3.0 (TID 23)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 4.0 in stage 3.0 (TID 24)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 5.0 in stage 3.0 (TID 25)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 6.0 in stage 3.0 (TID 26)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 8.0 in stage 3.0 (TID 28)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 7.0 in stage 3.0 (TID 27)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 10.0 in stage 3.0 (TID 30)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 12.0 in stage 3.0 (TID 32)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 13.0 in stage 3.0 (TID 33)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 9.0 in stage 3.0 (TID 29)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 11.0 in stage 3.0 (TID 31)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 14.0 in stage 3.0 (TID 34)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 15.0 in stage 3.0 (TID 35)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 16.0 in stage 3.0 (TID 36)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 17.0 in stage 3.0 (TID 37)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 18.0 in stage 3.0 (TID 38)\n",
      "24/12/20 17:28:50 INFO Executor: Running task 19.0 in stage 3.0 (TID 39)\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "24/12/20 17:28:50 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:zookeeper.version=3.5.7-f0fdd52973d373ffd9c86b81d99842dc2c7f660e, built on 02/10/2020 11:30 GMT\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:host.name=hadoop113\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:java.version=1.8.0_432\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:java.vendor=Private Build\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:java.class.path=/usr/local/lib/python3.8/dist-packages/pyspark/conf:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-rbac-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-sql_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/zookeeper-jute-3.6.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-compress-1.21.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-xml_2.12-1.2.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-kqueue-4.1.74.Final-osx-aarch_64.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-beeline-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-exec-2.3.9-core.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/joda-time-2.10.13.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-batch-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-dbcp-1.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-reflect-2.12.15.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-kubernetes_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/xbean-asm9-shaded-4.20.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arrow-memory-core-7.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hk2-utils-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/lz4-java-1.8.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/log4j-core-2.17.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/JTransforms-3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/curator-client-2.13.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-shaded-guava-1.1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/log4j-1.2-api-2.17.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-policy-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/flatbuffers-java-1.12.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-hive_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/HikariCP-2.5.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-jackson-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hk2-api-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/shims-0.9.25.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-extensions-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/antlr4-runtime-4.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/breeze-macros_2.12-1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-lang3-3.12.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/shapeless_2.12-2.3.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/datanucleus-api-jdo-4.2.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-launcher_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-text-1.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/generex-1.0.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-container-servlet-core-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-apiextensions-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-cli-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/javax.jdo-3.2.0-m3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jdo-api-3.0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jpam-1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/stax-api-1.0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-metrics-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/javolution-5.5.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/protobuf-java-2.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-autoscaling-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-databind-2.13.4.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arrow-memory-netty-7.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-networking-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-resolver-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-catalyst_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-compiler-2.12.15.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-serde-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/istack-commons-runtime-3.0.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/libfb303-0.9.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/avro-ipc-1.11.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-client-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-jmx-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-codec-1.15.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-core-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jline-2.14.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-collection-compat_2.12-2.1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.validation-api-2.0.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/core-1.1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/slf4j-api-1.7.32.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-logging-1.1.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/javassist-3.25.0-GA.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-events-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/zstd-jni-1.5.2-1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-hadoop-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/snakeyaml-1.31.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spire-macros_2.12-0.17.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-all-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-scheduling-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/zookeeper-3.6.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-cli-1.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/threeten-extra-1.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/snappy-java-1.1.8.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-tcnative-classes-2.0.48.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-network-common_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/curator-recipes-2.13.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-lang-2.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-parser-combinators_2.12-1.1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arpack_combined_all-0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/py4j-0.10.9.5.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-discovery-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jsr305-3.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-service-rpc-3.1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-datatype-jsr310-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-graphx_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spire_2.12-0.17.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/okhttp-3.12.12.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/bonecp-0.8.0.RELEASE.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jcl-over-slf4j-1.7.32.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-epoll-4.1.74.Final-linux-aarch_64.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/orc-shims-1.7.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/transaction-api-1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spire-platform_2.12-0.17.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/guava-14.0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/chill-java-0.10.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/minlog-1.3.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-buffer-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-pool-1.5.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/opencsv-2.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/orc-mapreduce-1.7.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/aopalliance-repackaged-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arrow-vector-7.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-common-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/scala-library-2.12.15.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/univocity-parsers-2.9.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-unsafe_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-handler-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/chill_2.12-0.10.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-node-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-container-servlet-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-common-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json4s-jackson_2.12-3.7.0-M11.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arpack-2.2.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/JLargeArrays-1.5.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-math3-3.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-core-asl-1.9.13.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/datanucleus-core-4.1.17.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.inject-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/janino-3.0.16.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-api-3.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-server-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/arrow-format-7.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-compiler-3.0.16.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-mllib-local_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/xz-1.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-dataformat-yaml-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.servlet-api-4.0.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json4s-ast_2.12-3.7.0-M11.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-collections-3.2.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-classes-kqueue-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/pickle-1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jul-to-slf4j-1.7.32.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/avro-1.11.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/RoaringBitmap-0.9.25.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jaxb-runtime-2.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/super-csv-2.2.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/httpcore-4.4.14.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-yarn_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-encoding-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json4s-core_2.12-3.7.0-M11.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/lapack-2.2.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.annotation-api-1.3.5.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-flowcontrol-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/orc-core-1.7.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-unix-common-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-module-scala_2.12-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/compress-lzf-1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/objenesis-3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-core-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/httpclient-4.5.13.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ivy-2.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-yarn-server-web-proxy-3.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/log4j-api-2.17.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-shims-scheduler-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/automaton-1.11-8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/curator-framework-2.13.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/ST4-4.0.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-kvstore_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-certificates-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-client-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/avro-mapred-1.11.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kryo-shaded-4.0.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-core-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/zjsonpatch-0.3.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jodd-core-3.5.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-crypto-1.1.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-sketch_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-shims-common-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/velocity-1.5.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hadoop-client-runtime-3.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-annotations-2.13.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-coordination-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-storage-api-2.7.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-format-structures-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-vector-code-gen-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/oro-2.0.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-epoll-4.1.74.Final-linux-x86_64.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-metastore-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-common-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jta-1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-graphite-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-admissionregistration-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/osgi-resource-locator-1.0.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-collections4-4.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/commons-io-2.11.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/derby-10.14.2.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spire-util_2.12-0.17.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/blas-2.2.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-shims-0.23-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-json-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/paranamer-2.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/aircompressor-0.21.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-mesos_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/parquet-column-1.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-codec-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/antlr-runtime-3.5.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-storageclass-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/metrics-jvm-4.2.7.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-network-shuffle_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/leveldbjni-all-1.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/stream-2.9.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/cats-kernel_2.12-2.1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/gson-2.2.4.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-repl_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/breeze_2.12-1.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-native-kqueue-4.1.74.Final-osx-x86_64.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-shims-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/annotations-17.0.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/mesos-1.4.3-shaded-protobuf.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.ws.rs-api-2.1.6.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-mllib_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/okio-1.14.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/tink-1.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-transport-classes-epoll-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-hive-thriftserver_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-apps-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jersey-hk2-2.36.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jackson-mapper-asl-1.9.13.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/algebra_2.12-2.0.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/log4j-slf4j-impl-2.17.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-llap-common-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json-1.8.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/audience-annotations-0.5.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hive-jdbc-2.3.9.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/jakarta.xml.bind-api-2.3.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/activation-1.1.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/netty-common-4.1.74.Final.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/libthrift-0.12.0.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-core_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/datanucleus-rdbms-4.1.19.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/rocksdbjni-6.20.3.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/logging-interceptor-3.12.12.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/json4s-scalap_2.12-3.7.0-M11.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-tags_2.12-3.3.1-tests.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-streaming_2.12-3.3.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/hk2-locator-2.6.1.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/kubernetes-model-common-5.12.2.jar:/usr/local/lib/python3.8/dist-packages/pyspark/jars/spark-tags_2.12-3.3.1.jar\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:java.compiler=<NA>\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:os.name=Linux\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:os.arch=amd64\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:os.version=6.8.0-51-generic\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:user.name=root\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:user.home=/root\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:user.dir=/\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:os.memory.free=785MB\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:os.memory.max=957MB\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Client environment:os.memory.total=957MB\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient$$Lambda$3074/2103910349@4c14d3a8\n",
      "24/12/20 17:28:50 INFO X509Util: Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38056, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38060, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38080, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38062, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38092, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38058, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c260033, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c260032, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c26002f, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c260030, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c260031, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c260034, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38118, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38106, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38114, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38110, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38112, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38142, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38122, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38108, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38124, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38250, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38120, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38116, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38278, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Socket connection established, initiating session, client: /127.0.0.1:38266, server: localhost/127.0.0.1:2181\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c260035, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c260037, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c260036, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c260038, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c26003c, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c26003a, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c26003e, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c26003f, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c260040, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c260041, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c26003d, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c260042, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c260039, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x10000666c26003b, negotiated timeout = 90000\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 57, boot = 14, init = 43, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 45, boot = -479, init = 524, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 48, boot = -477, init = 525, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 42, boot = -508, init = 550, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 58, boot = -505, init = 563, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 48, boot = 6, init = 42, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 42, boot = -487, init = 529, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 46, boot = -506, init = 552, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 44, boot = -477, init = 521, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 51, boot = -478, init = 529, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 47, boot = -476, init = 523, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 49, boot = -501, init = 550, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 51, boot = 3, init = 48, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 52, boot = -477, init = 529, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 41, boot = -498, init = 539, finish = 0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000012_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000010_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000000_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000008_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000006_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000016_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000018_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000001_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000002_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000009_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000013_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000004_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000014_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000017_0\n",
      "24/12/20 17:28:50 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000005_0\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 4.0 in stage 3.0 (TID 24). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 12.0 in stage 3.0 (TID 32). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 9.0 in stage 3.0 (TID 29). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 5.0 in stage 3.0 (TID 25). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 6.0 in stage 3.0 (TID 26). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 10.0 in stage 3.0 (TID 30). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 0.0 in stage 3.0 (TID 20). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 18.0 in stage 3.0 (TID 38). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 2.0 in stage 3.0 (TID 22). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 16.0 in stage 3.0 (TID 36). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 17.0 in stage 3.0 (TID 37). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 13.0 in stage 3.0 (TID 33). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 8.0 in stage 3.0 (TID 28). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 1.0 in stage 3.0 (TID 21). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO Executor: Finished task 14.0 in stage 3.0 (TID 34). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 24) in 385 ms on hadoop113 (executor driver) (1/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 25) in 385 ms on hadoop113 (executor driver) (2/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 10.0 in stage 3.0 (TID 30) in 384 ms on hadoop113 (executor driver) (3/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 20) in 387 ms on hadoop113 (executor driver) (4/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 18.0 in stage 3.0 (TID 38) in 384 ms on hadoop113 (executor driver) (5/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 12.0 in stage 3.0 (TID 32) in 387 ms on hadoop113 (executor driver) (6/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 6.0 in stage 3.0 (TID 26) in 389 ms on hadoop113 (executor driver) (7/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 9.0 in stage 3.0 (TID 29) in 388 ms on hadoop113 (executor driver) (8/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 22) in 390 ms on hadoop113 (executor driver) (9/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 16.0 in stage 3.0 (TID 36) in 388 ms on hadoop113 (executor driver) (10/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 13.0 in stage 3.0 (TID 33) in 388 ms on hadoop113 (executor driver) (11/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 21) in 391 ms on hadoop113 (executor driver) (12/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 17.0 in stage 3.0 (TID 37) in 389 ms on hadoop113 (executor driver) (13/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 8.0 in stage 3.0 (TID 28) in 393 ms on hadoop113 (executor driver) (14/20)\n",
      "24/12/20 17:28:50 INFO TaskSetManager: Finished task 14.0 in stage 3.0 (TID 34) in 391 ms on hadoop113 (executor driver) (15/20)\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 41, boot = -503, init = 544, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 47, boot = -480, init = 527, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 46, boot = -492, init = 538, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 47, boot = 4, init = 43, finish = 0\n",
      "24/12/20 17:28:50 INFO PythonRunner: Times: total = 52, boot = 10, init = 42, finish = 0\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Session: 0x10000666c260038 closed\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Session: 0x10000666c260033 closed\n",
      "24/12/20 17:28:50 INFO ClientCnxn: EventThread shut down for session: 0x10000666c260033\n",
      "24/12/20 17:28:50 INFO ClientCnxn: EventThread shut down for session: 0x10000666c260038\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Session: 0x10000666c26003b closed\n",
      "24/12/20 17:28:50 INFO ClientCnxn: EventThread shut down for session: 0x10000666c26003b\n",
      "24/12/20 17:28:50 INFO ZooKeeper: Session: 0x10000666c260030 closed\n",
      "24/12/20 17:28:50 INFO ClientCnxn: EventThread shut down for session: 0x10000666c260030\n",
      "24/12/20 17:28:51 INFO ZooKeeper: Session: 0x10000666c260042 closed\n",
      "24/12/20 17:28:51 INFO ClientCnxn: EventThread shut down for session: 0x10000666c260042\n",
      "24/12/20 17:28:51 INFO ZooKeeper: Session: 0x10000666c26003a closed\n",
      "24/12/20 17:28:51 INFO ClientCnxn: EventThread shut down for session: 0x10000666c26003a\n",
      "24/12/20 17:28:51 INFO ZooKeeper: Session: 0x10000666c260039 closed\n",
      "24/12/20 17:28:51 INFO ClientCnxn: EventThread shut down for session: 0x10000666c260039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==========================================>              (15 + 5) / 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/20 17:28:51 INFO ZooKeeper: Session: 0x10000666c26003f closed\n",
      "24/12/20 17:28:51 INFO ClientCnxn: EventThread shut down for session: 0x10000666c26003f\n",
      "24/12/20 17:28:51 INFO ZooKeeper: Session: 0x10000666c260034 closed\n",
      "24/12/20 17:28:51 INFO ClientCnxn: EventThread shut down for session: 0x10000666c260034\n",
      "24/12/20 17:28:51 INFO ZooKeeper: Session: 0x10000666c260036 closed\n",
      "24/12/20 17:28:51 INFO ClientCnxn: EventThread shut down for session: 0x10000666c260036\n",
      "24/12/20 17:28:51 INFO ZooKeeper: Session: 0x10000666c260031 closed\n",
      "24/12/20 17:28:51 INFO ClientCnxn: EventThread shut down for session: 0x10000666c260031\n",
      "24/12/20 17:28:51 INFO ZooKeeper: Session: 0x10000666c260040 closed\n",
      "24/12/20 17:28:51 INFO ClientCnxn: EventThread shut down for session: 0x10000666c260040\n",
      "24/12/20 17:28:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000003_0\n",
      "24/12/20 17:28:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000007_0\n",
      "24/12/20 17:28:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000011_0\n",
      "24/12/20 17:28:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000019_0\n",
      "24/12/20 17:28:51 INFO SparkHadoopMapRedUtil: No need to commit output of task because needsTaskCommit=false: attempt_202412201728505790436547717127108_0011_m_000015_0\n",
      "24/12/20 17:28:51 INFO Executor: Finished task 11.0 in stage 3.0 (TID 31). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:51 INFO Executor: Finished task 15.0 in stage 3.0 (TID 35). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:51 INFO Executor: Finished task 19.0 in stage 3.0 (TID 39). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:51 INFO Executor: Finished task 7.0 in stage 3.0 (TID 27). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:51 INFO Executor: Finished task 3.0 in stage 3.0 (TID 23). 1938 bytes result sent to driver\n",
      "24/12/20 17:28:51 INFO TaskSetManager: Finished task 11.0 in stage 3.0 (TID 31) in 1212 ms on hadoop113 (executor driver) (16/20)\n",
      "24/12/20 17:28:51 INFO TaskSetManager: Finished task 19.0 in stage 3.0 (TID 39) in 1211 ms on hadoop113 (executor driver) (17/20)\n",
      "24/12/20 17:28:51 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 23) in 1214 ms on hadoop113 (executor driver) (18/20)\n",
      "24/12/20 17:28:51 INFO TaskSetManager: Finished task 15.0 in stage 3.0 (TID 35) in 1213 ms on hadoop113 (executor driver) (19/20)\n",
      "24/12/20 17:28:51 INFO TaskSetManager: Finished task 7.0 in stage 3.0 (TID 27) in 1216 ms on hadoop113 (executor driver) (20/20)\n",
      "24/12/20 17:28:51 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/12/20 17:28:51 INFO DAGScheduler: ResultStage 3 (runJob at SparkHadoopWriter.scala:83) finished in 1.269 s\n",
      "24/12/20 17:28:51 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/20 17:28:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/12/20 17:28:51 INFO DAGScheduler: Job 3 finished: runJob at SparkHadoopWriter.scala:83, took 1.274766 s\n",
      "24/12/20 17:28:51 INFO SparkHadoopWriter: Start to commit write Job job_202412201728505790436547717127108_0011.\n",
      "24/12/20 17:28:51 WARN FileOutputCommitter: Output Path is null in commitJob()\n",
      "24/12/20 17:28:51 INFO SparkHadoopWriter: Write Job job_202412201728505790436547717127108_0011 committed. Elapsed time: 0 ms.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id STRING :key, name STRING cf:name, city STRING cf:city\n"
     ]
    }
   ],
   "source": [
    "data = [('2','Telangana','Chhetri'),\n",
    "  ('3','Sikkim','Bhutia'),\n",
    "  ('4','Hyderabad','Shabbir'),\n",
    "  ('5','Kerala','Vijayan'),\n",
    "  ('6','Mizoram','Lalpekhlua')\n",
    "  ]\n",
    "columns = [\"id\",\"city\",\"name\"]\n",
    "goalsDF = spark.createDataFrame(data=data, schema = columns)\n",
    "goalsDF.show()\n",
    "\n",
    "# hdfs \n",
    "# goalsDF.write.mode(\"overwrite\").parquet(\"hdfs://hadoop:9000/tmp/test\")\n",
    "# spark.read.parquet(\"hdfs://hadoop:9000/tmp/test\").show()\n",
    "\n",
    "#write to hbase\n",
    "\n",
    "# hbase:001:0> create 'testspark', 'cf'\n",
    "# Created table testspark\n",
    "# Took 2.3158 seconds                                                                                                                                                                           \n",
    "# => Hbase::Table - testspark\n",
    "# hbase:003:0> scan 'testspark'\n",
    "\n",
    "# sao k co ip hbase ma no van chay dc k nhi :D\n",
    "# .option(\"hbase.zookeeper.quorum\", \"localhost:2182\") \\\n",
    "\n",
    "goalsDF.write\\\n",
    ".format(\"org.apache.hadoop.hbase.spark\")\\\n",
    ".option(\"hbase.columns.mapping\",\"id STRING :key, name STRING cf:name, city STRING cf:city\")\\\n",
    ".option(\"hbase.namespace\", \"default\")\\\n",
    ".option(\"hbase.table\", \"testspark\")\\\n",
    ".option(\"hbase.spark.use.hbasecontext\", False)\\\n",
    ".save() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f2f443-3791-4731-86a6-d71ffce4fbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/20 17:28:51 INFO PlanUtils: apply method failed with\n",
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1526)\n",
      "\tat org.apache.hadoop.hbase.spark.HBaseContext.<init>(HBaseContext.scala:71)\n",
      "\tat org.apache.hadoop.hbase.spark.HBaseRelation.<init>(DefaultSource.scala:171)\n",
      "\tat org.apache.hadoop.hbase.spark.DefaultSource.createRelation(DefaultSource.scala:75)\n",
      "\tat io.openlineage.spark.agent.lifecycle.plan.SaveIntoDataSourceCommandVisitor.apply(SaveIntoDataSourceCommandVisitor.java:152)\n",
      "\tat io.openlineage.spark.agent.lifecycle.plan.SaveIntoDataSourceCommandVisitor.apply(SaveIntoDataSourceCommandVisitor.java:53)\n",
      "\tat io.openlineage.spark.api.AbstractQueryPlanDatasetBuilder$1.apply(AbstractQueryPlanDatasetBuilder.java:97)\n",
      "\tat io.openlineage.spark.api.AbstractQueryPlanDatasetBuilder$1.apply(AbstractQueryPlanDatasetBuilder.java:86)\n",
      "\tat io.openlineage.spark.agent.util.PlanUtils.safeApply(PlanUtils.java:295)\n",
      "\tat io.openlineage.spark.api.AbstractQueryPlanDatasetBuilder.lambda$apply$0(AbstractQueryPlanDatasetBuilder.java:76)\n",
      "\tat java.util.Optional.map(Optional.java:215)\n",
      "\tat io.openlineage.spark.api.AbstractQueryPlanDatasetBuilder.apply(AbstractQueryPlanDatasetBuilder.java:68)\n",
      "\tat io.openlineage.spark.api.AbstractQueryPlanDatasetBuilder.apply(AbstractQueryPlanDatasetBuilder.java:40)\n",
      "\tat io.openlineage.spark.agent.util.PlanUtils.safeApply(PlanUtils.java:295)\n",
      "\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.lambda$null$30(OpenLineageRunEventBuilder.java:553)\n",
      "\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)\n",
      "\tat java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)\n",
      "\tat java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)\n",
      "\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\n",
      "\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\n",
      "\tat java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)\n",
      "\tat java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)\n",
      "\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n",
      "\tat java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)\n",
      "\tat java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)\n",
      "\tat java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)\n",
      "\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\n",
      "\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\n",
      "\tat java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:313)\n",
      "\tat java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:742)\n",
      "\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\n",
      "\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\n",
      "\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)\n",
      "\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n",
      "\tat java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)\n",
      "\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.buildOutputDatasets(OpenLineageRunEventBuilder.java:507)\n",
      "\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.populateRun(OpenLineageRunEventBuilder.java:373)\n",
      "\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.buildRun(OpenLineageRunEventBuilder.java:355)\n",
      "\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.buildRun(OpenLineageRunEventBuilder.java:297)\n",
      "\tat io.openlineage.spark.agent.lifecycle.SparkSQLExecutionContext.end(SparkSQLExecutionContext.java:263)\n",
      "\tat io.openlineage.spark.agent.OpenLineageSparkListener.lambda$onJobEnd$15(OpenLineageSparkListener.java:208)\n",
      "\tat io.openlineage.client.circuitBreaker.NoOpCircuitBreaker.run(NoOpCircuitBreaker.java:27)\n",
      "\tat io.openlineage.spark.agent.OpenLineageSparkListener.onJobEnd(OpenLineageSparkListener.java:205)\n",
      "\tat datahub.spark.DatahubSparkListener.onJobEnd(DatahubSparkListener.java:214)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "24/12/20 17:28:51 INFO SparkUI: Stopped Spark web UI at http://hadoop113:4040\n",
      "24/12/20 17:28:51 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-20T10:28:51.618Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"0193e39c-e360-7099-abfc-6d471184fdb3\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:51 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@92da341, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:51 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@92da341, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:51 INFO OpenLineageToDataHub: alo alaoala io.openlineage.client.OpenLineage$RunEvent@3fcc9b17\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690531618}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690531646}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734690531618, status=STARTED}, dataProcessInstanceProperties={name=0193e39c-e360-7099-abfc-6d471184fdb3, created={actor=urn:li:corpuser:datahub, time=1734690531618}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e360-7099-abfc-6d471184fdb3, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690531618, eventFormatter=datahub.event.EventFormatter@69de73f0), from {\"eventTime\":\"2024-12-20T10:28:51.618Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"RUNNING\",\"run\":{\"runId\":\"0193e39c-e360-7099-abfc-6d471184fdb3\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Collecting lineage successfully in 4 ms\n",
      "id STRING :key, name STRING cf:name, city STRING cf:city\n",
      "24/12/20 17:28:51 INFO PlanUtils: apply method failed with\n",
      "java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1526)\n",
      "\tat org.apache.hadoop.hbase.spark.HBaseContext.<init>(HBaseContext.scala:71)\n",
      "\tat org.apache.hadoop.hbase.spark.HBaseRelation.<init>(DefaultSource.scala:171)\n",
      "\tat org.apache.hadoop.hbase.spark.DefaultSource.createRelation(DefaultSource.scala:75)\n",
      "\tat io.openlineage.spark.agent.lifecycle.plan.SaveIntoDataSourceCommandVisitor.apply(SaveIntoDataSourceCommandVisitor.java:152)\n",
      "\tat io.openlineage.spark.agent.lifecycle.plan.SaveIntoDataSourceCommandVisitor.apply(SaveIntoDataSourceCommandVisitor.java:53)\n",
      "\tat io.openlineage.spark.api.AbstractQueryPlanDatasetBuilder$1.apply(AbstractQueryPlanDatasetBuilder.java:97)\n",
      "\tat io.openlineage.spark.api.AbstractQueryPlanDatasetBuilder$1.apply(AbstractQueryPlanDatasetBuilder.java:86)\n",
      "\tat io.openlineage.spark.agent.util.PlanUtils.safeApply(PlanUtils.java:295)\n",
      "\tat io.openlineage.spark.api.AbstractQueryPlanDatasetBuilder.lambda$apply$0(AbstractQueryPlanDatasetBuilder.java:76)\n",
      "\tat java.util.Optional.map(Optional.java:215)\n",
      "\tat io.openlineage.spark.api.AbstractQueryPlanDatasetBuilder.apply(AbstractQueryPlanDatasetBuilder.java:68)\n",
      "\tat io.openlineage.spark.api.AbstractQueryPlanDatasetBuilder.apply(AbstractQueryPlanDatasetBuilder.java:40)\n",
      "\tat io.openlineage.spark.agent.util.PlanUtils.safeApply(PlanUtils.java:295)\n",
      "\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.lambda$null$30(OpenLineageRunEventBuilder.java:553)\n",
      "\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)\n",
      "\tat java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)\n",
      "\tat java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)\n",
      "\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\n",
      "\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\n",
      "\tat java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150)\n",
      "\tat java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173)\n",
      "\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n",
      "\tat java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:485)\n",
      "\tat java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:272)\n",
      "\tat java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)\n",
      "\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\n",
      "\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\n",
      "\tat java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:313)\n",
      "\tat java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:742)\n",
      "\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\n",
      "\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\n",
      "\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)\n",
      "\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n",
      "\tat java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)\n",
      "\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.buildOutputDatasets(OpenLineageRunEventBuilder.java:507)\n",
      "\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.populateRun(OpenLineageRunEventBuilder.java:373)\n",
      "\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.buildRun(OpenLineageRunEventBuilder.java:355)\n",
      "\tat io.openlineage.spark.agent.lifecycle.OpenLineageRunEventBuilder.buildRun(OpenLineageRunEventBuilder.java:265)\n",
      "\tat io.openlineage.spark.agent.lifecycle.SparkSQLExecutionContext.end(SparkSQLExecutionContext.java:129)\n",
      "\tat io.openlineage.spark.agent.OpenLineageSparkListener.lambda$sparkSQLExecEnd$2(OpenLineageSparkListener.java:121)\n",
      "\tat io.openlineage.client.circuitBreaker.NoOpCircuitBreaker.run(NoOpCircuitBreaker.java:27)\n",
      "\tat io.openlineage.spark.agent.OpenLineageSparkListener.sparkSQLExecEnd(OpenLineageSparkListener.java:119)\n",
      "\tat io.openlineage.spark.agent.OpenLineageSparkListener.onOtherEvent(OpenLineageSparkListener.java:96)\n",
      "\tat datahub.spark.DatahubSparkListener.onOtherEvent(DatahubSparkListener.java:252)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:100)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "24/12/20 17:28:51 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-20T10:28:51.622Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193e39c-e360-7099-abfc-6d471184fdb3\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:51 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@28bd01d0, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:51 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@28bd01d0, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:51 INFO OpenLineageToDataHub: alo alaoala io.openlineage.client.OpenLineage$RunEvent@5bd0d4ac\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690531622}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690531664}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690531622, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e360-7099-abfc-6d471184fdb3, created={actor=urn:li:corpuser:datahub, time=1734690531622}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e360-7099-abfc-6d471184fdb3, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690531622, eventFormatter=datahub.event.EventFormatter@71abbf94), from {\"eventTime\":\"2024-12-20T10:28:51.622Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193e39c-e360-7099-abfc-6d471184fdb3\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11.execute_save_into_data_source_command.testspark\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Collecting lineage successfully in 1 ms\n",
      "24/12/20 17:28:51 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/20 17:28:51 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-20T10:28:51.633Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\",\"facets\":{\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"NONE\",\"integration\":\"SPARK\",\"jobType\":\"APPLICATION\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:51 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@2145fbb7, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:51 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@2145fbb7, Properties: {spark.master=local[*], spark.app.name=WriteToHbase11}\n",
      "24/12/20 17:28:51 INFO OpenLineageToDataHub: alo alaoala io.openlineage.client.OpenLineage$RunEvent@c5f075e\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690531633}, name=write_to_hbase11, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690531668}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690531633, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e16a-7bba-aca5-d7128b9085ec, created={actor=urn:li:corpuser:datahub, time=1734690531633}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16a-7bba-aca5-d7128b9085ec, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690531633, eventFormatter=datahub.event.EventFormatter@57d84744), from {\"eventTime\":\"2024-12-20T10:28:51.633Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193e39c-e16a-7bba-aca5-d7128b9085ec\",\"facets\":{\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToHbase11\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_hbase11\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"NONE\",\"integration\":\"SPARK\",\"jobType\":\"APPLICATION\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Collecting lineage successfully in 2 ms\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Merging job stored job DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), jobInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529244}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690529750}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734690529244, status=STARTED}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690529244}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690529244, eventFormatter=datahub.event.EventFormatter@16baf7bc) with DatahubJob(flowUrn=null, dataFlowInfo=null, jobUrn=null, jobInfo=null, flowOwnership=null, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent=null, dataProcessInstanceProperties=null, dataProcessInstanceRelationships=null, dataProcessInstanceUrn=null, inSet=[], outSet=[], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=0, eventFormatter=datahub.event.EventFormatter@193e4740)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {timestampMillis=1734690529244, status=STARTED}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: null\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Merging job stored job DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529845}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690529852}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690529845}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690529845, eventFormatter=datahub.event.EventFormatter@410d6341) with DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], jobId=0, processingEngineVersion=3.3.1, appId=local-1734690526732, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, startedAt=2024-12-20T10:28:46.185Z, sparkUser=root, processingEngine=spark, finishedAt=2024-12-20T10:28:51.671Z}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), jobInfo={customProperties={spark.master=local[*], jobId=0, processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529244}, name=WriteToHbase11, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership=null, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent=null, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690529244}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690529244, eventFormatter=datahub.event.EventFormatter@193e4740)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Merging job stored job DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={jobId=1, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), jobInfo={customProperties={jobId=1, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529855}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690529860}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734690529855, status=STARTED}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690529855}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690529855, eventFormatter=datahub.event.EventFormatter@42958b22) with DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, appId=local-1734690526732, hbaseTable=write_to_hbase11.collect_limit, startedAt=2024-12-20T10:28:46.185Z, spark.app.name=WriteToHbase11, sparkUser=root, processingEngine=spark, finishedAt=2024-12-20T10:28:51.671Z}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), jobInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529244}, name=WriteToHbase11, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership=null, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690529845}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690529845, eventFormatter=datahub.event.EventFormatter@193e4740)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {timestampMillis=1734690529855, status=STARTED}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Merging job stored job DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={jobId=2, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), jobInfo={customProperties={jobId=2, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529950}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690529957}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734690529950, status=STARTED}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690529950}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690529950, eventFormatter=datahub.event.EventFormatter@3e0de9a5) with DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], jobId=0, processingEngineVersion=3.3.1, appId=local-1734690526732, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, startedAt=2024-12-20T10:28:46.185Z, sparkUser=root, processingEngine=spark, finishedAt=2024-12-20T10:28:51.671Z}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), jobInfo={customProperties={spark.master=local[*], jobId=0, processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529244}, name=WriteToHbase11, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership=null, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690529855}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690529855, eventFormatter=datahub.event.EventFormatter@193e4740)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {timestampMillis=1734690529950, status=STARTED}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Merging job stored job DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690530084}, name=collect_limit, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690530088}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690530084, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690530084}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.collect_limit), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690530084, eventFormatter=datahub.event.EventFormatter@69d7d4e4) with DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, appId=local-1734690526732, hbaseTable=write_to_hbase11.collect_limit, startedAt=2024-12-20T10:28:46.185Z, spark.app.name=WriteToHbase11, sparkUser=root, processingEngine=spark, finishedAt=2024-12-20T10:28:51.671Z}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), jobInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529244}, name=WriteToHbase11, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership=null, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690529950}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690529950, eventFormatter=datahub.event.EventFormatter@193e4740)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690530084, status=COMPLETE}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Merging job stored job DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690530143}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690530273}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734690530143, status=STARTED}, dataProcessInstanceProperties={name=0193e39c-e360-7099-abfc-6d471184fdb3, created={actor=urn:li:corpuser:datahub, time=1734690530143}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e360-7099-abfc-6d471184fdb3, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690530143, eventFormatter=datahub.event.EventFormatter@412ba378) with DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], jobId=0, processingEngineVersion=3.3.1, appId=local-1734690526732, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, startedAt=2024-12-20T10:28:46.185Z, sparkUser=root, processingEngine=spark, finishedAt=2024-12-20T10:28:51.672Z}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), jobInfo={customProperties={spark.master=local[*], jobId=0, processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529244}, name=WriteToHbase11, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership=null, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e16d-7a25-9008-40559eacb573, created={actor=urn:li:corpuser:datahub, time=1734690530084}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690530084, eventFormatter=datahub.event.EventFormatter@193e4740)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {timestampMillis=1734690530143, status=STARTED}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Merging job stored job DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={jobId=3, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), jobInfo={customProperties={jobId=3, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690530344}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690530403}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734690530344, status=STARTED}, dataProcessInstanceProperties={name=0193e39c-e360-7099-abfc-6d471184fdb3, created={actor=urn:li:corpuser:datahub, time=1734690530344}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e360-7099-abfc-6d471184fdb3, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690530344, eventFormatter=datahub.event.EventFormatter@58afb3d0) with DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, appId=local-1734690526732, hbaseTable=write_to_hbase11.collect_limit, startedAt=2024-12-20T10:28:46.185Z, spark.app.name=WriteToHbase11, sparkUser=root, processingEngine=spark, finishedAt=2024-12-20T10:28:51.672Z}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), jobInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529244}, name=WriteToHbase11, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership=null, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e360-7099-abfc-6d471184fdb3, created={actor=urn:li:corpuser:datahub, time=1734690530143}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690530143, eventFormatter=datahub.event.EventFormatter@193e4740)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {timestampMillis=1734690530344, status=STARTED}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Merging job stored job DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690531618}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690531646}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={timestampMillis=1734690531618, status=STARTED}, dataProcessInstanceProperties={name=0193e39c-e360-7099-abfc-6d471184fdb3, created={actor=urn:li:corpuser:datahub, time=1734690531618}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e360-7099-abfc-6d471184fdb3, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690531618, eventFormatter=datahub.event.EventFormatter@69de73f0) with DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], jobId=0, processingEngineVersion=3.3.1, appId=local-1734690526732, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, startedAt=2024-12-20T10:28:46.185Z, sparkUser=root, processingEngine=spark, finishedAt=2024-12-20T10:28:51.672Z}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), jobInfo={customProperties={spark.master=local[*], jobId=0, processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529244}, name=WriteToHbase11, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership=null, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e360-7099-abfc-6d471184fdb3, created={actor=urn:li:corpuser:datahub, time=1734690530344}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690530344, eventFormatter=datahub.event.EventFormatter@193e4740)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {timestampMillis=1734690531618, status=STARTED}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Merging job stored job DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.execute_save_into_data_source_command.testspark, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690531622}, name=execute_save_into_data_source_command.testspark, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690531664}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690531622, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e360-7099-abfc-6d471184fdb3, created={actor=urn:li:corpuser:datahub, time=1734690531622}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11.execute_save_into_data_source_command.testspark), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e360-7099-abfc-6d471184fdb3, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690531622, eventFormatter=datahub.event.EventFormatter@71abbf94) with DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, appId=local-1734690526732, hbaseTable=write_to_hbase11.collect_limit, startedAt=2024-12-20T10:28:46.185Z, spark.app.name=WriteToHbase11, sparkUser=root, processingEngine=spark, finishedAt=2024-12-20T10:28:51.672Z}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), jobInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529244}, name=WriteToHbase11, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership=null, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e360-7099-abfc-6d471184fdb3, created={actor=urn:li:corpuser:datahub, time=1734690531618}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690531618, eventFormatter=datahub.event.EventFormatter@193e4740)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690531622, status=COMPLETE}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Merging job stored job DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11, spark.app.name=WriteToHbase11, processingEngine=spark}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690531633}, name=write_to_hbase11, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734690531668}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690531633, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e16a-7bba-aca5-d7128b9085ec, created={actor=urn:li:corpuser:datahub, time=1734690531633}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),write_to_hbase11), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16a-7bba-aca5-d7128b9085ec, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690531633, eventFormatter=datahub.event.EventFormatter@57d84744) with DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={spark.master=local[*], jobId=0, processingEngineVersion=3.3.1, appId=local-1734690526732, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, startedAt=2024-12-20T10:28:46.185Z, sparkUser=root, processingEngine=spark, finishedAt=2024-12-20T10:28:51.673Z}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), jobInfo={customProperties={spark.master=local[*], jobId=0, processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529244}, name=WriteToHbase11, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership=null, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e360-7099-abfc-6d471184fdb3, created={actor=urn:li:corpuser:datahub, time=1734690531622}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690531622, eventFormatter=datahub.event.EventFormatter@193e4740)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690531633, status=COMPLETE}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: DataProcessInstanceRunEvent: {result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Generating MCPs for job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), dataFlowInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, appId=local-1734690526732, hbaseTable=write_to_hbase11.collect_limit, startedAt=2024-12-20T10:28:46.185Z, spark.app.name=WriteToHbase11, sparkUser=root, processingEngine=spark, finishedAt=2024-12-20T10:28:51.673Z}, name=WriteToHbase11}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), jobInfo={customProperties={jobId=0, spark.master=local[*], processingEngineVersion=3.3.1, hbaseTable=write_to_hbase11.collect_limit, spark.app.name=WriteToHbase11, processingEngine=spark}, created={time=1734690529244}, name=WriteToHbase11, flowUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), type={string=spark}}, flowOwnership=null, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734690529845, status=COMPLETE}, dataProcessInstanceProperties={name=0193e39c-e16a-7bba-aca5-d7128b9085ec, created={actor=urn:li:corpuser:datahub, time=1734690531633}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), schemaMetadata=null, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734690531633, eventFormatter=datahub.event.EventFormatter@193e4740)\n",
      "24/12/20 17:28:51 INFO DatahubJob: Generating MCPs for job: urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11)\n",
      "24/12/20 17:28:51 INFO ZooKeeper: Session: 0x10000666c260032 closed\n",
      "24/12/20 17:28:51 INFO ClientCnxn: EventThread shut down for session: 0x10000666c260032\n",
      "24/12/20 17:28:51 INFO DatahubJob: Setting custom properties for job: urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11)\n",
      "24/12/20 17:28:51 INFO DatahubJob: Adding input and output to urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11) Number of outputs: 1, Number of inputs 0\n",
      "24/12/20 17:28:51 INFO DatahubJob: Adding DataJob edges to urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11)\n",
      "24/12/20 17:28:51 INFO DatahubJob: Adding input data jobs urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11) Number of jobs: 0\n",
      "24/12/20 17:28:51 INFO DatahubJob: Adding dataProcessInstanceProperties to urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11)\n",
      "24/12/20 17:28:51 INFO DatahubJob: Adding dataProcessInstanceRunEvent to urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11)\n",
      "24/12/20 17:28:51 INFO DatahubJob: Adding dataProcessInstanceRelationships to urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11)\n",
      "24/12/20 17:28:51 INFO DatahubJob: Mcp generation finished for urn urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: Emitting Coalesced lineage completed successfully\n",
      "24/12/20 17:28:51 INFO ZooKeeper: Session: 0x10000666c26003e closed\n",
      "24/12/20 17:28:51 INFO ClientCnxn: EventThread shut down for session: 0x10000666c26003e\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: emitting mcpw: {aspectName=dataFlowInfo, entityUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), entityType=dataFlow, changeType=UPSERT, aspect={contentType=application/json, value=ByteString(length=353,bytes=7b226375...3131227d)}}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: emitting mcpw: {aspectName=status, entityUrn=urn:li:dataFlow:(spark,WriteToHbase11,default), entityType=dataFlow, changeType=UPSERT, aspect={contentType=application/json, value=ByteString(length=17,bytes=7b227265...6c73657d)}}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: emitting mcpw: {aspectName=dataJobInfo, entityUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), entityType=dataJob, changeType=UPSERT, aspect={contentType=application/json, value=ByteString(length=343,bytes=7b226375...6b227d7d)}}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: emitting mcpw: {aspectName=status, entityUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), entityType=dataJob, changeType=UPSERT, aspect={contentType=application/json, value=ByteString(length=17,bytes=7b227265...6c73657d)}}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: emitting mcpw: {aspectName=datasetKey, entityUrn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), entityType=dataset, changeType=UPSERT, aspect={contentType=application/json, value=ByteString(length=80,bytes=7b226e61...4f44227d)}}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: emitting mcpw: {aspectName=status, entityUrn=urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD), entityType=dataset, changeType=UPSERT, aspect={contentType=application/json, value=ByteString(length=17,bytes=7b227265...6c73657d)}}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: emitting mcpw: {aspectName=dataJobInputOutput, entityUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11), entityType=dataJob, changeType=UPSERT, aspect={contentType=application/json, value=ByteString(length=261,bytes=7b22696e...3a5b5d7d)}}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: emitting mcpw: {aspectName=dataProcessInstanceInput, entityUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, entityType=dataProcessInstance, changeType=UPSERT, aspect={contentType=application/json, value=ByteString(length=13,bytes=7b22696e...3a5b5d7d)}}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: emitting mcpw: {aspectName=dataProcessInstanceOutput, entityUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, entityType=dataProcessInstance, changeType=UPSERT, aspect={contentType=application/json, value=ByteString(length=78,bytes=7b226f75...29225d7d)}}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: emitting mcpw: {aspectName=dataProcessInstanceProperties, entityUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, entityType=dataProcessInstance, changeType=UPSERT, aspect={contentType=application/json, value=ByteString(length=114,bytes=7b226e61...33337d7d)}}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: emitting mcpw: {aspectName=dataProcessInstanceRunEvent, entityUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, entityType=dataProcessInstance, changeType=UPSERT, aspect={contentType=application/json, value=ByteString(length=111,bytes=7b227265...5445227d)}}\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: emitting mcpw: {aspectName=dataProcessInstanceRelationships, entityUrn=urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573, entityType=dataProcessInstance, changeType=UPSERT, aspect={contentType=application/json, value=ByteString(length=122,bytes=7b227061...3a5b5d7d)}}\n",
      "24/12/20 17:28:51 INFO ZooKeeper: Session: 0x10000666c260035 closed\n",
      "24/12/20 17:28:51 INFO ClientCnxn: EventThread shut down for session: 0x10000666c260035\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: MetadataWriteResponse(success=true, responseContent={\"value\":\"urn:li:dataFlow:(spark,WriteToHbase11,default)\"}, underlyingResponse=200 null HTTP/1.1)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: MetadataWriteResponse(success=true, responseContent={\"value\":\"urn:li:dataFlow:(spark,WriteToHbase11,default)\"}, underlyingResponse=200 null HTTP/1.1)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: MetadataWriteResponse(success=true, responseContent={\"value\":\"urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11)\"}, underlyingResponse=200 null HTTP/1.1)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: MetadataWriteResponse(success=true, responseContent={\"value\":\"urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11)\"}, underlyingResponse=200 null HTTP/1.1)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: MetadataWriteResponse(success=true, responseContent={\"value\":\"urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD)\"}, underlyingResponse=200 null HTTP/1.1)\n",
      "24/12/20 17:28:51 INFO DatahubEventEmitter: MetadataWriteResponse(success=true, responseContent={\"value\":\"urn:li:dataset:(urn:li:dataPlatform:hdfs,/tmp/test111111,PROD)\"}, underlyingResponse=200 null HTTP/1.1)\n",
      "24/12/20 17:28:52 INFO DatahubEventEmitter: MetadataWriteResponse(success=true, responseContent={\"value\":\"urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToHbase11,default),WriteToHbase11)\"}, underlyingResponse=200 null HTTP/1.1)\n",
      "24/12/20 17:28:52 INFO DatahubEventEmitter: MetadataWriteResponse(success=true, responseContent={\"value\":\"urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573\"}, underlyingResponse=200 null HTTP/1.1)\n",
      "24/12/20 17:28:52 INFO DatahubEventEmitter: MetadataWriteResponse(success=true, responseContent={\"value\":\"urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573\"}, underlyingResponse=200 null HTTP/1.1)\n",
      "24/12/20 17:28:52 INFO DatahubEventEmitter: MetadataWriteResponse(success=true, responseContent={\"value\":\"urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573\"}, underlyingResponse=200 null HTTP/1.1)\n",
      "24/12/20 17:28:52 INFO DatahubEventEmitter: MetadataWriteResponse(success=true, responseContent={\"value\":\"urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573\"}, underlyingResponse=200 null HTTP/1.1)\n",
      "24/12/20 17:28:52 INFO DatahubEventEmitter: MetadataWriteResponse(success=true, responseContent={\"value\":\"urn:li:dataProcessInstance:0193e39c-e16d-7a25-9008-40559eacb573\"}, underlyingResponse=200 null HTTP/1.1)\n",
      "24/12/20 17:28:52 INFO DatahubEventEmitter: Emitting coalesced lineage completed in 355 ms\n",
      "24/12/20 17:28:52 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/12/20 17:28:52 INFO MemoryStore: MemoryStore cleared\n",
      "24/12/20 17:28:52 INFO BlockManager: BlockManager stopped\n",
      "24/12/20 17:28:52 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/12/20 17:28:52 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/12/20 17:28:52 INFO ZooKeeper: Session: 0x10000666c26003c closed\n",
      "24/12/20 17:28:52 INFO ClientCnxn: EventThread shut down for session: 0x10000666c26003c\n",
      "24/12/20 17:28:52 INFO SparkContext: Successfully stopped SparkContext\n",
      "24/12/20 17:28:52 INFO ClientCnxn: EventThread shut down for session: 0x10000666c26002f\n",
      "24/12/20 17:28:52 INFO ZooKeeper: Session: 0x10000666c26002f closed\n",
      "24/12/20 17:28:52 INFO ZooKeeper: Session: 0x10000666c260041 closed\n",
      "24/12/20 17:28:52 INFO ClientCnxn: EventThread shut down for session: 0x10000666c260041\n",
      "24/12/20 17:28:52 INFO ZooKeeper: Session: 0x10000666c260037 closed\n",
      "24/12/20 17:28:52 INFO ClientCnxn: EventThread shut down for session: 0x10000666c260037\n",
      "24/12/20 17:28:52 INFO ZooKeeper: Session: 0x10000666c26003d closed\n",
      "24/12/20 17:28:52 INFO ClientCnxn: EventThread shut down for session: 0x10000666c26003d\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
