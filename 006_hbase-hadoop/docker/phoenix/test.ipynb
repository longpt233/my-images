{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "\u001b[K     |████████▏                       | 80.8 MB 194 kB/s eta 0:20:16^C\n",
      "\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# !pip install --proxy http://10.208.164.185:9999 pyspark\n",
    "# !pip install --proxy http://10.208.164.185:9999 -U ipykernel # để bên ngoài connect vào trong dc notebook (từ vscode)\n",
    "\n",
    "# RUN pip3 install --proxy http://10.60.117.103:8085 jupyterlab\n",
    "# pip install jupyterlab\n",
    "# nohup python3 -m jupyterlab --ip=0.0.0.0 --port=18080 --NotebookApp.token=1111 --allow-root > /dev/null &\n",
    "\n",
    "# cài pyspark mới nhất lỗi gì ấy quên rồi hic (hình như là scala )\n",
    "# RUN pip3 install --proxy http://10.60.117.103:8085 pyspark==3.3.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghi vao mysql cua datahub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "config_datahub = {'spark.extraListeners': 'datahub.spark.DatahubSparkListener',\n",
    "                  '\\\"spark.datahub.rest.server': 'http://10.208.164.167:8080\\\"',\n",
    "                  'spark.datahub.metadata.dataset.materialize': 'true',  # hien hdfs\n",
    "                  '\\\"spark.datahub.file_partition_regexp': '/partition=.*|/_delta_log.*\\\"',\n",
    "                  'spark.datahub.metadata.dataset.experimental_include_schema_metadata': 'true'}   # ten cot, thogn tin cot\n",
    "\n",
    "# một task spark sẽ đi kèm với 1 pipeline, tức là có 10job thì sẽ có 20task spark và 10 pipeline\n",
    "# Tạo SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"WriteToPhoenix\") \\\n",
    "    .config(\"spark.jars\", \"/tmp/phoenix5-spark3-6.0.0.7.2.17.0-334.jar,/tmp/phoenix-hbase-compat-2.4.1-5.1.3.jar,\\\n",
    "    /usr/local/phoenix/phoenix-client-hbase-2.4-5.1.3.jar,/tmp/phoenix-core-5.1.3.jar,/tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar\") \\\n",
    "        .config(\"spark.extraListeners\", \"datahub.spark.DatahubSparkListener\")\\\n",
    "        .config(\"spark.datahub.rest.server\", \"http://10.208.164.167:8080\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Tạo DataFrame mẫu\n",
    "data = [\n",
    "    (1, \"John Doe\", 25),\n",
    "    (2, \"Jane Smith\", 30),\n",
    "    (3, \"Alice Johnson\", 22)\n",
    "]\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình kết nối Phoenix\n",
    "phoenix_url = \"jdbc:phoenix:localhost\"  # Thay đổi 'localhost' thành địa chỉ ZooKeeper của bạn\n",
    "table_name = \"USERS\"\n",
    "\n",
    "# Ghi DataFrame vào Phoenix\n",
    "df.write \\\n",
    "    .format(\"org.apache.phoenix.spark\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"zkUrl\", phoenix_url) \\\n",
    "    .option(\"table\", table_name) \\\n",
    "    .save()\n",
    "\n",
    "print(\"Data written to Phoenix successfully!\")\n",
    "\n",
    "# Dừng SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# RUN pip3 install --proxy http://10.60.117.103:8085 pyspark==3.3.1 -> spak 3.5 van dang loi\n",
    "\n",
    "# Initialize Spark session with HBase configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WriteToHBase\") \\\n",
    "    .config(\"spark.jars\", \"/tmp/hbase-spark-1.0.1-SNAPSHOT_spark331_hbase2415.jar,\\\n",
    "    /tmp/hbase-spark-protocol-shaded-1.0.1-SNAPSHOT_spark331_hbase2415.jar,\\\n",
    "    /tmp/hbase-shaded-mapreduce-2.4.15.jar,/hbase-site.xml.jar,/tmp/scala-parser-combinators_2.12-2.1.1.jar,/tmp/htrace-core4-4.2.0-incubating.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "    # .config(\"spark.hadoop.hbase.zookeeper.quorum\", \"localhost\") \\\n",
    "    # .config(\"spark.hadoop.hbase.zookeeper.property.clientPort\", \"2181\") \\\n",
    "    # .config(\"spark.hadoop.hbase.master\", \"localhost:16000\") \\\n",
    "    # .config(\"spark.executor.extraClassPath\", \"/usr/local/hbase/lib/\")\\\n",
    "    # .config(\"spark.driver.extraClassPath\", \"/usr/local/hbase/lib/\")\\\n",
    "# /hbase-site.xml.jar, ?????\n",
    "#     /tmp/hbase-client-2.4.14.jar,/tmp/hbase-server-2.4.14.jar,/tmp/hbase-mapreduce-2.4.15.jar\n",
    "#     /tmp/hbase-spark3-protocol-shaded-1.0.0.7.2.17.0-334.jar\n",
    "#    /tmp/phoenix5-spark3-6.0.0.7.2.17.0-334.jar,/tmp/phoenix-hbase-compat-2.4.1-5.1.3.jar,/tmp/scala-parser-combinators_2.12-2.1.1.jar,\\\n",
    "#    /usr/local/phoenix/phoenix-client-hbase-2.4-5.1.3.jar,/tmp/phoenix-core-5.1.3.jar,\\\n",
    "    # /tmp/hbase-client-2.4.14.jar,/tmp/hbase-server-2.4.14.jar,/tmp/hbase-mapreduce-2.4.15.jar, \\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
