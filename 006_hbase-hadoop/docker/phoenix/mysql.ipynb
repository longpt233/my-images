{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc63fa7-4ad7-4f6f-a9fe-fc55d45b29d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/16 10:36:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/16 10:36:47 INFO ArgumentParser: Couldn't log config from file, will read it from SparkConf\n",
      "24/12/16 10:36:47 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/16 10:36:47 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/16 10:36:47 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/16 10:36:48 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/16 10:36:48 DEBUG KafkaRelationVisitor: Kafka classes have not been checked yet\n",
      "24/12/16 10:36:48 DEBUG KafkaRelationVisitor: Kafka classes availability: false\n",
      "24/12/16 10:36:48 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/16 10:36:48 DEBUG SparkApplicationExecutionContext: SparkListenerApplicationStart - applicationId: local-1734320207265\n",
      "24/12/16 10:36:48 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional.empty with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/16 10:36:48 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional.empty with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/16 10:36:48 DEBUG FileSystem: Looking for FS supporting file\n",
      "24/12/16 10:36:48 DEBUG FileSystem: looking for configuration option fs.file.impl\n",
      "24/12/16 10:36:48 DEBUG FileSystem: Looking in service filesystems for implementation class\n",
      "24/12/16 10:36:48 DEBUG FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem\n",
      "24/12/16 10:36:48 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/12/16 10:36:48 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.datahub.metadata.dataset.experimental_include_schema_metadata -> true\n",
      "24/12/16 10:36:48 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.extraListeners -> datahub.spark.DatahubSparkListener\n",
      "24/12/16 10:36:48 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.jars -> /tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,/tmp/mysql-connector-java-8.0.29.jar\n",
      "24/12/16 10:36:48 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.app.name -> WriteToMySQL222\n",
      "24/12/16 10:36:48 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.datahub.rest.server -> http://10.208.164.167:8080\n",
      "24/12/16 10:36:48 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.datahub.metadata.dataset.materialize -> true\n",
      "24/12/16 10:36:48 INFO SharedState: Warehouse path is 'file:/spark-warehouse'.\n",
      "24/12/16 10:36:48 DEBUG FsUrlStreamHandlerFactory: Creating handler for protocol jar\n",
      "24/12/16 10:36:48 DEBUG FileSystem: Looking for FS supporting jar\n",
      "24/12/16 10:36:48 DEBUG FileSystem: looking for configuration option fs.jar.impl\n",
      "24/12/16 10:36:48 DEBUG FileSystem: Looking in service filesystems for implementation class\n",
      "24/12/16 10:36:48 DEBUG FsUrlStreamHandlerFactory: Unknown protocol jar, delegating to default implementation\n",
      "24/12/16 10:36:48 DEBUG FsUrlStreamHandlerFactory: Creating handler for protocol file\n",
      "24/12/16 10:36:48 DEBUG FileSystem: Looking for FS supporting file\n",
      "24/12/16 10:36:48 DEBUG FileSystem: looking for configuration option fs.file.impl\n",
      "24/12/16 10:36:48 DEBUG FileSystem: Looking in service filesystems for implementation class\n",
      "24/12/16 10:36:48 DEBUG FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem\n",
      "24/12/16 10:36:48 DEBUG FsUrlStreamHandlerFactory: Found implementation of file: class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem\n",
      "24/12/16 10:36:48 DEBUG FsUrlStreamHandlerFactory: Using handler for protocol file\n",
      "24/12/16 10:36:48 DEBUG SparkApplicationExecutionContext: Posting event for applicationId local-1734320207265 start: io.openlineage.client.OpenLineage$RunEvent@c4003e2\n",
      "24/12/16 10:36:48 DEBUG OpenLineageClient: OpenLineageClient will emit lineage event: {\"eventTime\":\"2024-12-16T03:36:46.67Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193cd8a-385c-73e6-9e47-b9010c063b85\",\"facets\":{\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"spark_applicationDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"master\":\"local[*]\",\"appName\":\"WriteToMySQL222\",\"applicationId\":\"local-1734320207265\",\"deployMode\":\"client\",\"driverHost\":\"hadoop113\",\"userName\":\"root\",\"uiWebUrl\":\"http://hadoop113:4040\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"NONE\",\"integration\":\"SPARK\",\"jobType\":\"APPLICATION\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/16 10:36:48 INFO ConsoleTransport: {\"eventTime\":\"2024-12-16T03:36:46.67Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193cd8a-385c-73e6-9e47-b9010c063b85\",\"facets\":{\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"spark_applicationDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"master\":\"local[*]\",\"appName\":\"WriteToMySQL222\",\"applicationId\":\"local-1734320207265\",\"deployMode\":\"client\",\"driverHost\":\"hadoop113\",\"userName\":\"root\",\"uiWebUrl\":\"http://hadoop113:4040\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"NONE\",\"integration\":\"SPARK\",\"jobType\":\"APPLICATION\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/16 10:36:48 DEBUG EventEmitter: Emitting lineage completed successfully: {\"eventTime\":\"2024-12-16T03:36:46.67Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193cd8a-385c-73e6-9e47-b9010c063b85\",\"facets\":{\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"spark_applicationDetails\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"master\":\"local[*]\",\"appName\":\"WriteToMySQL222\",\"applicationId\":\"local-1734320207265\",\"deployMode\":\"client\",\"driverHost\":\"hadoop113\",\"userName\":\"root\",\"uiWebUrl\":\"http://hadoop113:4040\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"NONE\",\"integration\":\"SPARK\",\"jobType\":\"APPLICATION\"}}},\"inputs\":[],\"outputs\":[]}\n",
      "24/12/16 10:36:48 INFO DatahubSparkListener: onApplicationStart completed successfully in 946 ms\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session with HBase configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WriteToMySQL222\") \\\n",
    "    .config(\"spark.jars\", \"/tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,/tmp/mysql-connector-java-8.0.29.jar\") \\   # chạy phát sẽ có linearg trên datahub\n",
    "    .config(\"spark.extraListeners\", \"datahub.spark.DatahubSparkListener\")\\\n",
    "    .config(\"spark.datahub.rest.server\", \"http://10.208.164.167:8080\")\\\n",
    "    .config(\"spark.datahub.metadata.dataset.materialize\", \"true\")\\\n",
    "    .config(\"spark.datahub.metadata.dataset.experimental_include_schema_metadata\", \"true\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b32905b-aa9f-4a6f-af34-222608824d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/16 10:36:53 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$pythonToJava$1\n",
      "24/12/16 10:36:53 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$pythonToJava$1) is now cleaned +++\n",
      "24/12/16 10:36:53 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$toJavaArray$1\n",
      "24/12/16 10:36:53 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$toJavaArray$1) is now cleaned +++\n",
      "24/12/16 10:36:53 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$applySchemaToPythonRDD$1\n",
      "24/12/16 10:36:53 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$applySchemaToPythonRDD$1) is now cleaned +++\n",
      "24/12/16 10:36:53 DEBUG CatalystSqlParser: Parsing command: spark_grouping_id\n",
      "24/12/16 10:36:54 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart\n",
      "24/12/16 10:36:54 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/16 10:36:54 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/16 10:36:54 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/16 10:36:54 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionStart - executionId: 0\n",
      "24/12/16 10:36:54 DEBUG JdbcOptionsInWrite: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:54 DEBUG ConnectionProvider: Loaded built-in provider: org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider@f487cea\n",
      "24/12/16 10:36:54 DEBUG ConnectionProvider: Loaded built-in provider: org.apache.spark.sql.execution.datasources.jdbc.connection.DB2ConnectionProvider@afed97f\n",
      "24/12/16 10:36:54 DEBUG ConnectionProvider: Loaded built-in provider: org.apache.spark.sql.execution.datasources.jdbc.connection.MariaDBConnectionProvider@275e01d8\n",
      "24/12/16 10:36:54 DEBUG ConnectionProvider: Loaded built-in provider: org.apache.spark.sql.execution.datasources.jdbc.connection.MSSQLConnectionProvider@1f2a6042\n",
      "24/12/16 10:36:54 DEBUG ConnectionProvider: Loaded built-in provider: org.apache.spark.sql.execution.datasources.jdbc.connection.PostgresConnectionProvider@5fd2b85b\n",
      "24/12/16 10:36:54 DEBUG ConnectionProvider: Loaded built-in provider: org.apache.spark.sql.execution.datasources.jdbc.connection.OracleConnectionProvider@f351485\n",
      "24/12/16 10:36:54 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:54 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:54 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:54 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:54 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:54 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:54 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:54 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:mysql://10.208.164.167:3306/my_database and properties: {user=my_user, password=my_password}\n",
      "24/12/16 10:36:54 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"ID\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"9ebd6366-e844-4d24-9bf8-268e095f22af\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"Name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"9ebd6366-e844-4d24-9bf8-268e095f22af\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"Age\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"9ebd6366-e844-4d24-9bf8-268e095f22af\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]\n",
      "24/12/16 10:36:54 DEBUG FsUrlStreamHandlerFactory: Creating handler for protocol http\n",
      "24/12/16 10:36:54 DEBUG FsUrlStreamHandlerFactory: Unknown protocol http, delegating to default implementation\n",
      "24/12/16 10:36:55 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator rdd_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     rdd_input_0 = inputs[0];\n",
      "/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( rdd_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();\n",
      "/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 028 */       boolean rdd_isNull_0 = rdd_row_0.isNullAt(0);\n",
      "/* 029 */       long rdd_value_0 = rdd_isNull_0 ?\n",
      "/* 030 */       -1L : (rdd_row_0.getLong(0));\n",
      "/* 031 */       boolean rdd_isNull_1 = rdd_row_0.isNullAt(1);\n",
      "/* 032 */       UTF8String rdd_value_1 = rdd_isNull_1 ?\n",
      "/* 033 */       null : (rdd_row_0.getUTF8String(1));\n",
      "/* 034 */       boolean rdd_isNull_2 = rdd_row_0.isNullAt(2);\n",
      "/* 035 */       long rdd_value_2 = rdd_isNull_2 ?\n",
      "/* 036 */       -1L : (rdd_row_0.getLong(2));\n",
      "/* 037 */       rdd_mutableStateArray_0[0].reset();\n",
      "/* 038 */\n",
      "/* 039 */       rdd_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 040 */\n",
      "/* 041 */       if (rdd_isNull_0) {\n",
      "/* 042 */         rdd_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 043 */       } else {\n",
      "/* 044 */         rdd_mutableStateArray_0[0].write(0, rdd_value_0);\n",
      "/* 045 */       }\n",
      "/* 046 */\n",
      "/* 047 */       if (rdd_isNull_1) {\n",
      "/* 048 */         rdd_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */       } else {\n",
      "/* 050 */         rdd_mutableStateArray_0[0].write(1, rdd_value_1);\n",
      "/* 051 */       }\n",
      "/* 052 */\n",
      "/* 053 */       if (rdd_isNull_2) {\n",
      "/* 054 */         rdd_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 055 */       } else {\n",
      "/* 056 */         rdd_mutableStateArray_0[0].write(2, rdd_value_2);\n",
      "/* 057 */       }\n",
      "/* 058 */       append((rdd_mutableStateArray_0[0].getRow()));\n",
      "/* 059 */       if (shouldStop()) return;\n",
      "/* 060 */     }\n",
      "/* 061 */   }\n",
      "/* 062 */\n",
      "/* 063 */ }\n",
      "\n",
      "24/12/16 10:36:55 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator rdd_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] rdd_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     rdd_input_0 = inputs[0];\n",
      "/* 020 */     rdd_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( rdd_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow rdd_row_0 = (InternalRow) rdd_input_0.next();\n",
      "/* 027 */       ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 028 */       boolean rdd_isNull_0 = rdd_row_0.isNullAt(0);\n",
      "/* 029 */       long rdd_value_0 = rdd_isNull_0 ?\n",
      "/* 030 */       -1L : (rdd_row_0.getLong(0));\n",
      "/* 031 */       boolean rdd_isNull_1 = rdd_row_0.isNullAt(1);\n",
      "/* 032 */       UTF8String rdd_value_1 = rdd_isNull_1 ?\n",
      "/* 033 */       null : (rdd_row_0.getUTF8String(1));\n",
      "/* 034 */       boolean rdd_isNull_2 = rdd_row_0.isNullAt(2);\n",
      "/* 035 */       long rdd_value_2 = rdd_isNull_2 ?\n",
      "/* 036 */       -1L : (rdd_row_0.getLong(2));\n",
      "/* 037 */       rdd_mutableStateArray_0[0].reset();\n",
      "/* 038 */\n",
      "/* 039 */       rdd_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 040 */\n",
      "/* 041 */       if (rdd_isNull_0) {\n",
      "/* 042 */         rdd_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 043 */       } else {\n",
      "/* 044 */         rdd_mutableStateArray_0[0].write(0, rdd_value_0);\n",
      "/* 045 */       }\n",
      "/* 046 */\n",
      "/* 047 */       if (rdd_isNull_1) {\n",
      "/* 048 */         rdd_mutableStateArray_0[0].setNullAt(1);\n",
      "/* 049 */       } else {\n",
      "/* 050 */         rdd_mutableStateArray_0[0].write(1, rdd_value_1);\n",
      "/* 051 */       }\n",
      "/* 052 */\n",
      "/* 053 */       if (rdd_isNull_2) {\n",
      "/* 054 */         rdd_mutableStateArray_0[0].setNullAt(2);\n",
      "/* 055 */       } else {\n",
      "/* 056 */         rdd_mutableStateArray_0[0].write(2, rdd_value_2);\n",
      "/* 057 */       }\n",
      "/* 058 */       append((rdd_mutableStateArray_0[0].getRow()));\n",
      "/* 059 */       if (shouldStop()) return;\n",
      "/* 060 */     }\n",
      "/* 061 */   }\n",
      "/* 062 */\n",
      "/* 063 */ }\n",
      "\n",
      "24/12/16 10:36:55 INFO CodeGenerator: Code generated in 85.745077 ms\n",
      "24/12/16 10:36:55 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "24/12/16 10:36:55 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "24/12/16 10:36:55 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$rdd$1\n",
      "24/12/16 10:36:55 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++\n",
      "24/12/16 10:36:55 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$saveTable$1$adapted\n",
      "24/12/16 10:36:55 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$saveTable$1$adapted) is now cleaned +++\n",
      "24/12/16 10:36:55 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$foreachPartition$2$adapted\n",
      "24/12/16 10:36:55 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$foreachPartition$2$adapted) is now cleaned +++\n",
      "24/12/16 10:36:55 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "24/12/16 10:36:55 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "24/12/16 10:36:55 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "24/12/16 10:36:55 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 8 took 0.000525 seconds\n",
      "24/12/16 10:36:55 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "24/12/16 10:36:55 INFO DAGScheduler: Got job 0 (save at NativeMethodAccessorImpl.java:0) with 20 output partitions\n",
      "24/12/16 10:36:55 INFO DAGScheduler: Final stage: ResultStage 0 (save at NativeMethodAccessorImpl.java:0)\n",
      "24/12/16 10:36:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/12/16 10:36:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/12/16 10:36:55 DEBUG DAGScheduler: submitStage(ResultStage 0 (name=save at NativeMethodAccessorImpl.java:0;jobs=0))\n",
      "24/12/16 10:36:55 DEBUG DAGScheduler: missing: List()\n",
      "24/12/16 10:36:55 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[8] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "24/12/16 10:36:55 DEBUG DAGScheduler: submitMissingTasks(ResultStage 0)\n",
      "24/12/16 10:36:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 26.8 KiB, free 366.3 MiB)\n",
      "24/12/16 10:36:55 DEBUG BlockManager: Put block broadcast_0 locally took 17 ms\n",
      "24/12/16 10:36:55 DEBUG BlockManager: Putting block broadcast_0 without replication took 19 ms\n",
      "24/12/16 10:36:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.1 KiB, free 366.3 MiB)\n",
      "24/12/16 10:36:55 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, hadoop113, 35617, None)\n",
      "24/12/16 10:36:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on hadoop113:35617 (size: 13.1 KiB, free: 366.3 MiB)\n",
      "24/12/16 10:36:55 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0\n",
      "24/12/16 10:36:55 DEBUG BlockManager: Told master about block broadcast_0_piece0\n",
      "24/12/16 10:36:55 DEBUG BlockManager: Put block broadcast_0_piece0 locally took 5 ms\n",
      "24/12/16 10:36:55 DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took 5 ms\n",
      "24/12/16 10:36:55 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1513\n",
      "24/12/16 10:36:55 INFO DAGScheduler: Submitting 20 missing tasks from ResultStage 0 (MapPartitionsRDD[8] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "24/12/16 10:36:55 INFO TaskSchedulerImpl: Adding task set 0.0 with 20 tasks resource profile 0\n",
      "24/12/16 10:36:55 DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0\n",
      "24/12/16 10:36:55 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "24/12/16 10:36:55 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY\n",
      "24/12/16 10:36:55 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0\n",
      "24/12/16 10:36:55 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (hadoop113, executor driver, partition 0, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (hadoop113, executor driver, partition 1, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (hadoop113, executor driver, partition 2, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (hadoop113, executor driver, partition 3, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (hadoop113, executor driver, partition 4, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (hadoop113, executor driver, partition 5, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (hadoop113, executor driver, partition 6, PROCESS_LOCAL, 4475 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (hadoop113, executor driver, partition 7, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8) (hadoop113, executor driver, partition 8, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9) (hadoop113, executor driver, partition 9, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 10.0 in stage 0.0 (TID 10) (hadoop113, executor driver, partition 10, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 11.0 in stage 0.0 (TID 11) (hadoop113, executor driver, partition 11, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 12.0 in stage 0.0 (TID 12) (hadoop113, executor driver, partition 12, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 13.0 in stage 0.0 (TID 13) (hadoop113, executor driver, partition 13, PROCESS_LOCAL, 4477 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 14.0 in stage 0.0 (TID 14) (hadoop113, executor driver, partition 14, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 15.0 in stage 0.0 (TID 15) (hadoop113, executor driver, partition 15, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 16.0 in stage 0.0 (TID 16) (hadoop113, executor driver, partition 16, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 17.0 in stage 0.0 (TID 17) (hadoop113, executor driver, partition 17, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 18.0 in stage 0.0 (TID 18) (hadoop113, executor driver, partition 18, PROCESS_LOCAL, 4433 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO TaskSetManager: Starting task 19.0 in stage 0.0 (TID 19) (hadoop113, executor driver, partition 19, PROCESS_LOCAL, 4480 bytes) taskResourceAssignments Map()\n",
      "24/12/16 10:36:55 INFO Executor: Running task 10.0 in stage 0.0 (TID 10)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 17.0 in stage 0.0 (TID 17)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 13.0 in stage 0.0 (TID 13)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 11.0 in stage 0.0 (TID 11)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 9.0 in stage 0.0 (TID 9)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 15.0 in stage 0.0 (TID 15)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 6.0 in stage 0.0 (TID 6)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 19.0 in stage 0.0 (TID 19)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 12.0 in stage 0.0 (TID 12)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 18.0 in stage 0.0 (TID 18)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 16.0 in stage 0.0 (TID 16)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 14.0 in stage 0.0 (TID 14)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 8.0 in stage 0.0 (TID 8)\n",
      "24/12/16 10:36:55 INFO Executor: Running task 7.0 in stage 0.0 (TID 7)\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 8\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 7\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 4\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 2\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 10\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 19\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 3\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 12\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 20\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 5\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 11\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 1\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 6\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 9\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 17\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 16\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 14\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 13\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 18\n",
      "24/12/16 10:36:55 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 15\n",
      "24/12/16 10:36:55 DEBUG BlockManager: Getting local block broadcast_0\n",
      "24/12/16 10:36:55 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 20) / 20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, bigint, true], input[1, string, true].toString, input[2, bigint, true], StructField(ID,LongType,true), StructField(Name,StringType,true), StructField(Age,LongType,true)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     Object[] values_0 = new Object[3];\n",
      "/* 024 */\n",
      "/* 025 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 026 */     long value_1 = isNull_1 ?\n",
      "/* 027 */     -1L : (i.getLong(0));\n",
      "/* 028 */     if (isNull_1) {\n",
      "/* 029 */       values_0[0] = null;\n",
      "/* 030 */     } else {\n",
      "/* 031 */       values_0[0] = value_1;\n",
      "/* 032 */     }\n",
      "/* 033 */\n",
      "/* 034 */     boolean isNull_3 = i.isNullAt(1);\n",
      "/* 035 */     UTF8String value_3 = isNull_3 ?\n",
      "/* 036 */     null : (i.getUTF8String(1));\n",
      "/* 037 */     boolean isNull_2 = true;\n",
      "/* 038 */     java.lang.String value_2 = null;\n",
      "/* 039 */     if (!isNull_3) {\n",
      "/* 040 */       isNull_2 = false;\n",
      "/* 041 */       if (!isNull_2) {\n",
      "/* 042 */\n",
      "/* 043 */         Object funcResult_0 = null;\n",
      "/* 044 */         funcResult_0 = value_3.toString();\n",
      "/* 045 */         value_2 = (java.lang.String) funcResult_0;\n",
      "/* 046 */\n",
      "/* 047 */       }\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_2) {\n",
      "/* 050 */       values_0[1] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[1] = value_2;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     boolean isNull_4 = i.isNullAt(2);\n",
      "/* 056 */     long value_4 = isNull_4 ?\n",
      "/* 057 */     -1L : (i.getLong(2));\n",
      "/* 058 */     if (isNull_4) {\n",
      "/* 059 */       values_0[2] = null;\n",
      "/* 060 */     } else {\n",
      "/* 061 */       values_0[2] = value_4;\n",
      "/* 062 */     }\n",
      "/* 063 */\n",
      "/* 064 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 065 */     if (false) {\n",
      "/* 066 */       mutableRow.setNullAt(0);\n",
      "/* 067 */     } else {\n",
      "/* 068 */\n",
      "/* 069 */       mutableRow.update(0, value_0);\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     return mutableRow;\n",
      "/* 073 */   }\n",
      "/* 074 */\n",
      "/* 075 */\n",
      "/* 076 */ }\n",
      "\n",
      "24/12/16 10:36:56 INFO CodeGenerator: Code generated in 48.842084 ms\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 409, boot = 355, init = 54, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 392, boot = 343, init = 49, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 371, boot = 328, init = 43, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 356, boot = 313, init = 43, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 343, boot = 301, init = 42, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 343, boot = 299, init = 44, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 355, boot = 311, init = 43, finish = 1\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 348, boot = 304, init = 44, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 363, boot = 320, init = 43, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 361, boot = 316, init = 45, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 349, boot = 306, init = 43, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 380, boot = 335, init = 45, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 384, boot = 339, init = 45, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 375, boot = 331, init = 44, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 352, boot = 309, init = 42, finish = 1\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 367, boot = 324, init = 43, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 398, boot = 348, init = 50, finish = 0\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:mysql://10.208.164.167:3306/my_database and properties: {user=my_user, password=my_password}\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:mysql://10.208.164.167:3306/my_database and properties: {user=my_user, password=my_password}\n",
      "24/12/16 10:36:56 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:mysql://10.208.164.167:3306/my_database and properties: {user=my_user, password=my_password}\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 17.0 in stage 0.0 (TID 17). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 16.0 in stage 0.0 (TID 16). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 9.0 in stage 0.0 (TID 9). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1703 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 12.0 in stage 0.0 (TID 12). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 7.0 in stage 0.0 (TID 7). 1703 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 11.0 in stage 0.0 (TID 11). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 15.0 in stage 0.0 (TID 15). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 18.0 in stage 0.0 (TID 18). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 14.0 in stage 0.0 (TID 14). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 10.0 in stage 0.0 (TID 10). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 8.0 in stage 0.0 (TID 8). 1660 bytes result sent to driver\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 19\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 18\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 17\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 16\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 15\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 14\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 13\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 12\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 11\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 10\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 9\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 8\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 7\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 6\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 5\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 4\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 3\n",
      "24/12/16 10:36:56 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 19\n",
      "24/12/16 10:36:56 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 779 ms on hadoop113 (executor driver) (1/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 781 ms on hadoop113 (executor driver) (2/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 781 ms on hadoop113 (executor driver) (3/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 17.0 in stage 0.0 (TID 17) in 778 ms on hadoop113 (executor driver) (4/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 12.0 in stage 0.0 (TID 12) in 780 ms on hadoop113 (executor driver) (5/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 18.0 in stage 0.0 (TID 18) in 779 ms on hadoop113 (executor driver) (6/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 11.0 in stage 0.0 (TID 11) in 781 ms on hadoop113 (executor driver) (7/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 15.0 in stage 0.0 (TID 15) in 781 ms on hadoop113 (executor driver) (8/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 14.0 in stage 0.0 (TID 14) in 781 ms on hadoop113 (executor driver) (9/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 782 ms on hadoop113 (executor driver) (10/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 16.0 in stage 0.0 (TID 16) in 780 ms on hadoop113 (executor driver) (11/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 792 ms on hadoop113 (executor driver) (12/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 783 ms on hadoop113 (executor driver) (13/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 784 ms on hadoop113 (executor driver) (14/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 10.0 in stage 0.0 (TID 10) in 783 ms on hadoop113 (executor driver) (15/20)\n",
      "24/12/16 10:36:56 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 57589\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 784 ms on hadoop113 (executor driver) (16/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 784 ms on hadoop113 (executor driver) (17/20)\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 414, boot = 367, init = 47, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 342, boot = 295, init = 47, finish = 0\n",
      "24/12/16 10:36:56 INFO PythonRunner: Times: total = 346, boot = 302, init = 44, finish = 0\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 19.0 in stage 0.0 (TID 19). 1703 bytes result sent to driver\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 6.0 in stage 0.0 (TID 6). 1703 bytes result sent to driver\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 2\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 1\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 19.0 in stage 0.0 (TID 19) in 802 ms on hadoop113 (executor driver) (18/20)\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 805 ms on hadoop113 (executor driver) (19/20)\n",
      "24/12/16 10:36:56 INFO Executor: Finished task 13.0 in stage 0.0 (TID 13). 1703 bytes result sent to driver\n",
      "24/12/16 10:36:56 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 0\n",
      "24/12/16 10:36:56 INFO TaskSetManager: Finished task 13.0 in stage 0.0 (TID 13) in 806 ms on hadoop113 (executor driver) (20/20)\n",
      "24/12/16 10:36:56 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/12/16 10:36:56 INFO DAGScheduler: ResultStage 0 (save at NativeMethodAccessorImpl.java:0) finished in 0.934 s\n",
      "24/12/16 10:36:56 DEBUG DAGScheduler: After removal of stage 0, remaining stages = 0\n",
      "24/12/16 10:36:56 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/12/16 10:36:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/12/16 10:36:56 INFO DAGScheduler: Job 0 finished: save at NativeMethodAccessorImpl.java:0, took 0.961216 s\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:mysql://10.208.164.167:3306/my_database and properties: {user=my_user, password=my_password}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/16 10:36:56 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.ExecutedCommandExec\",\"num-children\":0,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"ID\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"9ebd6366-e844-4d24-9bf8-268e095f22af\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"Name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"9ebd6366-e844-4d24-9bf8-268e095f22af\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"Age\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"9ebd6366-e844-4d24-9bf8-268e095f22af\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]}]\n",
      "24/12/16 10:36:56 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "         +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/16 10:36:56 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "         +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/16 10:36:56 DEBUG KafkaRelationVisitor: Checking if provider is KafkaSourceProvider\n",
      "24/12/16 10:36:56 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/16 10:36:56 DEBUG KafkaRelationVisitor: Kafka classes are not available to check whether provider is KafkaSourceProvider\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:mysql://10.208.164.167:3306/my_database and properties: {user=my_user, password=my_password}\n",
      "24/12/16 10:36:56 DEBUG PlanUtils: Visitor null visited org.apache.spark.sql.execution.datasources.LogicalRelation, returned [io.openlineage.client.OpenLineage$OutputDataset@7f7b4f89]\n",
      "24/12/16 10:36:56 INFO RemovePathPatternUtils: Removing path pattern from dataset name person\n",
      "24/12/16 10:36:56 DEBUG HdfsPathDataset: path: person\n",
      "24/12/16 10:36:56 DEBUG RemovePathPatternUtils: Transformed path is person\n",
      "24/12/16 10:36:56 DEBUG SparkSQLExecutionContext: Posting event for start 0: io.openlineage.client.OpenLineage$RunEvent@24f08f4d\n",
      "24/12/16 10:36:56 DEBUG OpenLineageClient: OpenLineageClient will emit lineage event: {\"eventTime\":\"2024-12-16T03:36:54.808Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193cd8a-531f-7be9-82be-722b3b84fce1\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193cd8a-385c-73e6-9e47-b9010c063b85\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/16 10:36:56 INFO ConsoleTransport: {\"eventTime\":\"2024-12-16T03:36:54.808Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193cd8a-531f-7be9-82be-722b3b84fce1\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193cd8a-385c-73e6-9e47-b9010c063b85\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/16 10:36:56 DEBUG EventEmitter: Emitting lineage completed successfully: {\"eventTime\":\"2024-12-16T03:36:54.808Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"START\",\"run\":{\"runId\":\"0193cd8a-531f-7be9-82be-722b3b84fce1\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193cd8a-385c-73e6-9e47-b9010c063b85\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/16 10:36:56 INFO AsyncEventQueue: Process of event SparkListenerSQLExecutionStart(0,save at NativeMethodAccessorImpl.java:0,org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n",
      "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "java.lang.reflect.Method.invoke(Method.java:498)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:282)\n",
      "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750),== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand (1)\n",
      "   +- SaveIntoDataSourceCommand (2)\n",
      "         +- LogicalRDD (3)\n",
      "\n",
      "\n",
      "(1) Execute SaveIntoDataSourceCommand\n",
      "Output: []\n",
      "\n",
      "(2) SaveIntoDataSourceCommand\n",
      "Arguments: org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, [url=*********(redacted), driver=com.mysql.cj.jdbc.Driver, dbtable=person, user=my_user, password=*********(redacted)], Overwrite\n",
      "\n",
      "(3) LogicalRDD\n",
      "Arguments: [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      ",org.apache.spark.sql.execution.SparkPlanInfo@e60025fe,1734320214808,Map()) by listener DatahubSparkListener took 1.883963779s.\n",
      "24/12/16 10:36:56 INFO DatahubSparkListener: sparkEnv: spark.app.id=local-1734320207265\n",
      "spark.app.initial.jar.urls=spark://hadoop113:35983/jars/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,spark://hadoop113:35983/jars/mysql-connector-java-8.0.29.jar\n",
      "spark.app.name=WriteToMySQL222\n",
      "spark.app.startTime=1734320206670\n",
      "spark.app.submitTime=1734320206552\n",
      "spark.datahub.metadata.dataset.experimental_include_schema_metadata=true\n",
      "spark.datahub.metadata.dataset.materialize=true\n",
      "spark.datahub.rest.server=http://10.208.164.167:8080\n",
      "spark.driver.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.driver.host=hadoop113\n",
      "spark.driver.port=35983\n",
      "spark.executor.extraJavaOptions=-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "spark.executor.id=driver\n",
      "spark.extraListeners=datahub.spark.DatahubSparkListener\n",
      "spark.jars=/tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,/tmp/mysql-connector-java-8.0.29.jar\n",
      "spark.master=local[*]\n",
      "spark.rdd.compress=True\n",
      "spark.repl.local.jars=file:///tmp/acryl-spark-lineage-5206f9d-SNAPSHOT.jar,file:///tmp/mysql-connector-java-8.0.29.jar\n",
      "spark.serializer.objectStreamReset=100\n",
      "spark.sql.warehouse.dir=file:/spark-warehouse\n",
      "spark.submit.deployMode=client\n",
      "spark.submit.pyFiles=\n",
      "spark.ui.showConsoleProgress=true\n",
      "24/12/16 10:36:56 INFO DatahubSparkListener: Datahub configuration: {\n",
      "    # String: 1-3\n",
      "    \"metadata\" : {\n",
      "        # String: 1-3\n",
      "        \"dataset\" : {\n",
      "            # String: 1\n",
      "            \"experimental_include_schema_metadata\" : \"true\",\n",
      "            # String: 3\n",
      "            \"materialize\" : \"true\"\n",
      "        }\n",
      "    },\n",
      "    # String: 2\n",
      "    \"rest\" : {\n",
      "        # String: 2\n",
      "        \"server\" : \"http://10.208.164.167:8080\"\n",
      "    }\n",
      "}\n",
      "\n",
      "24/12/16 10:36:56 INFO DatahubSparkListener: REST Emitter Configuration: GMS url http://10.208.164.167:8080\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: loadDatahubConfig completed successfully in 105 ms\n",
      "24/12/16 10:36:56 INFO ArgumentParser: Couldn't log config from file, will read it from SparkConf\n",
      "24/12/16 10:36:56 DEBUG InternalLoggerFactory: Using SLF4J as the default logging framework\n",
      "24/12/16 10:36:56 DEBUG CompositeMeterRegistry: A MeterFilter is being configured after a Meter has been registered to this registry. All MeterFilters should be configured before any Meters are registered. If that is not possible or you have a use case where it should be allowed, let the Micrometer maintainers know at https://github.com/micrometer-metrics/micrometer/issues/4920.\n",
      "java.lang.Thread.getStackTrace(Thread.java:1564)\n",
      "\tat io.micrometer.core.instrument.MeterRegistry$Config.logWarningAboutLateFilter(MeterRegistry.java:844)\n",
      "\tat io.micrometer.core.instrument.MeterRegistry$Config.meterFilter(MeterRegistry.java:830)\n",
      "\tat io.micrometer.core.instrument.MeterRegistry$Config.commonTags(MeterRegistry.java:807)\n",
      "\tat datahub.spark.DatahubSparkListener.initializeMetrics(DatahubSparkListener.java:292)\n",
      "\tat datahub.spark.DatahubSparkListener.initializeContextFactoryIfNotInitialized(DatahubSparkListener.java:339)\n",
      "\tat datahub.spark.DatahubSparkListener.initializeContextFactoryIfNotInitialized(DatahubSparkListener.java:328)\n",
      "\tat datahub.spark.DatahubSparkListener.lambda$initializeContextFactoryIfNotInitialized$1(DatahubSparkListener.java:314)\n",
      "\tat java.util.Optional.ifPresent(Optional.java:159)\n",
      "\tat datahub.spark.DatahubSparkListener.initializeContextFactoryIfNotInitialized(DatahubSparkListener.java:314)\n",
      "\tat datahub.spark.DatahubSparkListener.onJobStart(DatahubSparkListener.java:221)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "24/12/16 10:36:56 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/16 10:36:56 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Job start called\n",
      "24/12/16 10:36:56 ERROR ContextFactory: Query execution is null: can't emit event for executionId 0\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onJobStart completed successfully in 117 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Task end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onTaskEnd completed successfully in 0 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Job end called\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: onJobEnd completed successfully in 1 ms\n",
      "24/12/16 10:36:56 DEBUG DatahubSparkListener: Other event called org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd\n",
      "24/12/16 10:36:56 INFO RedshiftVendor: Checking if Redshift classes are available\n",
      "24/12/16 10:36:56 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/16 10:36:56 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/16 10:36:56 DEBUG SparkSQLExecutionContext: SparkListenerSQLExecutionEnd - executionId: 0\n",
      "24/12/16 10:36:56 DEBUG OpenLineageRunEventBuilder: Traversing optimized plan [{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"ID\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"9ebd6366-e844-4d24-9bf8-268e095f22af\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"Name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"9ebd6366-e844-4d24-9bf8-268e095f22af\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"Age\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"9ebd6366-e844-4d24-9bf8-268e095f22af\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]\n",
      "24/12/16 10:36:56 DEBUG OpenLineageRunEventBuilder: Physical plan executed [{\"class\":\"org.apache.spark.sql.execution.command.ExecutedCommandExec\",\"num-children\":0,\"cmd\":[{\"class\":\"org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand\",\"num-children\":0,\"query\":[{\"class\":\"org.apache.spark.sql.execution.LogicalRDD\",\"num-children\":0,\"output\":[[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"ID\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":0,\"jvmId\":\"9ebd6366-e844-4d24-9bf8-268e095f22af\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"Name\",\"dataType\":\"string\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":1,\"jvmId\":\"9ebd6366-e844-4d24-9bf8-268e095f22af\"},\"qualifier\":[]}],[{\"class\":\"org.apache.spark.sql.catalyst.expressions.AttributeReference\",\"num-children\":0,\"name\":\"Age\",\"dataType\":\"long\",\"nullable\":true,\"metadata\":{},\"exprId\":{\"product-class\":\"org.apache.spark.sql.catalyst.expressions.ExprId\",\"id\":2,\"jvmId\":\"9ebd6366-e844-4d24-9bf8-268e095f22af\"},\"qualifier\":[]}]],\"rdd\":null,\"outputPartitioning\":{\"product-class\":\"org.apache.spark.sql.catalyst.plans.physical.UnknownPartitioning\",\"numPartitions\":0},\"outputOrdering\":[],\"isStreaming\":false,\"session\":null}],\"dataSource\":null,\"options\":null,\"mode\":null}]}]\n",
      "24/12/16 10:36:56 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "         +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "] with input dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/16 10:36:56 DEBUG OpenLineageRunEventBuilder: Visiting query plan Optional[== Parsed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "   +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "Execute SaveIntoDataSourceCommand\n",
      "   +- SaveIntoDataSourceCommand org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider@206b8b81, Map(url -> *********(redacted), driver -> com.mysql.cj.jdbc.Driver, dbtable -> person, user -> my_user, password -> *********(redacted)), Overwrite\n",
      "         +- LogicalRDD [ID#0L, Name#1, Age#2L], false\n",
      "] with output dataset builders [<function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>, <function1>]\n",
      "24/12/16 10:36:56 DEBUG KafkaRelationVisitor: Checking if provider is KafkaSourceProvider\n",
      "24/12/16 10:36:56 DEBUG KafkaRelationVisitor: Checking if Kafka classes are available\n",
      "24/12/16 10:36:56 DEBUG KafkaRelationVisitor: Kafka classes are not available to check whether provider is KafkaSourceProvider\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG JDBCOptions: Keytab path found, assuming manual upload\n",
      "24/12/16 10:36:56 DEBUG BasicConnectionProvider: JDBC connection initiated with URL: jdbc:mysql://10.208.164.167:3306/my_database and properties: {user=my_user, password=my_password}\n",
      "24/12/16 10:36:56 DEBUG PlanUtils: Visitor null visited org.apache.spark.sql.execution.datasources.LogicalRelation, returned [io.openlineage.client.OpenLineage$OutputDataset@7bd10412]\n",
      "24/12/16 10:36:56 INFO RemovePathPatternUtils: Removing path pattern from dataset name person\n",
      "24/12/16 10:36:56 DEBUG HdfsPathDataset: path: person\n",
      "24/12/16 10:36:56 DEBUG RemovePathPatternUtils: Transformed path is person\n",
      "24/12/16 10:36:56 DEBUG SparkSQLExecutionContext: Posting event for end 0: {\"eventTime\":\"2024-12-16T03:36:56.326Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193cd8a-5aed-76d9-aab2-3c7bc620d2de\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193cd8a-5ae8-7c85-b511-14a52532ad54\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/16 10:36:56 DEBUG DatahubEventEmitter: Emitting lineage: {\"eventTime\":\"2024-12-16T03:36:56.326Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193cd8a-5aed-76d9-aab2-3c7bc620d2de\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193cd8a-5ae8-7c85-b511-14a52532ad54\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/16 10:36:56 INFO OpenLineageToDataHub: Emitting lineage: {\"eventTime\":\"2024-12-16T03:36:56.326Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193cd8a-5aed-76d9-aab2-3c7bc620d2de\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193cd8a-5ae8-7c85-b511-14a52532ad54\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/16 10:36:56 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@32a1aa0e, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/12/16 10:36:56 DEBUG OpenLineageToDataHub: Datahub Config: DatahubOpenlineageConfig(isStreaming=false, pipelineName=WriteToMySQL222, platformInstance=null, commonDatasetPlatformInstance=null, platform=null, pathSpecs={}, filePartitionRegexpPattern=null, fabricType=PROD, materializeDataset=true, includeSchemaMetadata=true, captureColumnLevelLineage=true, parentJobUrn=null, usePatch=false, hivePlatformAlias=hive, urnAliases={}, disableSymlinkResolution=false, lowerCaseDatasetUrns=false)\n",
      "24/12/16 10:36:56 INFO OpenLineageToDataHub: Spark properties: io.openlineage.client.OpenLineage$DefaultRunFacet@32a1aa0e, Properties: {spark.master=local[*], spark.app.name=WriteToMySQL222}\n",
      "24/12/16 10:36:56 DEBUG OpenLineageToDataHub: Dataset URN: Optional[urn:li:dataset:(urn:li:dataPlatform:mysql,person,PROD)], alias_list: {}\n",
      "24/12/16 10:36:56 DEBUG OpenLineageToDataHub: Building output with person\n",
      "24/12/16 10:36:56 INFO DatahubEventEmitter: Converted Job: DatahubJob(flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), dataFlowInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, name=WriteToMySQL222}, jobUrn=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_save_into_data_source_command.person), jobInfo={customProperties={spark.master=local[*], processingEngineVersion=3.3.1, spark.app.name=WriteToMySQL222, processingEngine=spark}, created={time=1734320216326}, name=execute_save_into_data_source_command.person, flowUrn=urn:li:dataFlow:(spark,WriteToMySQL222,default), type={string=spark}}, flowOwnership={owners=[], lastModified={actor=urn:li:corpuser:datahub, time=1734320216895}}, flowGlobalTags=null, flowDomains=null, flowPlatformInstance=null, dataProcessInstanceRunEvent={result={type=SUCCESS, nativeResultType=COMPLETE}, timestampMillis=1734320216326, status=COMPLETE}, dataProcessInstanceProperties={name=0193cd8a-5aed-76d9-aab2-3c7bc620d2de, created={actor=urn:li:corpuser:datahub, time=1734320216326}}, dataProcessInstanceRelationships={parentTemplate=urn:li:dataJob:(urn:li:dataFlow:(spark,WriteToMySQL222,default),write_to_my_sql222.execute_save_into_data_source_command.person), upstreamInstances=[]}, dataProcessInstanceUrn=urn:li:dataProcessInstance:0193cd8a-5aed-76d9-aab2-3c7bc620d2de, inSet=[], outSet=[DatahubDataset(urn=urn:li:dataset:(urn:li:dataPlatform:mysql,person,PROD), schemaMetadata={platformSchema={com.linkedin.schema.MySqlDDL={tableSchema=[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]}}, schemaName=, fields=[{fieldPath=ID, type={type={com.linkedin.schema.NumberType={}}}, nativeDataType=long}, {fieldPath=Name, type={type={com.linkedin.schema.StringType={}}}, nativeDataType=string}, {fieldPath=Age, type={type={com.linkedin.schema.NumberType={}}}, nativeDataType=long}], version=1, hash=, platform=urn:li:dataPlatform:mysql}, lineage=null)], parentJobs=[], datasetProperties={}, startTime=0, endTime=0, eventTime=1734320216326, eventFormatter=datahub.event.EventFormatter@72ece7ce), from {\"eventTime\":\"2024-12-16T03:36:56.326Z\",\"producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunEvent\",\"eventType\":\"COMPLETE\",\"run\":{\"runId\":\"0193cd8a-5aed-76d9-aab2-3c7bc620d2de\",\"facets\":{\"parent\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/ParentRunFacet.json#/$defs/ParentRunFacet\",\"run\":{\"runId\":\"0193cd8a-5ae8-7c85-b511-14a52532ad54\"},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222\"}},\"processing_engine\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/ProcessingEngineRunFacet.json#/$defs/ProcessingEngineRunFacet\",\"version\":\"3.3.1\",\"name\":\"spark\"},\"environment-properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"environment-properties\":{}},\"spark_properties\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/2-0-2/OpenLineage.json#/$defs/RunFacet\",\"properties\":{\"spark.master\":\"local[*]\",\"spark.app.name\":\"WriteToMySQL222\"}}}},\"job\":{\"namespace\":\"default\",\"name\":\"write_to_my_sql222.execute_save_into_data_source_command.person\",\"facets\":{\"jobType\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/2-0-2/JobTypeJobFacet.json#/$defs/JobTypeJobFacet\",\"processingType\":\"BATCH\",\"integration\":\"SPARK\",\"jobType\":\"SQL_JOB\"}}},\"inputs\":[],\"outputs\":[{\"namespace\":\"mysql://10.208.164.167:3306\",\"name\":\"person\",\"facets\":{\"dataSource\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/DatasourceDatasetFacet.json#/$defs/DatasourceDatasetFacet\",\"name\":\"mysql://10.208.164.167:3306\",\"uri\":\"mysql://10.208.164.167:3306\"},\"schema\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-1-1/SchemaDatasetFacet.json#/$defs/SchemaDatasetFacet\",\"fields\":[{\"name\":\"ID\",\"type\":\"long\"},{\"name\":\"Name\",\"type\":\"string\"},{\"name\":\"Age\",\"type\":\"long\"}]},\"lifecycleStateChange\":{\"_producer\":\"https://github.com/OpenLineage/OpenLineage/tree/1.16.0/integration/spark\",\"_schemaURL\":\"https://openlineage.io/spec/facets/1-0-1/LifecycleStateChangeDatasetFacet.json#/$defs/LifecycleStateChangeDatasetFacet\",\"lifecycleStateChange\":\"OVERWRITE\"}},\"outputFacets\":{}}]}\n",
      "24/12/16 10:36:56 INFO DatahubEventEmitter: Collecting lineage successfully in 121 ms\n",
      "24/12/16 10:37:06 DEBUG ExecutorMetricsPoller: removing (0, 0) from stageTCMP\n"
     ]
    }
   ],
   "source": [
    "data = [(1, \"John Doe\", 25), (2, \"Jane Smith\", 30), (3, \"Alice Johnson\", 22)]\n",
    "columns = [\"ID\", \"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# CREATE DATABASE my_database;\n",
    "# CREATE USER 'my_user'@'localhost' IDENTIFIED BY 'my_password';\n",
    "# CREATE USER 'my_user'@'%' IDENTIFIED BY 'my_password';\n",
    "# GRANT ALL PRIVILEGES ON my_database.* TO 'my_user'@'%';\n",
    "# FLUSH PRIVILEGES;\n",
    "# USE my_database;\n",
    "# select * from person;\n",
    "# truncate table person;\n",
    "# drop  table person;\n",
    "# CREATE TABLE person (\n",
    "#     ID INT PRIMARY KEY,\n",
    "#     Name VARCHAR(100),\n",
    "#     Age INT\n",
    "# );\n",
    "\n",
    "df.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://10.208.164.167:3306/my_database\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"person\") \\\n",
    "    .option(\"user\", \"my_user\") \\\n",
    "    .option(\"password\", \"my_password\") \\\n",
    "    .mode(\"overwrite\").save()   # nó tự tạo bảng cho, chú ý option overwrite hay không"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
